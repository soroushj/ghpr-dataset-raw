{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "Active healthcheck for cluster is failing\r\n\r\nActive healthcheck is failing though I am getting response from the upstream. Not sure if I am missing anything in the configuration.\r\n\r\n*Admin and Stats Output*:\r\ncluster.sample-cluster.bind_errors: 0\r\ncluster.sample-cluster.external.upstream_rq_200: 11\r\ncluster.sample-cluster.external.upstream_rq_2xx: 11\r\ncluster.sample-cluster.external.upstream_rq_301: 1\r\ncluster.sample-cluster.external.upstream_rq_3xx: 1\r\ncluster.sample-cluster.health_check.attempt: 12\r\ncluster.sample-cluster.health_check.failure: 12\r\ncluster.sample-cluster.health_check.healthy: 0\r\ncluster.sample-cluster.health_check.network_failure: 12\r\ncluster.sample-cluster.health_check.passive_failure: 0\r\ncluster.sample-cluster.health_check.success: 0\r\ncluster.sample-cluster.health_check.verify_cluster: 0\r\ncluster.sample-cluster.lb_healthy_panic: 12\r\ncluster.sample-cluster.lb_local_cluster_not_ok: 0\r\ncluster.sample-cluster.lb_recalculate_zone_structures: 0\r\ncluster.sample-cluster.lb_subsets_active: 0\r\ncluster.sample-cluster.lb_subsets_created: 0\r\ncluster.sample-cluster.lb_subsets_fallback: 0\r\ncluster.sample-cluster.lb_subsets_removed: 0\r\ncluster.sample-cluster.lb_subsets_selected: 0\r\ncluster.sample-cluster.lb_zone_cluster_too_small: 0\r\ncluster.sample-cluster.lb_zone_no_capacity_left: 0\r\ncluster.sample-cluster.lb_zone_number_differs: 0\r\ncluster.sample-cluster.lb_zone_routing_all_directly: 0\r\ncluster.sample-cluster.lb_zone_routing_cross_zone: 0\r\ncluster.sample-cluster.lb_zone_routing_sampled: 0\r\ncluster.sample-cluster.max_host_weight: 0\r\ncluster.sample-cluster.membership_change: 1\r\ncluster.sample-cluster.membership_healthy: 0\r\ncluster.sample-cluster.membership_total: 1\r\n\r\n*Config*:\r\n```\r\nnode:\r\n  id: some-node\r\n  cluster: default-cluster\r\n  locality:\r\n        zone: default-zone\r\n\r\nadmin:\r\n  access_log_path: /dev/stdout\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 10000 }\r\n    filter_chains:\r\n      filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: sample-cluster\r\n          http_filters:\r\n          - name: envoy.router\r\n  clusters:\r\n  - name: sample-cluster\r\n    type: LOGICAL_DNS\r\n    connect_timeout: 0.25s\r\n    lb_policy: ROUND_ROBIN\r\n    dns_lookup_family: V4_ONLY\r\n    hosts:\r\n    - socket_address:\r\n        protocol: TCP\r\n        address: some-domain.com  #have changed it for privacy concern\r\n        port_value: 80\r\n    health_checks:\r\n      timeout: 2s\r\n      interval: 5s\r\n      interval_jitter: 1s\r\n      unhealthy_threshold: 1\r\n      healthy_threshold: 3\r\n      no_traffic_interval: 60s\r\n      event_log_path: /dev/stdout\r\n      http_health_check:\r\n        path: /\r\n```\r\n\r\n*Logs*:\r\n$ [2018-07-19 20:54:09.457][19][info][main] source/server/server.cc:183] initializing epoch 0 (hot restart version=10.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363 size=2654312)\r\n[2018-07-19 20:54:09.457][19][info][main] source/server/server.cc:185] statically linked extensions:\r\n[2018-07-19 20:54:09.457][19][info][main] source/server/server.cc:187]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log\r\n[2018-07-19 20:54:09.457][19][info][main] source/server/server.cc:190]   filters.http: envoy.buffer,envoy.cors,envoy.ext_authz,envoy.fault,envoy.filters.http.header_to_metadata,envoy.filters.http.jwt_authn,envoy.filters.http.rbac,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash\r\n[2018-07-19 20:54:09.457][19][info][main] source/server/server.cc:193]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol,envoy.listener.tls_inspector\r\n[2018-07-19 20:54:09.457][19][info][main] source/server/server.cc:196]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.thrift_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy\r\n[2018-07-19 20:54:09.458][19][info][main] source/server/server.cc:198]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.stat_sinks.hystrix,envoy.statsd\r\n[2018-07-19 20:54:09.458][19][info][main] source/server/server.cc:200]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.zipkin\r\n[2018-07-19 20:54:09.458][19][info][main] source/server/server.cc:203]   transport_sockets.downstream: envoy.transport_sockets.capture,raw_buffer,tls\r\n[2018-07-19 20:54:09.458][19][info][main] source/server/server.cc:206]   transport_sockets.upstream: envoy.transport_sockets.capture,raw_buffer,tls\r\n[2018-07-19 20:54:09.463][19][debug][main] source/server/server.cc:234] admin address: 0.0.0.0:9901\r\n[2018-07-19 20:54:09.464][19][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)\r\n[2018-07-19 20:54:09.465][22][debug][grpc] source/common/grpc/google_async_client_impl.cc:39] completionThread running\r\n[2018-07-19 20:54:09.466][19][debug][upstream] source/common/upstream/cluster_manager_impl.cc:707] adding TLS initial cluster sample-cluster\r\n[2018-07-19 20:54:09.466][19][debug][upstream] source/common/upstream/logical_dns_cluster.cc:70] starting async DNS resolution for some-domain.com\r\n[2018-07-19 20:54:09.466][19][debug][upstream] source/common/network/dns_impl.cc:147] Setting DNS resolution timer for 5000 milliseconds\r\n[2018-07-19 20:54:09.466][19][debug][upstream] source/common/upstream/cluster_manager_impl.cc:61] cm init: adding: cluster=sample-cluster primary=1 secondary=0\r\n[2018-07-19 20:54:09.466][19][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)\r\n[2018-07-19 20:54:09.466][19][debug][config] source/server/configuration_impl.cc:62] listener #0:\r\n[2018-07-19 20:54:09.466][19][debug][config] source/server/listener_manager_impl.cc:528] begin add/update listener: name=listener_0 hash=16491985507912357005\r\n[2018-07-19 20:54:09.466][19][debug][config] source/server/listener_manager_impl.cc:38]   filter #0:\r\n[2018-07-19 20:54:09.466][19][debug][config] source/server/listener_manager_impl.cc:39]     name: envoy.http_connection_manager\r\n[2018-07-19 20:54:09.466][19][debug][config] source/server/listener_manager_impl.cc:42]   config: {\"http_filters\":[{\"name\":\"envoy.router\"}],\"route_config\":{\"virtual_hosts\":[{\"name\":\"local_service\",\"domains\":[\"*\"],\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"sample-cluster\"}}]}],\"name\":\"local_route\"},\"stat_prefix\":\"ingress_http\",\"codec_type\":null}\r\n[2018-07-19 20:54:09.468][19][debug][config] source/extensions/filters/network/http_connection_manager/config.cc:279]     http filter #0\r\n[2018-07-19 20:54:09.468][19][debug][config] source/extensions/filters/network/http_connection_manager/config.cc:280]       name: envoy.router\r\n[2018-07-19 20:54:09.468][19][debug][config] source/extensions/filters/network/http_connection_manager/config.cc:284]     config: {}\r\n[2018-07-19 20:54:09.468][19][debug][config] source/server/listener_manager_impl.cc:414] add active listener: name=listener_0, hash=16491985507912357005, address=0.0.0.0:10000\r\n[2018-07-19 20:54:09.468][19][info][config] source/server/configuration_impl.cc:94] loading tracing configuration\r\n[2018-07-19 20:54:09.468][19][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration\r\n[2018-07-19 20:54:09.468][19][info][main] source/server/server.cc:410] starting main dispatch loop\r\n[2018-07-19 20:54:09.468][19][debug][upstream] source/common/upstream/logical_dns_cluster.cc:78] async DNS resolution complete for some-domain.com\r\n[2018-07-19 20:54:09.468][19][debug][client] source/common/http/codec_client.cc:25] [C0] connecting\r\n[2018-07-19 20:54:09.468][19][debug][connection] source/common/network/connection_impl.cc:570] [C0] connecting to 0.0.0.0:0\r\n[2018-07-19 20:54:09.468][19][debug][connection] source/common/network/connection_impl.cc:579] [C0] connection in progress\r\n[2018-07-19 20:54:09.472][19][debug][connection] source/common/network/connection_impl.cc:475] [C0] delayed connection error: 111\r\n[2018-07-19 20:54:09.472][19][debug][connection] source/common/network/connection_impl.cc:133] [C0] closing socket: 0\r\n[2018-07-19 20:54:09.472][19][debug][client] source/common/http/codec_client.cc:81] [C0] disconnect. resetting 1 pending requests\r\n[2018-07-19 20:54:09.472][19][debug][client] source/common/http/codec_client.cc:104] [C0] request reset\r\n[2018-07-19 20:54:09.472][19][debug][hc] source/common/upstream/health_checker_impl.cc:170] [C0] connection/stream error health_flags=healthy\r\n[2018-07-19 20:54:09.472][19][debug][upstream] source/common/upstream/cluster_manager_impl.cc:844] membership update for TLS cluster sample-cluster\r\n{\"health_checker_type\":\"HTTP\",\"host\":{\"socket_address\":{\"protocol\":\"TCP\",\"address\":\"0.0.0.0\",\"resolver_name\":\"\",\"ipv4_compat\":false,\"port_value\":0}},\"cluster_name\":\"sample-cluster\",\"eject_unhealthy_event\":{\"failure_type\":\"NETWORK\"}}\r\n[2018-07-19 20:54:09.472][19][debug][upstream] source/common/upstream/cluster_manager_impl.cc:89] cm init: init complete: cluster=sample-cluster primary=0 secondary=0\r\n\r\ncurl -v http://some-domain.com:80\r\n* About to connect() to some-domain.com port 80 (#0)\r\n*   Trying 148.x.x.x...\r\n* Connected to some-domain.com (148.x.x.x) port 80 (#0)\r\n> GET / HTTP/1.1\r\n> User-Agent: curl/7.29.0\r\n> Host: some-domain.com\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 301 Moved Permanently\r\n< Content-Type: text/html; charset=utf-8\r\n< Location: /ui/\r\n< Date: Thu, 19 Jul 2018 21:11:50 GMT\r\n< Content-Length: 39\r\n<\r\n<a href=\"/ui/\">Moved Permanently</a>.\r\n\r\n* Connection #0 to host some-domain.com left intact",
  "closed_at": "2018-08-23T13:32:32Z",
  "closed_by": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/18220477?v=4",
    "events_url": "https://api.github.com/users/alyssawilk/events{/privacy}",
    "followers_url": "https://api.github.com/users/alyssawilk/followers",
    "following_url": "https://api.github.com/users/alyssawilk/following{/other_user}",
    "gists_url": "https://api.github.com/users/alyssawilk/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/alyssawilk",
    "id": 18220477,
    "login": "alyssawilk",
    "node_id": "MDQ6VXNlcjE4MjIwNDc3",
    "organizations_url": "https://api.github.com/users/alyssawilk/orgs",
    "received_events_url": "https://api.github.com/users/alyssawilk/received_events",
    "repos_url": "https://api.github.com/users/alyssawilk/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/alyssawilk/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/alyssawilk/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/alyssawilk"
  },
  "comments": 6,
  "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3908/comments",
  "created_at": "2018-07-19T21:16:25Z",
  "events_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3908/events",
  "html_url": "https://github.com/envoyproxy/envoy/issues/3908",
  "id": 342892509,
  "labels": [
    {
      "color": "ee0701",
      "default": true,
      "description": null,
      "id": 421403907,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MjE0MDM5MDc=",
      "url": "https://api.github.com/repos/envoyproxy/envoy/labels/bug"
    },
    {
      "color": "cc317c",
      "default": true,
      "description": "Questions that are neither investigations, bugs, nor enhancements",
      "id": 421403912,
      "name": "question",
      "node_id": "MDU6TGFiZWw0MjE0MDM5MTI=",
      "url": "https://api.github.com/repos/envoyproxy/envoy/labels/question"
    }
  ],
  "labels_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3908/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUzNDI4OTI1MDk=",
  "number": 3908,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/envoyproxy/envoy",
  "state": "closed",
  "title": "Active health check for LOGICAL_DNS cluster is failing",
  "updated_at": "2018-08-23T13:32:32Z",
  "url": "https://api.github.com/repos/envoyproxy/envoy/issues/3908",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/17709850?v=4",
    "events_url": "https://api.github.com/users/psrin7/events{/privacy}",
    "followers_url": "https://api.github.com/users/psrin7/followers",
    "following_url": "https://api.github.com/users/psrin7/following{/other_user}",
    "gists_url": "https://api.github.com/users/psrin7/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/psrin7",
    "id": 17709850,
    "login": "psrin7",
    "node_id": "MDQ6VXNlcjE3NzA5ODUw",
    "organizations_url": "https://api.github.com/users/psrin7/orgs",
    "received_events_url": "https://api.github.com/users/psrin7/received_events",
    "repos_url": "https://api.github.com/users/psrin7/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/psrin7/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/psrin7/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/psrin7"
  }
}