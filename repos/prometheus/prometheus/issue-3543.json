{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "**What did you do?**\r\nRan prom/prometheus:master with prom/alertmanager:v0.11.0. I'm using master instead of v2.0.0 because I need to pick up the fixes in prometheus/tsdb#213 and #3508.\r\n\r\n**What did you expect to see?**\r\nAlerts posted to alertmanager from prometheus work as expected\r\n\r\n**What did you see instead? Under which circumstances?**\r\nIn prometheus logs:\r\n```\r\nlevel=error ts=2017-12-04T15:57:01.4290217Z caller=notifier.go:444 component=notifier alertmanager=http://172.30.18.142:9093/api/v1/alerts count=64 msg=\"Error sending alert\" err=\"bad response status 400 Bad Request\"\r\n```\r\n\r\nIn alertmanager logs:\r\n```\r\nlevel=error ts=2017-12-04T15:57:01.42907689Z caller=api.go:780 msg=\"API error\" err=\"bad_data: start time must be before end time; start time must be before end time; start time must be before end time; start time must be before end time; start time must be before end time\"\r\n```\r\n\r\n**Environment**\r\nKubernetes 1.8.2\r\n\r\n* System information:\r\n```\r\n/prometheus $ uname -srm\r\nLinux 4.4.0-101-generic x86_64\r\n```\r\n\r\n* Prometheus version:\r\n```\r\n/prometheus $ prometheus --version\r\nprometheus, version 2.0.0 (branch: master, revision: 30b4439bbd795fee4703b8f5015da6bf13fee0ac)\r\n  build user:       root@69018d7b60eb\r\n  build date:       20171204-11:46:26\r\n  go version:       go1.9.2\r\n```\r\n\r\n* Alertmanager version:\r\n```\r\nalertmanager, version 0.11.0 (branch: HEAD, revision: 30dd0426c08b6479d9a26259ea5efd63bc1ee273)\r\n  build user:       root@3e103e3fc918\r\n  build date:       20171116-17:43:56\r\n  go version:       go1.9.2\r\n```\r\n\r\n* Prometheus configuration file:\r\n```\r\nglobal:\r\n  scrape_interval: 30s\r\n  scrape_timeout: 10s\r\n  evaluation_interval: 30s\r\n  external_labels:\r\n    core_env: kube\r\nalerting:\r\n  alertmanagers:\r\n  - kubernetes_sd_configs:\r\n    - api_server: null\r\n      role: pod\r\n      namespaces:\r\n        names: []\r\n    scheme: http\r\n    timeout: 10s\r\n    relabel_configs:\r\n    - source_labels: [__meta_kubernetes_pod_label_app]\r\n      separator: ;\r\n      regex: alertmanager\r\n      replacement: $1\r\n      action: keep\r\n    - source_labels: [__meta_kubernetes_namespace]\r\n      separator: ;\r\n      regex: default\r\n      replacement: $1\r\n      action: keep\r\n    - source_labels: [__meta_kubernetes_pod_container_port_number]\r\n      separator: ;\r\n      regex: null\r\n      replacement: $1\r\n      action: drop\r\nrule_files:\r\n- /config/prometheus-2/rules\r\nscrape_configs:\r\n- job_name: prometheus\r\n  scrape_interval: 30s\r\n  scrape_timeout: 10s\r\n  metrics_path: /metrics\r\n  scheme: http\r\n  static_configs:\r\n  - targets:\r\n    - localhost:9091\r\n- job_name: job-N\r\n  scrape_interval: 30s\r\n  scrape_timeout: 10s\r\n  metrics_path: /metrics\r\n  scheme: http\r\n  metric_relabel_configs:\r\n  - source_labels: [__name__]\r\n    separator: ;\r\n    regex: go_.+\r\n    replacement: $1\r\n    action: drop\r\n  - source_labels: [__name__]\r\n    separator: ;\r\n    regex: http_.+\r\n    replacement: $1\r\n    action: drop\r\n  static_configs:\r\n  - targets:\r\n    -  <snip>\r\n    labels:\r\n      <snip>\r\n  - targets:\r\n    - <snip>\r\n    labels:\r\n      <snip>\r\n.\r\n.\r\n.\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\n    global:\r\n      resolve_timeout: 1h\r\n      slack_api_url: \"{{ inquisition_slack_url }}\"\r\n\r\n    route:\r\n      group_by: ['alertname']\r\n      group_wait: 2s\r\n      group_interval: 10s\r\n      receiver: slack\r\n      # If an alert has been sent, then wait \"repeat_interval\" time before resending it\r\n      repeat_interval: 1h\r\n      routes:\r\n      - match:\r\n          destination: inq-enforcer\r\n        receiver: inq-enforcer\r\n        repeat_interval: 10m\r\n        group_by: ['instance','alertname','hostname','privateip','publicip']\r\n        group_wait: 2s\r\n        group_interval: 10s\r\n      - match:\r\n          destination: inq-town-crier\r\n        receiver: inq-town-crier\r\n        repeat_interval: 10m\r\n        group_by: ['instance','alertname','hostname','privateip','publicip']\r\n        group_wait: 2s\r\n        group_interval: 10s\r\n      - match:\r\n          destination: slack\r\n        receiver: slack\r\n        group_by: ['instance','alertname','hostname','privateip','publicip']\r\n        group_wait: 2s\r\n        group_interval: 10s\r\n      - match:\r\n          destination: slack-summarise\r\n        receiver: slack-summarise\r\n        group_by: ['alertname']\r\n        group_wait: 1m\r\n        group_interval: 10m\r\n        repeat_interval: 30m\r\n      - match:\r\n          destination: inq-overwatch\r\n        receiver: inq-overwatch\r\n      - match:\r\n          destination: pagerduty\r\n        receiver: pagerduty\r\n        group_by: ['instance','alertname','hostname','privateip','publicip']\r\n\r\n    receivers:\r\n    - name: inq-enforcer\r\n      webhook_configs:\r\n        - url: '<URL>'\r\n    - name: inq-town-crier\r\n      webhook_configs:\r\n        - url: '<URL>'\r\n    - name: slack\r\n      slack_configs:\r\n      - channel: \"#{{ notification_channel }}\"\r\n        title: \"[AlertManager] {{ '{{' }} .CommonAnnotations.summary {{ '}}' }}\"\r\n        text: \"{{ '{{' }} .CommonLabels.alertname {{ '}}' }} - {{ '{{' }} .CommonAnnotations.description {{ '}}' }}\"\r\n    - name: slack-summarise\r\n      slack_configs:\r\n      - channel: \"#{{ notification_channel }}\"\r\n        title: \"[AlertManager] {{ '{{' }} len .Alerts {{ '}}' }} {{ '{{' }} .CommonAnnotations.grouped_summary {{ '}}' }}\"\r\n        text: \"{{ '{{' }} .CommonLabels.alertname {{ '}}' }} - {{ '{{' }} len .Alerts {{ '}}' }} {{ '{{' }} .CommonAnnotations.grouped_description {{ '}}' }}\"\r\n    - name: inq-overwatch\r\n      slack_configs:\r\n      - channel: \"#{{ inquisition_notification_channel }}\"\r\n        title: \"{{ '{{' }} .CommonAnnotations.summary {{ '}}' }}\"\r\n        text: \"{{ '{{' }} .CommonAnnotations.description {{ '}}' }}\"\r\n      webhook_configs:\r\n        - url: '<URL>'\r\n    - name: pagerduty\r\n      pagerduty_configs:\r\n      - service_key: \"{{ inquisition_monitoring_pagerduty_key }}\"\r\n```\r\n\r\n* Logs:\r\nProm:\r\n```\r\nlevel=info ts=2017-12-04T15:53:31.333931883Z caller=main.go:220 msg=\"Starting Prometheus\" version=\"(version=2.0.0, branch=master, revision=36190239f6d965f5e009da2fae8b502c09399df5)\"\r\nlevel=info ts=2017-12-04T15:53:31.334009371Z caller=main.go:221 build_context=\"(go=go1.9.2, user=root@9d041e93ac17, date=20171130-13:36:53)\"\r\nlevel=info ts=2017-12-04T15:53:31.334034568Z caller=main.go:222 host_details=\"(Linux 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 prometheus-2-996bbbd66-4h47w (none))\"\r\nlevel=info ts=2017-12-04T15:53:31.338234558Z caller=main.go:398 msg=\"Starting TSDB ...\"\r\nlevel=info ts=2017-12-04T15:53:31.338073315Z caller=targetmanager.go:71 component=\"target manager\" msg=\"Starting target manager...\"\r\nlevel=info ts=2017-12-04T15:53:31.338340621Z caller=web.go:380 component=web msg=\"Start listening for connections\" address=:9091\r\nlevel=info ts=2017-12-04T15:54:11.407839167Z caller=main.go:408 msg=\"TSDB started\"\r\nlevel=info ts=2017-12-04T15:54:11.407932743Z caller=main.go:490 msg=\"Loading configuration file\" filename=/config/prometheus-2/prometheus\r\nlevel=info ts=2017-12-04T15:54:12.74356221Z caller=kubernetes.go:100 component=notifier discovery=k8s msg=\"Using pod service account via in-cluster config\"\r\nlevel=info ts=2017-12-04T15:54:12.751527333Z caller=main.go:385 msg=\"Server is ready to receive requests.\"\r\nlevel=error ts=2017-12-04T15:57:01.4290217Z caller=notifier.go:444 component=notifier alertmanager=http://172.30.18.142:9093/api/v1/alerts count=64 msg=\"Error sending alert\" err=\"bad response status 400 Bad Request\"\r\nlevel=error ts=2017-12-04T15:58:01.456900885Z caller=notifier.go:444 component=notifier alertmanager=http://172.30.18.142:9093/api/v1/alerts count=64 msg=\"Error sending alert\" err=\"bad response status 400 Bad Request\"\r\nlevel=error ts=2017-12-04T15:58:01.464241747Z caller=notifier.go:444 component=notifier alertmanager=http://172.30.18.142:9093/api/v1/alerts count=64 msg=\"Error sending alert\" err=\"bad response status 400 Bad Request\"\r\n```\r\n\r\nAlertmanager:\r\n```\r\n.\r\n.\r\n.\r\nlevel=info ts=2017-12-04T15:40:50.614454519Z caller=silence.go:279 component=silences msg=\"Maintenance done\" duration=75.744\u00b5s\r\nlevel=info ts=2017-12-04T15:40:50.62667297Z caller=nflog.go:304 component=nflog msg=\"Maintenance done\" duration=99.391\u00b5s\r\nlevel=info ts=2017-12-04T15:55:50.609303021Z caller=nflog.go:287 component=nflog msg=\"Running maintenance\"\r\nlevel=info ts=2017-12-04T15:55:50.609515273Z caller=silence.go:262 component=silences msg=\"Running maintenance\"\r\nlevel=info ts=2017-12-04T15:55:50.61830075Z caller=silence.go:279 component=silences msg=\"Maintenance done\" duration=63.853\u00b5s\r\nlevel=info ts=2017-12-04T15:55:50.631665188Z caller=nflog.go:304 component=nflog msg=\"Maintenance done\" duration=85.098\u00b5s\r\nlevel=error ts=2017-12-04T15:57:01.42907689Z caller=api.go:780 msg=\"API error\" err=\"bad_data: start time must be before end time; start time must be before end time; start time must be before end time; start time must be before end time; start time must be before end time\"\r\nlevel=error ts=2017-12-04T15:58:01.456887499Z caller=api.go:780 msg=\"API error\" err=\"bad_data: start time must be before end time; start time must be before end time; start time must be before end time; start time must be before end time\"\r\nlevel=error ts=2017-12-04T15:58:01.46423974Z caller=api.go:780 msg=\"API error\" err=\"bad_data: start time must be before end time; start time must be before end time; start time must be before end time\"\r\n```\r\n\r\nAlertmanager has been up for 3 days prior to this issue (which occurred when I updated the prometheus container) so cut the rest of the logs out for clarity. I can provide them if needed.",
  "closed_at": "2018-02-01T16:18:10Z",
  "closed_by": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/5826063?v=4",
    "events_url": "https://api.github.com/users/conr/events{/privacy}",
    "followers_url": "https://api.github.com/users/conr/followers",
    "following_url": "https://api.github.com/users/conr/following{/other_user}",
    "gists_url": "https://api.github.com/users/conr/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/conr",
    "id": 5826063,
    "login": "conr",
    "node_id": "MDQ6VXNlcjU4MjYwNjM=",
    "organizations_url": "https://api.github.com/users/conr/orgs",
    "received_events_url": "https://api.github.com/users/conr/received_events",
    "repos_url": "https://api.github.com/users/conr/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/conr/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/conr/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/conr"
  },
  "comments": 19,
  "comments_url": "https://api.github.com/repos/prometheus/prometheus/issues/3543/comments",
  "created_at": "2017-12-04T16:10:47Z",
  "events_url": "https://api.github.com/repos/prometheus/prometheus/issues/3543/events",
  "html_url": "https://github.com/prometheus/prometheus/issues/3543",
  "id": 279052202,
  "labels": [
    {
      "color": "0052cc",
      "default": false,
      "description": null,
      "id": 365567376,
      "name": "component/notify",
      "node_id": "MDU6TGFiZWwzNjU1NjczNzY=",
      "url": "https://api.github.com/repos/prometheus/prometheus/labels/component/notify"
    },
    {
      "color": "ff0000",
      "default": false,
      "description": null,
      "id": 365563588,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwzNjU1NjM1ODg=",
      "url": "https://api.github.com/repos/prometheus/prometheus/labels/kind/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/prometheus/prometheus/issues/3543/labels{/name}",
  "locked": true,
  "milestone": null,
  "node_id": "MDU6SXNzdWUyNzkwNTIyMDI=",
  "number": 3543,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/prometheus/prometheus",
  "state": "closed",
  "title": "Bad data error posting alerts to alertmanager",
  "updated_at": "2019-03-22T09:28:29Z",
  "url": "https://api.github.com/repos/prometheus/prometheus/issues/3543",
  "user": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/698988?v=4",
    "events_url": "https://api.github.com/users/alxmk/events{/privacy}",
    "followers_url": "https://api.github.com/users/alxmk/followers",
    "following_url": "https://api.github.com/users/alxmk/following{/other_user}",
    "gists_url": "https://api.github.com/users/alxmk/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/alxmk",
    "id": 698988,
    "login": "alxmk",
    "node_id": "MDQ6VXNlcjY5ODk4OA==",
    "organizations_url": "https://api.github.com/users/alxmk/orgs",
    "received_events_url": "https://api.github.com/users/alxmk/received_events",
    "repos_url": "https://api.github.com/users/alxmk/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/alxmk/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/alxmk/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/alxmk"
  }
}