{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\nThe related osd will not be added to the cluster again after ungraceful K8s node restart.\r\n\r\n**Expected behavior:**\r\n* during K8s node outage the ceph cluster status should be degraded\r\n* after the K8s node has been started again the osd should be integrated again in the ceph cluster\r\n* no data loss, minimal replication effort\r\n\r\n**How to reproduce it (minimal and precise):**\r\n* Shutdown the K8s worker node: `virsh destroy --graceful --domain k8s-worker-04`\r\n  * ceph status will report `HEALTH_WARN` after a while\r\n  * one mon and one osd reported as lost\r\n  * reduced total volume capacity\r\n  * ceph volume access still possible\r\n* Start the K8s worker node: `virsh start --domain k8s-worker-04`\r\n  * ceph status still `HEALTH_WARN`\r\n  * one osd reported as lost\r\n  * reduced total volume capacity\r\n  * ceph volume access still possible\r\n  * `rook-ceph-osd-2-xyz` in Init:CrashLoopBackOff because of\r\n```bash\r\nControlled By:  ReplicaSet/rook-ceph-osd-2-85967dc998\r\nInit Containers:\r\n  activate-osd:\r\n    Container ID:  docker://60636a068071e44c9600d251a0015f873352faf6b79394c99ed5910f52160073\r\n    Image:         ceph/ceph:v14.2.8\r\n    Image ID:      docker-pullable://ceph/ceph@sha256:a3d6360ee9685447bb316b1e4ce10229580ba81e37d111c479788446e7233eef\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      /bin/bash\r\n      -c\r\n\r\n      set -ex\r\n\r\n      OSD_ID=2\r\n      OSD_UUID=de214744-b37b-44ff-a5f0-5522102babb5\r\n      OSD_STORE_FLAG=\"--bluestore\"\r\n      TMP_DIR=$(mktemp -d)\r\n      OSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\r\n\r\n      # active the osd with ceph-volume\r\n      ceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\r\n\r\n      # copy the tmpfs directory to a temporary directory\r\n      # this is needed because when the init container exits, the tmpfs goes away and its content with it\r\n      # this will result in the emptydir to be empty when accessed by the main osd container\r\n      cp --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\r\n\r\n      # unmount the tmpfs since we don't need it anymore\r\n      umount \"$OSD_DATA_DIR\"\r\n\r\n      # copy back the content of the tmpfs into the original osd directory\r\n      cp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\r\n\r\n      # retain ownership of files to the ceph user/group\r\n      chown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\r\n\r\n      # remove the temporary directory\r\n      rm --recursive --force \"$TMP_DIR\"\r\n\r\n    State:          Waiting\r\n      Reason:       CrashLoopBackOff\r\n    Last State:     Terminated\r\n      Reason:       Error\r\n      Exit Code:    1\r\n      Started:      Sun, 15 Mar 2020 18:06:29 +0100\r\n      Finished:     Sun, 15 Mar 2020 18:06:30 +0100\r\n    Ready:          False\r\n    Restart Count:  5\r\n```\r\n\r\n**File(s) to submit**:\r\n* `kubectl apply -f https://raw.githubusercontent.com/rook/rook/[v1.2.5|v1.2.6]/cluster/examples/kubernetes/ceph/common.yaml`\r\n* `kubectl apply -f ceph-operator.yaml` \r\n  * based on `https://raw.githubusercontent.com/rook/rook/[v1.2.5|v1.2.6]/cluster/examples/kubernetes/ceph/operator.yaml`\r\n  * rke specific kubelet path added\r\n```yaml\r\n- name: ROOK_CSI_KUBELET_DIR_PATH\r\n  value: \"/opt/rke/var/lib/kubelet\"\r\n```\r\n* `kubectl apply -f ./rke/rook.io/ceph-cluster.yaml`\r\n  * based on `https://raw.githubusercontent.com/rook/rook/[v1.2.5|v1.2.6]/cluster/examples/kubernetes/ceph/cluster.yaml`\r\n  * filter for k8s worker Ceph disks added\r\n```yaml\r\nstorage:\r\n    deviceFilter: \"^vd[b]\"\r\n```\r\n* enable pod disruption budgets\r\n```yaml\r\ndisruptionManagement:\r\n    managePodBudgets: true\r\n```\r\n* `kubectl apply -f https://raw.githubusercontent.com/rook/rook/v1.2.6/cluster/examples/kubernetes/ceph/enable-csi-2.0-rbac.yaml`\r\n* `kubectl apply -f ceph-storageclass-erasurecoding.yaml`\r\n```yaml\r\napiVersion: storage.k8s.io/v1\r\nkind: StorageClass\r\nmetadata:\r\n   name: rook-ceph-block-erasurecoding\r\nprovisioner: rook-ceph.rbd.csi.ceph.com\r\nparameters:\r\n    # clusterID is the namespace where the rook cluster is running\r\n    # If you change this namespace, also change the namespace below where the secret namespaces are defined\r\n    clusterID: rook-ceph\r\n\r\n    # If you want to use erasure coded pool with RBD, you need to create\r\n    # two pools. one erasure coded and one replicated.\r\n    # You need to specify the replicated pool here in the `pool` parameter, it is\r\n    # used for the metadata of the images.\r\n    # The erasure coded pool must be set as the `dataPool` parameter below.\r\n    dataPool: ec-data-pool\r\n    pool: replicated-metadata-pool\r\n\r\n    # RBD image format. Defaults to \"2\".\r\n    imageFormat: \"2\"\r\n\r\n    # RBD image features. Available for imageFormat: \"2\". CSI RBD currently supports only `layering` feature.\r\n    imageFeatures: layering\r\n\r\n    # The secrets contain Ceph admin credentials. These are generated automatically by the operator\r\n    # in the same namespace as the cluster.\r\n    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner\r\n    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph\r\n    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner\r\n    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph\r\n    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node\r\n    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph\r\n    # Specify the filesystem type of the volume. If not specified, csi-provisioner\r\n    # will set default as `ext4`.\r\n    csi.storage.k8s.io/fstype: xfs\r\n# uncomment the following to use rbd-nbd as mounter on supported nodes\r\n# **IMPORTANT**: If you are using rbd-nbd as the mounter, during upgrade you will be hit a ceph-csi\r\n# issue that causes the mount to be disconnected. You will need to follow special upgrade steps\r\n# to restart your application pods. Therefore, this option is not recommended.\r\n#mounter: rbd-nbd\r\nallowVolumeExpansion: true\r\nreclaimPolicy: Delete\r\n```\r\n* `kubectl apply -f ceph-erasurecodingpool.yaml`\r\n```yaml\r\napiVersion: ceph.rook.io/v1\r\nkind: CephBlockPool\r\nmetadata:\r\n  name: replicated-metadata-pool\r\n  namespace: rook-ceph\r\nspec:\r\n  replicated:\r\n    size: 2\r\n---\r\napiVersion: ceph.rook.io/v1\r\nkind: CephBlockPool\r\nmetadata:\r\n  name: ec-data-pool\r\n  namespace: rook-ceph\r\nspec:\r\n  # Make sure you have enough nodes and OSDs running bluestore to support the replica size or erasure code chunks.\r\n  # For the below settings, you need at least 3 OSDs on different nodes (because the `failureDomain` is `host` by default).\r\n  erasureCoded:\r\n    dataChunks: 2\r\n    codingChunks: 1\r\n```\r\n* `kubectl apply -f https://raw.githubusercontent.com/rook/rook/[v1.2.5|v1.2.6]/cluster/examples/kubernetes/ceph/dashboard-loadbalancer.yaml`\r\n* `kubectl apply -f https://raw.githubusercontent.com/rook/rook/[v1.2.5|v1.2.6]/cluster/examples/kubernetes/ceph/toolbox.yaml`\r\n\r\n**Environment**:\r\n* OS: `RancherOS 1.5.5`\r\n* Kernel: `4.14.138-rancher`\r\n* hardware configuration: \r\n  * one physical server (8 cores, 64GB RAM, 2x SSD)\r\n  * KVM\r\n  * 3x Master Node VMs \r\n  * 4x Worker Node VMs\r\n  * same SSD for all VMs\r\n* Rook version: `1.2.5` and later `1.2.6`\r\n* Storage backend version: `14.2.7` and later `14.2.8`\r\n* Kubernetes version: `v1.15.9-rancher1-1` and later `v1.16.6-rancher1-2`\r\n* Kubernetes cluster type: `rke 1.0.4`\r\n  * kubelet settings for rook paths added\r\n```yaml\r\nkubelet:\r\n    extra_args:\r\n      volume-plugin-dir: /usr/libexec/kubernetes/kubelet-plugins/volume/exec\r\n      root-dir: /opt/rke/var/lib/kubelet\r\n    extra_binds:\r\n      - \"/usr/libexec/kubernetes/kubelet-plugins/volume/exec:/usr/libexec/kubernetes/kubelet-plugins/volume/exec\"\r\n      - \"/var/lib/kubelet/plugins_registry:/var/lib/kubelet/plugins_registry\"\r\n      - \"/var/lib/kubelet/pods:/var/lib/kubelet/pods:shared,z\"\r\n      - \"/opt/rke/var/lib/kubelet:/opt/rke/var/lib/kubelet:shared,z\"\r\n```\r\n* Storage backend status: `HEALTH_WARN`\r\n\r\n**Troubleshooting**:\r\n* i have tried to clean up the ceph config on the worker node, but without success\r\n```bash\r\nsudo shred -n 1 -z /dev/vdb\r\nsudo lvremove --select lv_name=~'osd-.*'\r\nsudo vgremove --select vg_name=~'ceph-.*'\r\nsudo pvremove /dev/vdb\r\nsudo rm -rfv /dev/ceph-*\r\nsudo rm -rfv /var/lib/rook\r\n```\r\n* I have also tried to remove the osd from the ceph cluster config, but then the operator complains about the missing osd\r\n```bash\r\nceph osd out osd.2\r\nceph osd crush remove osd.2\r\nceph auth del osd.2\r\nceph osd rm osd.2\r\nkubectl -n rook-ceph delete rook-ceph-osd-2-xyz\r\n```\r\n* deleting the `rook-ceph-osd-2` deployment also has not fixed the problem\r\n```bash\r\nkubectl -n rook-ceph delete deployment rook-ceph-osd-2\r\n```\r\n\r\nSorry for the lengthly description, but there are a lot of moving parts involved.",
  "closed_at": "2020-03-18T14:28:46Z",
  "closed_by": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "comments": 20,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/5027/comments",
  "created_at": "2020-03-15T19:39:02Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/5027/events",
  "html_url": "https://github.com/rook/rook/issues/5027",
  "id": 581797590,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/5027/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1ODE3OTc1OTA=",
  "number": 5027,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "rook-ceph-osd Init:CrashLoopBackOff after ungraceful K8s node restart",
  "updated_at": "2020-10-16T17:43:45Z",
  "url": "https://api.github.com/repos/rook/rook/issues/5027",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/10164833?v=4",
    "events_url": "https://api.github.com/users/birkb/events{/privacy}",
    "followers_url": "https://api.github.com/users/birkb/followers",
    "following_url": "https://api.github.com/users/birkb/following{/other_user}",
    "gists_url": "https://api.github.com/users/birkb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/birkb",
    "id": 10164833,
    "login": "birkb",
    "node_id": "MDQ6VXNlcjEwMTY0ODMz",
    "organizations_url": "https://api.github.com/users/birkb/orgs",
    "received_events_url": "https://api.github.com/users/birkb/received_events",
    "repos_url": "https://api.github.com/users/birkb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/birkb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/birkb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/birkb"
  }
}