{
  "_links": {
    "comments": {
      "href": "https://api.github.com/repos/rook/rook/issues/3974/comments"
    },
    "commits": {
      "href": "https://api.github.com/repos/rook/rook/pulls/3974/commits"
    },
    "html": {
      "href": "https://github.com/rook/rook/pull/3974"
    },
    "issue": {
      "href": "https://api.github.com/repos/rook/rook/issues/3974"
    },
    "review_comment": {
      "href": "https://api.github.com/repos/rook/rook/pulls/comments{/number}"
    },
    "review_comments": {
      "href": "https://api.github.com/repos/rook/rook/pulls/3974/comments"
    },
    "self": {
      "href": "https://api.github.com/repos/rook/rook/pulls/3974"
    },
    "statuses": {
      "href": "https://api.github.com/repos/rook/rook/statuses/2de0787fc811f210f0c2ee64ff0b22e9d6fefbf0"
    }
  },
  "active_lock_reason": null,
  "additions": 99,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "base": {
    "label": "rook:master",
    "ref": "master",
    "repo": {
      "archive_url": "https://api.github.com/repos/rook/rook/{archive_format}{/ref}",
      "archived": false,
      "assignees_url": "https://api.github.com/repos/rook/rook/assignees{/user}",
      "blobs_url": "https://api.github.com/repos/rook/rook/git/blobs{/sha}",
      "branches_url": "https://api.github.com/repos/rook/rook/branches{/branch}",
      "clone_url": "https://github.com/rook/rook.git",
      "collaborators_url": "https://api.github.com/repos/rook/rook/collaborators{/collaborator}",
      "comments_url": "https://api.github.com/repos/rook/rook/comments{/number}",
      "commits_url": "https://api.github.com/repos/rook/rook/commits{/sha}",
      "compare_url": "https://api.github.com/repos/rook/rook/compare/{base}...{head}",
      "contents_url": "https://api.github.com/repos/rook/rook/contents/{+path}",
      "contributors_url": "https://api.github.com/repos/rook/rook/contributors",
      "created_at": "2016-07-08T22:45:05Z",
      "default_branch": "master",
      "deployments_url": "https://api.github.com/repos/rook/rook/deployments",
      "description": "Storage Orchestration for Kubernetes",
      "disabled": false,
      "downloads_url": "https://api.github.com/repos/rook/rook/downloads",
      "events_url": "https://api.github.com/repos/rook/rook/events",
      "fork": false,
      "forks": 1652,
      "forks_count": 1652,
      "forks_url": "https://api.github.com/repos/rook/rook/forks",
      "full_name": "rook/rook",
      "git_commits_url": "https://api.github.com/repos/rook/rook/git/commits{/sha}",
      "git_refs_url": "https://api.github.com/repos/rook/rook/git/refs{/sha}",
      "git_tags_url": "https://api.github.com/repos/rook/rook/git/tags{/sha}",
      "git_url": "git://github.com/rook/rook.git",
      "has_downloads": true,
      "has_issues": true,
      "has_pages": false,
      "has_projects": true,
      "has_wiki": true,
      "homepage": "https://rook.io",
      "hooks_url": "https://api.github.com/repos/rook/rook/hooks",
      "html_url": "https://github.com/rook/rook",
      "id": 62921553,
      "issue_comment_url": "https://api.github.com/repos/rook/rook/issues/comments{/number}",
      "issue_events_url": "https://api.github.com/repos/rook/rook/issues/events{/number}",
      "issues_url": "https://api.github.com/repos/rook/rook/issues{/number}",
      "keys_url": "https://api.github.com/repos/rook/rook/keys{/key_id}",
      "labels_url": "https://api.github.com/repos/rook/rook/labels{/name}",
      "language": "Go",
      "languages_url": "https://api.github.com/repos/rook/rook/languages",
      "license": {
        "key": "apache-2.0",
        "name": "Apache License 2.0",
        "node_id": "MDc6TGljZW5zZTI=",
        "spdx_id": "Apache-2.0",
        "url": "https://api.github.com/licenses/apache-2.0"
      },
      "merges_url": "https://api.github.com/repos/rook/rook/merges",
      "milestones_url": "https://api.github.com/repos/rook/rook/milestones{/number}",
      "mirror_url": null,
      "name": "rook",
      "node_id": "MDEwOlJlcG9zaXRvcnk2MjkyMTU1Mw==",
      "notifications_url": "https://api.github.com/repos/rook/rook/notifications{?since,all,participating}",
      "open_issues": 232,
      "open_issues_count": 232,
      "owner": {
        "avatar_url": "https://avatars1.githubusercontent.com/u/22860722?v=4",
        "events_url": "https://api.github.com/users/rook/events{/privacy}",
        "followers_url": "https://api.github.com/users/rook/followers",
        "following_url": "https://api.github.com/users/rook/following{/other_user}",
        "gists_url": "https://api.github.com/users/rook/gists{/gist_id}",
        "gravatar_id": "",
        "html_url": "https://github.com/rook",
        "id": 22860722,
        "login": "rook",
        "node_id": "MDEyOk9yZ2FuaXphdGlvbjIyODYwNzIy",
        "organizations_url": "https://api.github.com/users/rook/orgs",
        "received_events_url": "https://api.github.com/users/rook/received_events",
        "repos_url": "https://api.github.com/users/rook/repos",
        "site_admin": false,
        "starred_url": "https://api.github.com/users/rook/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/rook/subscriptions",
        "type": "Organization",
        "url": "https://api.github.com/users/rook"
      },
      "private": false,
      "pulls_url": "https://api.github.com/repos/rook/rook/pulls{/number}",
      "pushed_at": "2020-10-28T00:36:56Z",
      "releases_url": "https://api.github.com/repos/rook/rook/releases{/id}",
      "size": 37445,
      "ssh_url": "git@github.com:rook/rook.git",
      "stargazers_count": 7863,
      "stargazers_url": "https://api.github.com/repos/rook/rook/stargazers",
      "statuses_url": "https://api.github.com/repos/rook/rook/statuses/{sha}",
      "subscribers_url": "https://api.github.com/repos/rook/rook/subscribers",
      "subscription_url": "https://api.github.com/repos/rook/rook/subscription",
      "svn_url": "https://github.com/rook/rook",
      "tags_url": "https://api.github.com/repos/rook/rook/tags",
      "teams_url": "https://api.github.com/repos/rook/rook/teams",
      "trees_url": "https://api.github.com/repos/rook/rook/git/trees{/sha}",
      "updated_at": "2020-10-28T04:16:55Z",
      "url": "https://api.github.com/repos/rook/rook",
      "watchers": 7863,
      "watchers_count": 7863
    },
    "sha": "91b8a96fb6f6b2e8f15c48630f726880f91d1c6a",
    "user": {
      "avatar_url": "https://avatars1.githubusercontent.com/u/22860722?v=4",
      "events_url": "https://api.github.com/users/rook/events{/privacy}",
      "followers_url": "https://api.github.com/users/rook/followers",
      "following_url": "https://api.github.com/users/rook/following{/other_user}",
      "gists_url": "https://api.github.com/users/rook/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/rook",
      "id": 22860722,
      "login": "rook",
      "node_id": "MDEyOk9yZ2FuaXphdGlvbjIyODYwNzIy",
      "organizations_url": "https://api.github.com/users/rook/orgs",
      "received_events_url": "https://api.github.com/users/rook/received_events",
      "repos_url": "https://api.github.com/users/rook/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/rook/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/rook/subscriptions",
      "type": "Organization",
      "url": "https://api.github.com/users/rook"
    }
  },
  "body": "**Description of your changes:**\r\nNew setting to avoid delete pools on <filesystem>/<object store> deletion\r\n\r\nDespite of the suggestion of @travisn  i have preferred to include a new \"preservePools\" attribute in the Ceph filesytem and Ceph Object store k8 objects.\r\nMy decision is based in the fact that these objects need several pools to keep their integrity healthy. So if we want to avoid to delete the pools storing the information in a filesystem/onject store we must be sure that all the pools are preserved always.\r\nTo have the \"reclaimpolicy\" (retain/delete) attribute at pool level , implies to specify this attribute for each pool, and therefore this increase the probability of error and make less comfortable to set this type of change\r\n\r\nIn any case, we also can include the \"reclaim policy\" attribute in pools. By the moment,  i think is not needed, but probably I cannot see all the implications. So if i miss something please indicate it.\r\n\r\nBelow the details about the verification of the fix/enhancement:\r\n\r\n**Which issue is resolved by this Pull Request:**\r\nResolves #2206 \r\n\r\n**Checklist:**\r\n\r\n- [x] Reviewed the developer guide on [Submitting a Pull Request](https://rook.io/docs/rook/master/development-flow.html#submitting-a-pull-request)\r\n- [x] Documentation has been updated, if necessary.\r\n- [ ] Unit tests have been added, if necessary.\r\n- [ ] Integration tests have been added, if necessary.\r\n- [ ] Pending release notes updated with breaking and/or notable changes, if necessary.\r\n- [ ] Upgrade from previous release is tested and upgrade user guide is updated, if necessary.\r\n- [x] Code generation (`make codegen`) has been run to update object specifications, if necessary.\r\n- [ ] Comments have been added or updated based on the standards set in [CONTRIBUTING.md](https://github.com/rook/rook/blob/master/CONTRIBUTING.md#comments)\r\n- [ ] Add the flag for skipping the CI if this PR does not require a build. See [here](https://github.com/rook/rook/blob/master/INSTALL.md#skip-ci) for more details.\r\n\r\nSigned-off-by: Juan Miguel Olmo Mart\u00ednez jolmomar@redhat.com\r\n\r\n\r\n\r\n### **Details about verification:**\r\n\r\n**FILESYSTEM:**\r\n\r\nDelete a filesystem preserving pools\r\n\r\n1. In the filesystem CRD the \"Preserve Pools\" Attribute must be set to true. Create the filesystem:\r\n\r\n`$ kubectl create -f filesystem.yaml`\r\n\r\n2. Filesystem \"myfs\" has been created using \"myfs-data0\" and \"myfs-metadata\" pools:\r\n\r\n```\r\nroot@node3 /]# ceph -s\r\n  cluster:\r\n    id:     db6b561a-c2f8-4f07-bc32-728eccdf8c74\r\n    health: HEALTH_WARN\r\n            too few PGs per OSD (16 < min 30)\r\n\r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 17h)\r\n    mgr: a(active, since 91s)\r\n    mds: myfs:1 {0=myfs-a=up:active} 1 up:standby-replay\r\n    osd: 3 osds: 3 up (since 17h), 3 in (since 17h)\r\n\r\n  data:\r\n    pools:   2 pools, 16 pgs\r\n    objects: 22 objects, 2.2 KiB\r\n    usage:   3.1 GiB used, 69 GiB / 72 GiB avail\r\n    pgs:     16 active+clean\r\n\r\n  io:\r\n    client:   852 B/s rd, 1 op/s rd, 0 op/s wr\r\n\r\n\r\n[root@node3 /]# rados df\r\nPOOL_NAME        USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS     RD WR_OPS     WR USED COMPR UNDER COMPR\r\nmyfs-data0        0 B       0      0      0                  0       0        0      0    0 B      0    0 B        0 B         0 B\r\nmyfs-metadata 1.5 MiB      22      0     66                  0       0        0    120 64 KiB     45 13 KiB        0 B         0 B\r\n\r\ntotal_objects    22\r\ntotal_used       3.1 GiB\r\ntotal_avail      69 GiB\r\ntotal_space      72 GiB\r\n\r\n```\r\n\r\n3. Check value of \"Preserve Pools\" attribute in \"myfs\" k8 filesystem object:\r\n\r\n```\r\n$ kubectl -n rook-ceph get cephfilesystems myfs\r\nNAME      ACTIVEMDS   AGE\r\nmyfs      1           20m\r\n\r\n$ kubectl -n rook-ceph describe cephfilesystems myfs\r\nName:         myfs\r\nNamespace:    rook-ceph\r\nLabels:       <none>\r\nAnnotations:  <none>\r\nAPI Version:  ceph.rook.io/v1\r\nKind:         CephFilesystem\r\nMetadata:\r\n  Creation Timestamp:  2019-09-24T09:43:37Z\r\n  Generation:          1\r\n  Resource Version:    227581\r\n  Self Link:           /apis/ceph.rook.io/v1/namespaces/rook-ceph/cephfilesystems/myfs\r\n  UID:                 57d497e7-daf9-432d-8a04-159d15a2a0bd\r\nSpec:\r\n  Data Pools:\r\n    Failure Domain:  host\r\n    Replicated:\r\n      Size:  3\r\n  Metadata Pool:\r\n    Replicated:\r\n      Size:  3\r\n  Metadata Server:\r\n    Active Count:    1\r\n    Active Standby:  true\r\n    Annotations:     <nil>\r\n    Placement:\r\n      Pod Anti Affinity:\r\n        Required During Scheduling Ignored During Execution:\r\n          Label Selector:\r\n            Match Expressions:\r\n              Key:       app\r\n              Operator:  In\r\n              Values:\r\n                rook-ceph-mds\r\n          Topology Key:  kubernetes.io/hostname\r\n    Resources:           <nil>\r\n  Preserve Pools:        true\r\nEvents:                  <none>\r\n\r\n```\r\n4. Delete the Filesystem\r\n\r\n```\r\n$ kubectl -n rook-ceph delete cephfilesystem myfs\r\ncephfilesystem.ceph.rook.io \"myfs\" deleted\r\n\r\n```\r\n\r\nUsing Ceph toolbox:\r\n\r\n```\r\n[root@node3 /]# ceph -s\r\n  cluster:\r\n    id:     db6b561a-c2f8-4f07-bc32-728eccdf8c74\r\n    health: HEALTH_WARN\r\n            too few PGs per OSD (16 < min 30)\r\n\r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 17h)\r\n    mgr: a(active, since 9m)\r\n    osd: 3 osds: 3 up (since 17h), 3 in (since 17h)\r\n\r\n  data:\r\n    pools:   2 pools, 16 pgs\r\n    objects: 22 objects, 2.2 KiB\r\n    usage:   3.1 GiB used, 69 GiB / 72 GiB avail\r\n    pgs:     16 active+clean\r\n\r\n[root@node3 /]# rados df\r\nPOOL_NAME        USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS     WR USED COMPR UNDER COMPR\r\nmyfs-data0        0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\nmyfs-metadata 1.5 MiB      22      0     66                  0       0        0    842 425 KiB     45 13 KiB        0 B         0 B\r\n\r\ntotal_objects    22\r\ntotal_used       3.1 GiB\r\ntotal_avail      69 GiB\r\ntotal_space      72 GiB\r\n\r\n\r\n```\r\nIn the operator log file we can see:\r\n\r\n```\r\n...\r\n2019-09-24 10:10:30.391771 I | op-file: Downing filesystem myfs\r\n2019-09-24 10:10:30.393017 I | exec: Running command: ceph fs fail myfs --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/472395614\r\n2019-09-24 10:10:31.165975 I | exec: myfs marked not joinable; MDS cannot join the cluster. All MDS ranks marked failed.\r\n2019-09-24 10:10:31.166061 I | op-file: Downed filesystem myfs\r\n2019-09-24 10:10:31.166069 I | op-file: GO TO DELETE FILESYSTEM\r\n2019-09-24 10:10:31.166164 I | exec: Running command: ceph fs get myfs --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/300680485\r\n2019-09-24 10:10:31.672102 I | exec: Running command: ceph fs rm myfs --yes-i-really-mean-it --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/493696576\r\n2019-09-24 10:10:33.109234 I | cephclient: <PreservePools> is set in filesystem <myfs>. Pools not deleted\r\n2019-09-24 10:10:33.116488 I | op-mds: deleting mds deployment rook-ceph-mds-myfs-a\r\n2019-09-24 10:10:33.125196 I | op-mds: deleting mds deployment rook-ceph-mds-myfs-b\r\n...\r\n```\r\n\r\n**OBJECT STORE:**\r\nThe object store CRD has been modified to allow also preserve pools. It can be achieved setting the \"preservePools\" attribute in the object store CRD to true.\r\n\r\nVerification:\r\n\r\n1. Create a object store with the \"preservePools\" attribute set to \"true\"\r\n\r\n```\r\n$ cat object.yaml | grep preservePools\r\n  preservePools: true\r\n\r\n$ kubectl create -f object.yaml\r\ncephobjectstore.ceph.rook.io/my-store created\r\n```\r\n\r\n2. Using ceph toolbox, verify Ceph and Pools status:\r\n\r\n```\r\nroot@node3 /]# ceph -s\r\n  cluster:\r\n    id:     db6b561a-c2f8-4f07-bc32-728eccdf8c74\r\n    health: HEALTH_OK\r\n\r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 19h)\r\n    mgr: a(active, since 2m)\r\n    osd: 3 osds: 3 up (since 19h), 3 in (since 19h)\r\n    rgw: 1 daemon active (my.store.a)\r\n\r\n  data:\r\n    pools:   6 pools, 48 pgs\r\n    objects: 201 objects, 3.8 KiB\r\n    usage:   3.1 GiB used, 69 GiB / 72 GiB avail\r\n    pgs:     48 active+clean\r\n\r\n  io:\r\n    client:   7.8 KiB/s rd, 0 B/s wr, 11 op/s rd, 7 op/s wr\r\n\r\n[root@node3 /]# rados df\r\nPOOL_NAME                     USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS     WR USED COMPR UNDER COMPR\r\n.rgw.root                  2.8 MiB      16      0     48                  0       0        0     46  46 KiB     32 23 KiB        0 B         0 B\r\nmy-store.rgw.buckets.data      0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\nmy-store.rgw.buckets.index     0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\nmy-store.rgw.control           0 B       8      0     24                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\nmy-store.rgw.log           192 KiB     177      0    531                  0       0        0    526 350 KiB    353  1 KiB        0 B         0 B\r\nmy-store.rgw.meta              0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\n\r\ntotal_objects    201\r\ntotal_used       3.1 GiB\r\ntotal_avail      69 GiB\r\ntotal_space      72 GiB\r\n```\r\n\r\n3. Delete Object store\r\n\r\n```\r\n$ kubectl -n rook-ceph delete CephObjectStore my-store\r\ncephobjectstore.ceph.rook.io \"my-store\" deleted\r\n\r\n```\r\n4. Verify Ceph and pool status\r\n\r\n```\r\n[root@node3 /]# ceph -s\r\n  cluster:\r\n    id:     db6b561a-c2f8-4f07-bc32-728eccdf8c74\r\n    health: HEALTH_OK\r\n\r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 19h)\r\n    mgr: a(active, since 3m)\r\n    osd: 3 osds: 3 up (since 19h), 3 in (since 19h)\r\n\r\n  data:\r\n    pools:   6 pools, 48 pgs\r\n    objects: 193 objects, 2.2 KiB\r\n    usage:   3.1 GiB used, 69 GiB / 72 GiB avail\r\n    pgs:     48 active+clean\r\n\r\n  io:\r\n    client:   767 B/s rd, 0 B/s wr, 0 op/s rd, 0 op/s wr\r\n\r\n[root@node3 /]# rados df\r\nPOOL_NAME                     USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS     WR USED COMPR UNDER COMPR\r\n.rgw.root                  1.3 MiB       7      0     21                  0       0        0     55  55 KiB     41 23 KiB        0 B         0 B\r\nmy-store.rgw.buckets.data      0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\nmy-store.rgw.buckets.index     0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\nmy-store.rgw.control           0 B       8      0     24                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\nmy-store.rgw.log           192 KiB     178      0    534                  0       0        0    528 352 KiB    357  5 KiB        0 B         0 B\r\nmy-store.rgw.meta              0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\n\r\ntotal_objects    193\r\ntotal_used       3.1 GiB\r\ntotal_avail      69 GiB\r\ntotal_space      72 GiB\r\n```\r\n\r\n5. Check operator logs:\r\n```\r\n...\r\n2019-09-24 11:25:41.713460 I | op-object: Deleting object store my-store from namespace rook-ceph\r\n2019-09-24 11:25:41.762358 I | op-k8sutil: removing deployment rook-ceph-rgw-my-store-a if it exists\r\n2019-09-24 11:25:41.778529 I | op-k8sutil: Removed deployment rook-ceph-rgw-my-store-a\r\n2019-09-24 11:25:41.785292 I | op-k8sutil: rook-ceph-rgw-my-store-a still found. waiting...\r\n2019-09-24 11:25:43.793416 I | op-k8sutil: rook-ceph-rgw-my-store-a still found. waiting...\r\n2019-09-24 11:25:45.798573 I | op-k8sutil: confirmed rook-ceph-rgw-my-store-a does not exist\r\n2019-09-24 11:25:45.804315 I | exec: Running command: ceph auth del client.my.store.a --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/067910445\r\n2019-09-24 11:25:46.250311 I | exec: updated\r\n2019-09-24 11:25:46.250451 I | exec: Running command: radosgw-admin realm list --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring\r\n2019-09-24 11:25:46.489530 I | op-object: Found stores [my-store] when deleting store my-store\r\n2019-09-24 11:25:46.489601 I | exec: Running command: radosgw-admin realm delete --rgw-realm my-store --rgw-realm=my-store --rgw-zonegroup=my-store --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring\r\n2019-09-24 11:25:46.726192 I | exec: Running command: radosgw-admin zonegroup delete --rgw-zonegroup my-store --rgw-realm=my-store --rgw-zonegroup=my-store --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring\r\n2019-09-24 11:25:46.950101 I | exec: Running command: radosgw-admin zone delete --rgw-zone my-store --rgw-realm=my-store --rgw-zonegroup=my-store --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring\r\n2019-09-24 11:25:47.173069 I | op-object: <PreservePools> is set in object store <my-store>. Pools not deleted\r\n2019-09-24 11:25:47.173124 I | op-object: Completed deleting object store my-store\r\n...\r\n\r\n```\r\n\r\n==================================================================\r\n\r\n**FILESYSTEM**\r\n\r\n**verify that deleting a filesystem , pools are deleted**\r\n\r\n1. In the filesystem CRD the \"Preserve Pools\" Attribute must be set to false, or not included.\r\n\r\n```\r\n$ cat filesystem.yaml | grep preservePools\r\n  preservePools: false\r\n\r\n```\r\nCreate the filesystem:\r\n\r\n`$ kubectl create -f filesystem.yaml`\r\n\r\nOnce the filesystem is created we have:\r\n\r\n```\r\n$ kubectl -n rook-ceph get cephfilesystems myfs\r\nNAME      ACTIVEMDS   AGE\r\nmyfs      1           23s\r\n\r\n$ kubectl -n rook-ceph describe cephfilesystems myfs | grep Preserve\r\n  Preserve Pools:        false\r\n```\r\n\r\nUsing the ceph toolbox:\r\n\r\n```\r\n[root@node3 /]# ceph -s\r\n  cluster:\r\n    id:     db6b561a-c2f8-4f07-bc32-728eccdf8c74\r\n    health: HEALTH_WARN\r\n            too few PGs per OSD (16 < min 30)\r\n\r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 17h)\r\n    mgr: a(active, since 18m)\r\n    mds: myfs:1 {0=myfs-a=up:active} 1 up:standby-replay\r\n    osd: 3 osds: 3 up (since 17h), 3 in (since 17h)\r\n\r\n  data:\r\n    pools:   4 pools, 16 pgs\r\n    objects: 22 objects, 2.2 KiB\r\n    usage:   3.1 GiB used, 69 GiB / 72 GiB avail\r\n    pgs:     16 active+clean\r\n\r\n  io:\r\n    client:   1.2 KiB/s rd, 2 op/s rd, 0 op/s wr\r\n\r\n    [root@node3 /]# rados df\r\n    POOL_NAME        USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS     WR USED COMPR UNDER COMPR\r\n    myfs-data0        0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\n    myfs-metadata 1.5 MiB      22      0     66                  0       0        0    230 119 KiB     45 13 KiB        0 B         0 B\r\n\r\n    total_objects    22\r\n    total_used       3.1 GiB\r\n    total_avail      69 GiB\r\n    total_space      72 GiB\r\n```\r\n\r\n2. Delete the filesystem and verify ceph and pools STATUS\r\n\r\n```\r\n$ kubectl -n rook-ceph delete cephfilesystem myfs\r\ncephfilesystem.ceph.rook.io \"myfs\" deleted\r\n```\r\n\r\nUsing the Ceph toolbox:\r\n\r\n```\r\n[root@node3 /]# ceph -s\r\n  cluster:\r\n    id:     db6b561a-c2f8-4f07-bc32-728eccdf8c74\r\n    health: HEALTH_OK\r\n\r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 18h)\r\n    mgr: a(active, since 24m)\r\n    osd: 3 osds: 3 up (since 17h), 3 in (since 17h)\r\n\r\n  data:\r\n    pools:   3 pools, 0 pgs\r\n    objects: 0 objects, 0 B\r\n    usage:   3.1 GiB used, 69 GiB / 72 GiB avail\r\n    pgs:\r\n\r\n    [root@node3 /]# rados df\r\n    POOL_NAME USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS RD WR_OPS WR USED COMPR UNDER COMPR\r\n\r\n    total_objects    0\r\n    total_used       3.1 GiB\r\n    total_avail      69 GiB\r\n    total_space      72 GiB\r\n\r\n```\r\nand in the operator log we can see:\r\n\r\n```\r\n...\r\n2019-09-24 10:25:32.607202 I | op-file: Downing filesystem myfs\r\n2019-09-24 10:25:32.608043 I | exec: Running command: ceph fs fail myfs --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/804323567\r\n2019-09-24 10:25:33.343347 I | exec: myfs marked not joinable; MDS cannot join the cluster. All MDS ranks marked failed.\r\n2019-09-24 10:25:33.343458 I | op-file: Downed filesystem myfs\r\n2019-09-24 10:25:33.343467 I | op-file: GO TO DELETE FILESYSTEM\r\n2019-09-24 10:25:33.343532 I | exec: Running command: ceph fs get myfs --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/752691586\r\n2019-09-24 10:25:33.863220 I | exec: Running command: ceph fs rm myfs --yes-i-really-mean-it --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/325110521\r\n2019-09-24 10:25:35.265936 I | exec: Running command: ceph osd lspools --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/983511556\r\n2019-09-24 10:25:35.766810 I | exec: Running command: ceph osd pool get myfs-metadata all --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/096677779\r\n2019-09-24 10:25:36.280598 I | cephclient: purging pool myfs-metadata (id=21)\r\n2019-09-24 10:25:36.280754 I | exec: Running command: ceph osd pool delete myfs-metadata myfs-metadata --yes-i-really-really-mean-it --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/802382806\r\n2019-09-24 10:25:37.301183 I | exec: pool 'myfs-metadata' removed\r\n2019-09-24 10:25:37.301447 I | exec: Running command: ceph osd crush rule rm myfs-metadata --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/122894653\r\n2019-09-24 10:25:38.388965 I | cephclient: purge completed for pool myfs-metadata\r\n2019-09-24 10:25:38.389055 I | exec: Running command: ceph osd pool get myfs-data0 all --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/907849080\r\n2019-09-24 10:25:38.793918 I | cephclient: purging pool myfs-data0 (id=22)\r\n2019-09-24 10:25:38.793996 I | exec: Running command: ceph osd pool delete myfs-data0 myfs-data0 --yes-i-really-really-mean-it --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/505333879\r\n2019-09-24 10:25:39.398101 I | exec: pool 'myfs-data0' removed\r\n2019-09-24 10:25:39.398365 I | exec: Running command: ceph osd crush rule rm myfs-data0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/148229482\r\n2019-09-24 10:25:40.426258 I | cephclient: purge completed for pool myfs-data0\r\n2019-09-24 10:25:40.430637 I | op-mds: deleting mds deployment rook-ceph-mds-myfs-a\r\n2019-09-24 10:25:40.435734 I | op-mds: deleting mds deployment rook-ceph-mds-myfs-b\r\n...\r\n```\r\n\r\n**OBJECT STORE**\r\n\r\n1. Set the object store attribute  preservePools to False\r\n\r\n```\r\n$ cat object.yaml | grep preservePools\r\n  preservePools: false\r\n```\r\n\r\n2. Create the object store\r\n\r\n```\r\n$ kubectl create -f object.yaml\r\ncephobjectstore.ceph.rook.io/my-store created\r\n\r\n```\r\n\r\n3. Check Ceph and pool status\r\n\r\n```\r\n[root@node3 /]# ceph -s\r\n  cluster:\r\n    id:     db6b561a-c2f8-4f07-bc32-728eccdf8c74\r\n    health: HEALTH_OK\r\n\r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 19h)\r\n    mgr: a(active, since 16m)\r\n    osd: 3 osds: 3 up (since 19h), 3 in (since 19h)\r\n    rgw: 1 daemon active (my.store.a)\r\n\r\n  data:\r\n    pools:   9 pools, 48 pgs\r\n    objects: 208 objects, 6.0 KiB\r\n    usage:   3.1 GiB used, 69 GiB / 72 GiB avail\r\n    pgs:     48 active+clean\r\n\r\n  io:\r\n    client:   14 KiB/s rd, 0 B/s wr, 21 op/s rd, 14 op/s wr\r\n\r\n    [root@node3 /]# rados df\r\n    POOL_NAME                     USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS     WR USED COMPR UNDER COMPR\r\n    .rgw.root                  4.1 MiB      23      0     69                  0       0        0    101 101 KiB     73 46 KiB        0 B         0 B\r\n    my-store.rgw.buckets.data      0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\n    my-store.rgw.buckets.index     0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\n    my-store.rgw.control           0 B       8      0     24                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\n    my-store.rgw.log           192 KiB     178      0    534                  0       0        0    527 351 KiB    355  3 KiB        0 B         0 B\r\n    my-store.rgw.meta              0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B\r\n\r\n    total_objects    209\r\n    total_used       3.1 GiB\r\n    total_avail      69 GiB\r\n    total_space      72 GiB\r\n\r\n```\r\n\r\n4. Delete the object store\r\n\r\n```\r\n$ kubectl -n rook-ceph delete CephObjectStore my-store\r\ncephobjectstore.ceph.rook.io \"my-store\" deleted\r\n```\r\n\r\n5. Check Ceph and pool status\r\n\r\n```\r\n[root@node3 /]# ceph -s\r\n  cluster:\r\n    id:     db6b561a-c2f8-4f07-bc32-728eccdf8c74\r\n    health: HEALTH_OK\r\n\r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 19h)\r\n    mgr: a(active, since 17m)\r\n    osd: 3 osds: 3 up (since 19h), 3 in (since 19h)\r\n\r\n  data:\r\n    pools:   7 pools, 0 pgs\r\n    objects: 0 objects, 0 B\r\n    usage:   3.1 GiB used, 69 GiB / 72 GiB avail\r\n    pgs:\r\n\r\n[root@node3 /]# rados df\r\nPOOL_NAME USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS RD WR_OPS WR USED COMPR UNDER COMPR\r\n\r\ntotal_objects    0\r\ntotal_used       3.1 GiB\r\ntotal_avail      69 GiB\r\ntotal_space      72 GiB\r\n```\r\n\r\n\r\n",
  "changed_files": 20,
  "closed_at": "2019-10-01T07:19:56Z",
  "comments": 6,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/3974/comments",
  "commits": 1,
  "commits_url": "https://api.github.com/repos/rook/rook/pulls/3974/commits",
  "created_at": "2019-09-24T17:02:29Z",
  "deletions": 30,
  "diff_url": "https://github.com/rook/rook/pull/3974.diff",
  "draft": false,
  "head": {
    "label": "jmolmo:issue_2206",
    "ref": "issue_2206",
    "repo": {
      "archive_url": "https://api.github.com/repos/jmolmo/rook/{archive_format}{/ref}",
      "archived": false,
      "assignees_url": "https://api.github.com/repos/jmolmo/rook/assignees{/user}",
      "blobs_url": "https://api.github.com/repos/jmolmo/rook/git/blobs{/sha}",
      "branches_url": "https://api.github.com/repos/jmolmo/rook/branches{/branch}",
      "clone_url": "https://github.com/jmolmo/rook.git",
      "collaborators_url": "https://api.github.com/repos/jmolmo/rook/collaborators{/collaborator}",
      "comments_url": "https://api.github.com/repos/jmolmo/rook/comments{/number}",
      "commits_url": "https://api.github.com/repos/jmolmo/rook/commits{/sha}",
      "compare_url": "https://api.github.com/repos/jmolmo/rook/compare/{base}...{head}",
      "contents_url": "https://api.github.com/repos/jmolmo/rook/contents/{+path}",
      "contributors_url": "https://api.github.com/repos/jmolmo/rook/contributors",
      "created_at": "2018-08-27T07:01:34Z",
      "default_branch": "master",
      "deployments_url": "https://api.github.com/repos/jmolmo/rook/deployments",
      "description": "Storage Orchestration for Kubernetes",
      "disabled": false,
      "downloads_url": "https://api.github.com/repos/jmolmo/rook/downloads",
      "events_url": "https://api.github.com/repos/jmolmo/rook/events",
      "fork": true,
      "forks": 0,
      "forks_count": 0,
      "forks_url": "https://api.github.com/repos/jmolmo/rook/forks",
      "full_name": "jmolmo/rook",
      "git_commits_url": "https://api.github.com/repos/jmolmo/rook/git/commits{/sha}",
      "git_refs_url": "https://api.github.com/repos/jmolmo/rook/git/refs{/sha}",
      "git_tags_url": "https://api.github.com/repos/jmolmo/rook/git/tags{/sha}",
      "git_url": "git://github.com/jmolmo/rook.git",
      "has_downloads": true,
      "has_issues": false,
      "has_pages": false,
      "has_projects": true,
      "has_wiki": true,
      "homepage": "https://rook.io",
      "hooks_url": "https://api.github.com/repos/jmolmo/rook/hooks",
      "html_url": "https://github.com/jmolmo/rook",
      "id": 146259245,
      "issue_comment_url": "https://api.github.com/repos/jmolmo/rook/issues/comments{/number}",
      "issue_events_url": "https://api.github.com/repos/jmolmo/rook/issues/events{/number}",
      "issues_url": "https://api.github.com/repos/jmolmo/rook/issues{/number}",
      "keys_url": "https://api.github.com/repos/jmolmo/rook/keys{/key_id}",
      "labels_url": "https://api.github.com/repos/jmolmo/rook/labels{/name}",
      "language": "Go",
      "languages_url": "https://api.github.com/repos/jmolmo/rook/languages",
      "license": {
        "key": "apache-2.0",
        "name": "Apache License 2.0",
        "node_id": "MDc6TGljZW5zZTI=",
        "spdx_id": "Apache-2.0",
        "url": "https://api.github.com/licenses/apache-2.0"
      },
      "merges_url": "https://api.github.com/repos/jmolmo/rook/merges",
      "milestones_url": "https://api.github.com/repos/jmolmo/rook/milestones{/number}",
      "mirror_url": null,
      "name": "rook",
      "node_id": "MDEwOlJlcG9zaXRvcnkxNDYyNTkyNDU=",
      "notifications_url": "https://api.github.com/repos/jmolmo/rook/notifications{?since,all,participating}",
      "open_issues": 0,
      "open_issues_count": 0,
      "owner": {
        "avatar_url": "https://avatars2.githubusercontent.com/u/1820049?v=4",
        "events_url": "https://api.github.com/users/jmolmo/events{/privacy}",
        "followers_url": "https://api.github.com/users/jmolmo/followers",
        "following_url": "https://api.github.com/users/jmolmo/following{/other_user}",
        "gists_url": "https://api.github.com/users/jmolmo/gists{/gist_id}",
        "gravatar_id": "",
        "html_url": "https://github.com/jmolmo",
        "id": 1820049,
        "login": "jmolmo",
        "node_id": "MDQ6VXNlcjE4MjAwNDk=",
        "organizations_url": "https://api.github.com/users/jmolmo/orgs",
        "received_events_url": "https://api.github.com/users/jmolmo/received_events",
        "repos_url": "https://api.github.com/users/jmolmo/repos",
        "site_admin": false,
        "starred_url": "https://api.github.com/users/jmolmo/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jmolmo/subscriptions",
        "type": "User",
        "url": "https://api.github.com/users/jmolmo"
      },
      "private": false,
      "pulls_url": "https://api.github.com/repos/jmolmo/rook/pulls{/number}",
      "pushed_at": "2020-07-30T08:05:01Z",
      "releases_url": "https://api.github.com/repos/jmolmo/rook/releases{/id}",
      "size": 34360,
      "ssh_url": "git@github.com:jmolmo/rook.git",
      "stargazers_count": 0,
      "stargazers_url": "https://api.github.com/repos/jmolmo/rook/stargazers",
      "statuses_url": "https://api.github.com/repos/jmolmo/rook/statuses/{sha}",
      "subscribers_url": "https://api.github.com/repos/jmolmo/rook/subscribers",
      "subscription_url": "https://api.github.com/repos/jmolmo/rook/subscription",
      "svn_url": "https://github.com/jmolmo/rook",
      "tags_url": "https://api.github.com/repos/jmolmo/rook/tags",
      "teams_url": "https://api.github.com/repos/jmolmo/rook/teams",
      "trees_url": "https://api.github.com/repos/jmolmo/rook/git/trees{/sha}",
      "updated_at": "2020-07-28T16:07:15Z",
      "url": "https://api.github.com/repos/jmolmo/rook",
      "watchers": 0,
      "watchers_count": 0
    },
    "sha": "2de0787fc811f210f0c2ee64ff0b22e9d6fefbf0",
    "user": {
      "avatar_url": "https://avatars2.githubusercontent.com/u/1820049?v=4",
      "events_url": "https://api.github.com/users/jmolmo/events{/privacy}",
      "followers_url": "https://api.github.com/users/jmolmo/followers",
      "following_url": "https://api.github.com/users/jmolmo/following{/other_user}",
      "gists_url": "https://api.github.com/users/jmolmo/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/jmolmo",
      "id": 1820049,
      "login": "jmolmo",
      "node_id": "MDQ6VXNlcjE4MjAwNDk=",
      "organizations_url": "https://api.github.com/users/jmolmo/orgs",
      "received_events_url": "https://api.github.com/users/jmolmo/received_events",
      "repos_url": "https://api.github.com/users/jmolmo/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/jmolmo/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jmolmo/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/jmolmo"
    }
  },
  "html_url": "https://github.com/rook/rook/pull/3974",
  "id": 320872011,
  "issue_url": "https://api.github.com/repos/rook/rook/issues/3974",
  "labels": [
    {
      "color": "ef5c55",
      "default": false,
      "description": "main ceph tag",
      "id": 479456042,
      "name": "ceph",
      "node_id": "MDU6TGFiZWw0Nzk0NTYwNDI=",
      "url": "https://api.github.com/repos/rook/rook/labels/ceph"
    }
  ],
  "linked_issue_numbers": [
    2206
  ],
  "locked": false,
  "maintainer_can_modify": false,
  "merge_commit_sha": "6f4a7cf6c8f48b6e4c87bfa599f493cf206f0a46",
  "mergeable": null,
  "mergeable_state": "unknown",
  "merged": true,
  "merged_at": "2019-10-01T07:19:56Z",
  "merged_by": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "milestone": null,
  "node_id": "MDExOlB1bGxSZXF1ZXN0MzIwODcyMDEx",
  "number": 3974,
  "patch_url": "https://github.com/rook/rook/pull/3974.patch",
  "rebaseable": null,
  "requested_reviewers": [],
  "requested_teams": [],
  "review_comment_url": "https://api.github.com/repos/rook/rook/pulls/comments{/number}",
  "review_comments": 30,
  "review_comments_url": "https://api.github.com/repos/rook/rook/pulls/3974/comments",
  "state": "closed",
  "statuses_url": "https://api.github.com/repos/rook/rook/statuses/2de0787fc811f210f0c2ee64ff0b22e9d6fefbf0",
  "title": "New setting to avoid delete pools on <filesystem>/<object store> deletion",
  "updated_at": "2019-10-01T07:19:57Z",
  "url": "https://api.github.com/repos/rook/rook/pulls/3974",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/1820049?v=4",
    "events_url": "https://api.github.com/users/jmolmo/events{/privacy}",
    "followers_url": "https://api.github.com/users/jmolmo/followers",
    "following_url": "https://api.github.com/users/jmolmo/following{/other_user}",
    "gists_url": "https://api.github.com/users/jmolmo/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/jmolmo",
    "id": 1820049,
    "login": "jmolmo",
    "node_id": "MDQ6VXNlcjE4MjAwNDk=",
    "organizations_url": "https://api.github.com/users/jmolmo/orgs",
    "received_events_url": "https://api.github.com/users/jmolmo/received_events",
    "repos_url": "https://api.github.com/users/jmolmo/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/jmolmo/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/jmolmo/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/jmolmo"
  }
}