{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\nI deleted my original k8s cluster then attempted to follow the instructions here: https://rook.io/docs/rook/v1.3/ceph-disaster-recovery.html and got to the part about `cluster being in a no auth state` and have been unable to progress.\r\n\r\n**Expected behavior:**\r\n\r\nExpected osd and etc to come up at this stage.\r\n\r\n**File(s) to submit**:\r\n\r\n* Cluster CR (custom resource), typically called `cluster.yaml`, if necessary\r\n<details>\r\n  <summary>cluster.yaml</summary>\r\n\r\n```\r\napiVersion: ceph.rook.io/v1\r\nkind: CephCluster\r\nmetadata:\r\n  name: shino\r\n  namespace: rook-ceph\r\nspec:\r\n  cephVersion:\r\n    image: ceph/ceph:v15\r\n    allowUnsupported: true\r\n  dataDirHostPath: /var/lib/rook\r\n  mon:\r\n    count: 1\r\n    allowMultiplePerNode: true\r\n  placement:\r\n    all:\r\n      nodeAffinity:\r\n        requiredDuringSchedulingIgnoredDuringExecution:\r\n          nodeSelectorTerms:\r\n          - matchExpressions:\r\n            - key: role\r\n              operator: In\r\n              values:\r\n              - storage-node\r\n      tolerations:\r\n      - key: storage-node\r\n        operator: Exists\r\n  monitoring:\r\n    enabled: true\r\n    rulesNamespace: \"rook-ceph\"\r\n  dashboard:\r\n    enabled: true\r\n  crashCollector:\r\n    disable: true\r\n  # cluster level storage configuration and selection\r\n  storage:\r\n    useAllNodes: false\r\n    useAllDevices: false\r\n    nodes:\r\n    - name: \"shino\"\r\n      devices:\r\n      - name: nvme1n1\r\n      - name: sda\r\n```\r\n\r\n</details>\r\n\r\n* Operator's logs, if necessary\r\n\r\n<details>\r\n  <summary>rook-ceph-operator.log</summary>\r\n\r\n```\r\n2020-06-17 04:48:32.550748 I | rookcmd: starting Rook v1.3.5 with arguments '/usr/local/bin/rook ceph operator'\r\n2020-06-17 04:48:32.551029 I | rookcmd: flag values: --add_dir_header=false, --alsologtostderr=false, --csi-cephfs-plugin-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin.yaml, --csi-cephfs-provisioner-dep-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-dep.yaml, --csi-cephfs-provisioner-sts-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-sts.yaml, --csi-rbd-plugin-template-path=/etc/ceph-csi/rbd/csi-rbdplugin.yaml, --csi-rbd-provisioner-dep-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-dep.yaml, --csi-rbd-provisioner-sts-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-sts.yaml, --enable-discovery-daemon=true, --enable-flex-driver=false, --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-flush-frequency=5s, --log-level=INFO, --log_backtrace_at=:0, --log_dir=, --log_file=, --log_file_max_size=1800, --logtostderr=true, --master=, --mon-healthcheck-interval=45s, --mon-out-timeout=10m0s, --operator-image=, --service-account=, --skip_headers=false, --skip_log_headers=false, --stderrthreshold=2, --v=0, --vmodule=\r\n2020-06-17 04:48:32.551042 I | cephcmd: starting operator\r\n2020-06-17 04:48:32.725864 I | op-discover: rook-discover daemonset already exists, updating ...\r\n2020-06-17 04:48:32.745406 I | operator: rook-provisioner rook.io/block started using rook.io flex vendor dir\r\nI0617 04:48:32.745697       6 leaderelection.go:242] attempting to acquire leader lease  rook-ceph/rook.io-block...\r\n2020-06-17 04:48:32.746089 I | operator: rook-provisioner ceph.rook.io/block started using ceph.rook.io flex vendor dir\r\n2020-06-17 04:48:32.746118 I | operator: Watching all namespaces for cluster CRDs\r\n2020-06-17 04:48:32.746131 I | op-cluster: start watching clusters in all namespaces\r\n2020-06-17 04:48:32.746185 I | op-cluster: Enabling hotplug orchestration: ROOK_DISABLE_DEVICE_HOTPLUG=false\r\n2020-06-17 04:48:32.746218 I | operator: setting up the controller-runtime manager\r\nI0617 04:48:32.746235       6 leaderelection.go:242] attempting to acquire leader lease  rook-ceph/ceph.rook.io-block...\r\n2020-06-17 04:48:32.831233 I | op-cluster: starting cluster in namespace rook-ceph\r\n2020-06-17 04:48:32.929698 I | op-k8sutil: ROOK_CSI_ENABLE_RBD=\"true\" (env var)\r\n2020-06-17 04:48:32.933089 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS=\"true\" (env var)\r\n2020-06-17 04:48:32.936408 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION=\"false\" (default)\r\n2020-06-17 04:48:32.957696 I | op-k8sutil: ROOK_CSI_ENABLE_GRPC_METRICS=\"true\" (env var)\r\n2020-06-17 04:48:33.157705 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE=\"jaredallard/cephcsi:v2.1.0\" (env var)\r\n2020-06-17 04:48:33.357831 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE=\"jaredallard/csi-node-driver-registrar:v1.2.0\" (env var)\r\n2020-06-17 04:48:33.405394 I | operator: starting the controller-runtime manager\r\n2020-06-17 04:48:33.557440 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE=\"jaredallard/csi-provisioner:v1.4.0\" (env var)\r\n2020-06-17 04:48:33.825456 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE=\"jaredallard/csi-attacher:v2.1.0\" (env var)\r\n2020-06-17 04:48:33.958193 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE=\"jaredallard/csi-snapshotter:v1.2.2\" (env var)\r\n2020-06-17 04:48:34.158116 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH=\"/var/lib/kubelet\" (default)\r\n2020-06-17 04:48:34.425645 I | ceph-spec: \"ceph-block-pool-controller\": operator is not ready to run ceph command, cannot reconcile yet.\r\n2020-06-17 04:48:34.562597 I | ceph-csi: successfully created csi config map \"rook-ceph-csi-config\"\r\n2020-06-17 04:48:34.562690 I | ceph-csi: detecting the ceph csi image version for image \"jaredallard/cephcsi:v2.1.0\"\r\n2020-06-17 04:48:34.742440 I | ceph-spec: \"ceph-block-pool-controller\": operator is not ready to run ceph command, cannot reconcile yet.\r\n2020-06-17 04:48:37.452538 I | ceph-csi: Detected ceph CSI image version: \"v2.1.0\"\r\n2020-06-17 04:48:37.454603 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT=\"true\" (env var)\r\n2020-06-17 04:48:37.455451 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT=\"9091\" (default)\r\n2020-06-17 04:48:37.456234 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT=\"9081\" (default)\r\n2020-06-17 04:48:37.456918 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT=\"9090\" (default)\r\n2020-06-17 04:48:37.457602 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT=\"9080\" (default)\r\n2020-06-17 04:48:37.458291 I | op-k8sutil: CSI_ENABLE_SNAPSHOTTER=\"true\" (env var)\r\n2020-06-17 04:48:37.458993 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME=\"\" (env var)\r\n2020-06-17 04:48:37.459663 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME=\"\" (env var)\r\n2020-06-17 04:48:37.646504 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY=\"RollingUpdate\" (default)\r\n2020-06-17 04:48:37.846892 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY=\"RollingUpdate\" (default)\r\n2020-06-17 04:48:38.045236 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE=\"jaredallard/csi-resizer:v0.4.0\" (env var)\r\n2020-06-17 04:48:38.244320 I | op-k8sutil: CSI_LOG_LEVEL=\"\" (default)\r\n2020-06-17 04:48:38.446838 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS=\"\" (default)\r\n2020-06-17 04:48:38.646544 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY=\"\" (default)\r\n2020-06-17 04:48:39.055845 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS=\"\" (default)\r\n2020-06-17 04:48:39.246953 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY=\"\" (default)\r\n2020-06-17 04:48:39.446580 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE=\"\" (default)\r\n2020-06-17 04:48:39.646493 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE=\"\" (default)\r\n2020-06-17 04:48:40.446469 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE=\"\" (default)\r\n2020-06-17 04:48:40.629952 I | op-cluster: detecting the ceph image version for image ceph/ceph:v15...\r\n2020-06-17 04:48:40.646459 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE=\"\" (default)\r\n2020-06-17 04:48:41.856166 I | ceph-csi: CSIDriver CRD already had been registered for \"rook-ceph.rbd.csi.ceph.com\"\r\n2020-06-17 04:48:41.863995 I | ceph-csi: CSIDriver CRD already had been registered for \"rook-ceph.cephfs.csi.ceph.com\"\r\n2020-06-17 04:48:41.864027 I | ceph-csi: successfully started Ceph CSI driver(s)\r\n2020-06-17 04:48:43.643399 I | op-cluster: Detected ceph image version: \"15.2.3-0 octopus\"\r\n2020-06-17 04:48:43.646837 I | op-mon: parsing mon endpoints: a=10.43.6.158:6789\r\n2020-06-17 04:48:43.647044 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-06-17 04:48:43.647076 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\nI0617 04:48:49.003456       6 leaderelection.go:252] successfully acquired lease rook-ceph/rook.io-block\r\nI0617 04:48:49.003547       6 controller.go:780] Starting provisioner controller rook.io/block_rook-ceph-operator-f5b9bff9-bz8s2_5e4e0324-3dec-41cd-83db-951580b31e22!\r\nI0617 04:48:49.003676       6 event.go:281] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"rook-ceph\", Name:\"rook.io-block\", UID:\"1f6d2af1-8e8f-4a7d-8166-99f400e06029\", APIVersion:\"v1\", ResourceVersion:\"1766050\", FieldPath:\"\"}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-f5b9bff9-bz8s2_5e4e0324-3dec-41cd-83db-951580b31e22 became leader\r\nI0617 04:48:49.103675       6 controller.go:829] Started provisioner controller rook.io/block_rook-ceph-operator-f5b9bff9-bz8s2_5e4e0324-3dec-41cd-83db-951580b31e22!\r\nI0617 04:48:49.200290       6 leaderelection.go:252] successfully acquired lease rook-ceph/ceph.rook.io-block\r\nI0617 04:48:49.200381       6 controller.go:780] Starting provisioner controller ceph.rook.io/block_rook-ceph-operator-f5b9bff9-bz8s2_d7f25b93-baae-4247-bcbc-7db9c0e7c3df!\r\nI0617 04:48:49.200415       6 event.go:281] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"rook-ceph\", Name:\"ceph.rook.io-block\", UID:\"b2009966-fd1d-43ab-b5d9-cd63d57e14d1\", APIVersion:\"v1\", ResourceVersion:\"1766052\", FieldPath:\"\"}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-f5b9bff9-bz8s2_d7f25b93-baae-4247-bcbc-7db9c0e7c3df became leader\r\nI0617 04:48:49.700679       6 controller.go:829] Started provisioner controller ceph.rook.io/block_rook-ceph-operator-f5b9bff9-bz8s2_d7f25b93-baae-4247-bcbc-7db9c0e7c3df!\r\n2020-06-17 04:48:58.769583 E | op-cluster: failed to get ceph daemons versions. failed to run 'ceph versions: exit status 1\r\n2020-06-17 04:48:58.769601 I | op-cluster: cluster \"rook-ceph\": version \"15.2.3-0 octopus\" detected for image \"ceph/ceph:v15\"\r\n2020-06-17 04:48:58.777210 I | op-config: CephCluster \"rook-ceph\" status: \"Progressing\". \"Cluster is checking if updates are needed\"\r\n2020-06-17 04:48:58.788123 I | op-mon: start running mons\r\n2020-06-17 04:48:58.795267 I | op-mon: parsing mon endpoints: a=10.43.6.158:6789\r\n2020-06-17 04:48:58.807692 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.43.6.158:6789\"]}] data:a=10.43.6.158:6789 mapping:{\"node\":{\"a\":{\"Name\":\"shino\",\"Hostname\":\"shino\",\"Address\":\"192.168.50.207\"}}} maxMonId:0]\r\n2020-06-17 04:48:58.816554 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-06-17 04:48:58.816754 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-06-17 04:48:59.501012 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\n2020-06-17 04:48:59.581420 I | op-mon: targeting the mon count 1\r\n2020-06-17 04:49:14.658252 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\n2020-06-17 04:49:29.757738 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\n2020-06-17 04:49:45.048209 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\n2020-06-17 04:49:45.357682 W | op-mon: failed to set Rook and/or user-defined Ceph config options before starting mons; will retry after starting mons. failed to apply default Ceph configurations: failed to set one or more Ceph configs: failed to set ceph config in the centralized mon configuration database; you may need to use the rook-config-override ConfigMap. output: timed out\r\n: exit status 1: failed to set ceph config in the centralized mon configuration database; you may need to use the rook-config-override ConfigMap. output: timed out\r\n: exit status 1: failed to set ceph config in the centralized mon configuration database; you may need to use the rook-config-override ConfigMap. output: timed out\r\n: exit status 1\r\n2020-06-17 04:49:45.357699 I | op-mon: checking for basic quorum with existing mons\r\n2020-06-17 04:49:45.380727 I | op-mon: mon \"a\" endpoint are [v2:10.43.6.158:3300,v1:10.43.6.158:6789]\r\n2020-06-17 04:49:45.392395 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.43.6.158:6789\"]}] data:a=10.43.6.158:6789 mapping:{\"node\":{\"a\":{\"Name\":\"shino\",\"Hostname\":\"shino\",\"Address\":\"192.168.50.207\"}}} maxMonId:0]\r\n2020-06-17 04:49:45.400279 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-06-17 04:49:45.400450 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-06-17 04:49:45.408537 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-06-17 04:49:45.408701 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-06-17 04:49:45.422499 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed\r\n2020-06-17 04:49:45.437533 I | op-k8sutil: deployment \"rook-ceph-mon-a\" did not change, nothing to update\r\n2020-06-17 04:49:45.437579 I | op-mon: waiting for mon quorum with [a]\r\n2020-06-17 04:49:45.447066 I | op-mon: mons running: [a]\r\n2020-06-17 04:50:00.170933 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\nNOTE: Above is repeated about 100x\r\n2020-06-17 04:57:09.665900 I | op-mon: mons running: [a]\r\n2020-06-17 04:57:29.798426 I | op-mon: mons running: [a]\r\n2020-06-17 04:57:49.968373 I | op-mon: mons running: [a]\r\n2020-06-17 04:58:10.064485 I | op-mon: mons running: [a]\r\n2020-06-17 04:58:12.462804 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\n2020-06-17 04:58:27.881055 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\n2020-06-17 04:58:30.167686 I | op-mon: mons running: [a]\r\n2020-06-17 04:58:50.257722 I | op-mon: mons running: [a]\r\n2020-06-17 04:59:10.391672 I | op-mon: mons running: [a]\r\n2020-06-17 04:59:30.572791 I | op-mon: mons running: [a]\r\n2020-06-17 04:59:45.692259 E | op-cluster: failed to create cluster in namespace \"rook-ceph\". failed to start the mons: failed to start mon pods: failed to check mon quorum a: failed to wait for mon quorum: exceeded max retry count waiting for monitors to reach quorum\r\n2020-06-17 04:59:46.629922 I | op-cluster: detecting the ceph image version for image ceph/ceph:v15...\r\n2020-06-17 04:59:48.548779 I | op-cluster: Detected ceph image version: \"15.2.3-0 octopus\"\r\n2020-06-17 04:59:48.552339 I | op-mon: parsing mon endpoints: a=10.43.6.158:6789\r\n2020-06-17 04:59:48.552472 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-06-17 04:59:48.552509 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-06-17 05:00:03.627993 E | op-cluster: failed to get ceph daemons versions. failed to run 'ceph versions: exit status 1\r\n2020-06-17 05:00:03.628052 I | op-cluster: cluster \"rook-ceph\": version \"15.2.3-0 octopus\" detected for image \"ceph/ceph:v15\"\r\n2020-06-17 05:00:03.653155 I | op-config: CephCluster \"rook-ceph\" status: \"Progressing\". \"Cluster is checking if updates are needed\"\r\n2020-06-17 05:00:03.676159 I | op-mon: start running mons\r\n2020-06-17 05:00:03.683367 I | op-mon: parsing mon endpoints: a=10.43.6.158:6789\r\n2020-06-17 05:00:03.691751 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.43.6.158:6789\"]}] data:a=10.43.6.158:6789 mapping:{\"node\":{\"a\":{\"Name\":\"shino\",\"Hostname\":\"shino\",\"Address\":\"192.168.50.207\"}}} maxMonId:0]\r\n2020-06-17 05:00:03.695857 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-06-17 05:00:03.695938 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-06-17 05:00:04.266578 I | op-mon: targeting the mon count 1\r\n2020-06-17 05:00:49.760541 W | op-mon: failed to set Rook and/or user-defined Ceph config options before starting mons; will retry after starting mons. failed to apply default Ceph configurations: failed to set one or more Ceph configs: failed to set ceph config in the centralized mon configuration database; you may need to use the rook-config-override ConfigMap. output: timed out\r\n: exit status 1: failed to set ceph config in the centralized mon configuration database; you may need to use the rook-config-override ConfigMap. output: timed out\r\n: exit status 1: failed to set ceph config in the centralized mon configuration database; you may need to use the rook-config-override ConfigMap. output: timed out\r\n: exit status 1\r\n2020-06-17 05:00:49.760552 I | op-mon: checking for basic quorum with existing mons\r\n2020-06-17 05:00:49.788900 I | op-mon: mon \"a\" endpoint are [v2:10.43.6.158:3300,v1:10.43.6.158:6789]\r\n2020-06-17 05:00:49.800713 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.43.6.158:6789\"]}] data:a=10.43.6.158:6789 mapping:{\"node\":{\"a\":{\"Name\":\"shino\",\"Hostname\":\"shino\",\"Address\":\"192.168.50.207\"}}} maxMonId:0]\r\n2020-06-17 05:00:49.808815 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-06-17 05:00:49.808994 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-06-17 05:00:49.816917 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-06-17 05:00:49.817088 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-06-17 05:00:49.830436 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed\r\n2020-06-17 05:00:49.844140 I | op-k8sutil: deployment \"rook-ceph-mon-a\" did not change, nothing to update\r\n2020-06-17 05:00:49.844187 I | op-mon: waiting for mon quorum with [a]\r\n2020-06-17 05:00:49.852442 I | op-mon: mons running: [a]\r\n2020-06-17 05:01:10.199324 I | op-mon: mons running: [a]\r\n2020-06-17 05:01:11.373205 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\n2020-06-17 05:01:26.795527 E | ceph-spec: \"ceph-block-pool-controller\": ceph command error failed to get status. timed out\r\n: exit status 1\r\n2020-06-17 05:01:30.455745 I | op-mon: mons running: [a]\r\n2020-06-17 05:01:50.557823 I | op-mon: mons running: [a]\r\n2020-06-17 05:02:10.652319 I | op-mon: mons running: [a]\r\n2020-06-17 05:02:30.765566 I | op-mon: mons running: [a]\r\n```\r\n\r\n</details>\r\n\r\n* Monitor Logs\r\n\r\n<details>\r\n<summary>monitor.log</summary>\r\n\r\n```\r\ndebug 2020-06-17T04:50:28.145+0000 7fae40dc56c0  0 setuser_match_path /var/lib/ceph/mon/ceph-a/store.db owned by 167:167. set uid:gid to 167:167 (ceph:ceph)\r\ndebug 2020-06-17T04:50:28.145+0000 7fae40dc56c0  0 ceph version 15.2.3 (d289bbdec69ed7c1f516e0a093594580a76b78d0) octopus (stable), process ceph-mon, pid 1\r\ndebug 2020-06-17T04:50:28.145+0000 7fae40dc56c0  0 pidfile_write: ignore empty --pid-file\r\ndebug 2020-06-17T04:50:28.153+0000 7fae40dc56c0  0 load: jerasure load: lrc load: isa \r\ndebug 2020-06-17T04:50:28.153+0000 7fae40dc56c0  0  set rocksdb option compression = kNoCompression\r\ndebug 2020-06-17T04:50:28.153+0000 7fae40dc56c0  0  set rocksdb option level_compaction_dynamic_level_bytes = true\r\ndebug 2020-06-17T04:50:28.153+0000 7fae40dc56c0  0  set rocksdb option write_buffer_size = 33554432\r\ndebug 2020-06-17T04:50:28.153+0000 7fae40dc56c0  0  set rocksdb option compression = kNoCompression\r\ndebug 2020-06-17T04:50:28.153+0000 7fae40dc56c0  0  set rocksdb option level_compaction_dynamic_level_bytes = true\r\ndebug 2020-06-17T04:50:28.153+0000 7fae40dc56c0  0  set rocksdb option write_buffer_size = 33554432\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  1 rocksdb: do_open column families: [default]\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: RocksDB version: 6.1.2\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Git sha rocksdb_build_git_sha:@0@\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Compile date May 29 2020\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: DB SUMMARY\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: CURRENT file:  CURRENT\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: IDENTITY file:  IDENTITY\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: MANIFEST file:  MANIFEST-046283 size: 182 Bytes\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-a/store.db dir, Total Num: 2, files: 046280.sst 046282.sst \r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-a/store.db: 046284.log size: 49582 ; \r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                         Options.error_if_exists: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                       Options.create_if_missing: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                         Options.paranoid_checks: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                                     Options.env: 0x55e0e1c781a0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                                Options.info_log: 0x55e0e406e940\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                Options.max_file_opening_threads: 16\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                              Options.statistics: (nil)\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                               Options.use_fsync: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                       Options.max_log_file_size: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.max_manifest_file_size: 1073741824\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                   Options.log_file_time_to_roll: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                       Options.keep_log_file_num: 1000\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                    Options.recycle_log_file_num: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                         Options.allow_fallocate: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                        Options.allow_mmap_reads: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                       Options.allow_mmap_writes: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                        Options.use_direct_reads: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:          Options.create_missing_column_families: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                              Options.db_log_dir: \r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-a/store.db\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                Options.table_cache_numshardbits: 6\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                      Options.max_subcompactions: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.max_background_flushes: -1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                         Options.WAL_ttl_seconds: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                       Options.WAL_size_limit_MB: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.manifest_preallocation_size: 4194304\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                     Options.is_fd_close_on_exec: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                   Options.advise_random_on_open: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                    Options.db_write_buffer_size: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                    Options.write_buffer_manager: 0x55e0e4c00b40\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:         Options.access_hint_on_compaction_start: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:           Options.random_access_max_buffer_size: 1048576\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                      Options.use_adaptive_mutex: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                            Options.rate_limiter: (nil)\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                       Options.wal_recovery_mode: 2\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.enable_thread_tracking: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.enable_pipelined_write: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:         Options.allow_concurrent_memtable_write: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.write_thread_max_yield_usec: 100\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:            Options.write_thread_slow_yield_usec: 3\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                               Options.row_cache: None\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                              Options.wal_filter: None\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.avoid_flush_during_recovery: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.allow_ingest_behind: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.preserve_deletes: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.two_write_queues: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.manual_wal_flush: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.atomic_flush: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.avoid_unnecessary_blocking_io: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.max_background_jobs: 2\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.max_background_compactions: -1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.avoid_flush_during_shutdown: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.delayed_write_rate : 16777216\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.max_total_wal_size: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                   Options.stats_dump_period_sec: 600\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                 Options.stats_persist_period_sec: 600\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                 Options.stats_history_buffer_size: 1048576\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                          Options.max_open_files: -1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                          Options.bytes_per_sync: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                      Options.wal_bytes_per_sync: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:       Options.compaction_readahead_size: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Compression algorithms supported:\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: \tkZSTDNotFinalCompression supported: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: \tkZSTD supported: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: \tkXpressCompression supported: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: \tkLZ4HCCompression supported: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: \tkLZ4Compression supported: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: \tkBZip2Compression supported: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: \tkZlibCompression supported: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: \tkSnappyCompression supported: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Fast CRC32 supported: Supported on x86\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: [db/version_set.cc:3543] Recovering from manifest file: /var/lib/ceph/mon/ceph-a/store.db/MANIFEST-046283\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: [db/column_family.cc:477] --------------- Options for column family [default]:\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:           Options.merge_operator: \r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:        Options.compaction_filter: None\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:        Options.compaction_filter_factory: None\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:         Options.memtable_factory: SkipListFactory\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:            Options.table_factory: BlockBasedTable\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x55e0e3f8a130)\r\n  cache_index_and_filter_blocks: 1\r\n  cache_index_and_filter_blocks_with_high_priority: 1\r\n  pin_l0_filter_and_index_blocks_in_cache: 0\r\n  pin_top_level_index_and_filter: 1\r\n  index_type: 0\r\n  data_block_index_type: 0\r\n  data_block_hash_table_util_ratio: 0.750000\r\n  hash_index_allow_collision: 1\r\n  checksum: 1\r\n  no_block_cache: 0\r\n  block_cache: 0x55e0e3fc2610\r\n  block_cache_name: BinnedLRUCache\r\n  block_cache_options:\r\n    capacity : 536870912\r\n    num_shard_bits : 4\r\n    strict_capacity_limit : 0\r\n    high_pri_pool_ratio: 0.000\r\n  block_cache_compressed: (nil)\r\n  persistent_cache: (nil)\r\n  block_size: 4096\r\n  block_size_deviation: 10\r\n  block_restart_interval: 16\r\n  index_block_restart_interval: 1\r\n  metadata_block_size: 4096\r\n  partition_filters: 0\r\n  use_delta_encoding: 1\r\n  filter_policy: rocksdb.BuiltinBloomFilter\r\n  whole_key_filtering: 1\r\n  verify_compression: 0\r\n  read_amp_bytes_per_bit: 0\r\n  format_version: 2\r\n  enable_index_compression: 1\r\n  block_align: 0\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:        Options.write_buffer_size: 33554432\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:  Options.max_write_buffer_number: 2\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:          Options.compression: NoCompression\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.bottommost_compression: Disabled\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:       Options.prefix_extractor: nullptr\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.num_levels: 7\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:            Options.bottommost_compression_opts.window_bits: -14\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.bottommost_compression_opts.level: 32767\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:               Options.bottommost_compression_opts.strategy: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:         Options.bottommost_compression_opts.max_dict_bytes: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:         Options.bottommost_compression_opts.zstd_max_train_bytes: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.bottommost_compression_opts.enabled: false\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:            Options.compression_opts.window_bits: -14\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.compression_opts.level: 32767\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:               Options.compression_opts.strategy: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:         Options.compression_opts.zstd_max_train_bytes: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                  Options.compression_opts.enabled: false\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:              Options.level0_stop_writes_trigger: 36\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                   Options.target_file_size_base: 67108864\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:             Options.target_file_size_multiplier: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                Options.max_bytes_for_level_base: 268435456\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                    Options.max_compaction_bytes: 1677721600\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                        Options.arena_block_size: 4194304\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                Options.disable_auto_compactions: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                        Options.compaction_style: kCompactionStyleLevel\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                          Options.compaction_pri: kMinOverlappingRatio\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.compaction_options_universal.size_ratio: 1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: Options.compaction_options_fifo.allow_compaction: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                   Options.table_properties_collectors: \r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                   Options.inplace_update_support: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                 Options.inplace_update_num_locks: 10000\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:               Options.memtable_whole_key_filtering: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:   Options.memtable_huge_page_size: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                           Options.bloom_locality: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                    Options.max_successive_merges: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                Options.optimize_filters_for_hits: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                Options.paranoid_file_checks: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                Options.force_consistency_checks: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                Options.report_bg_io_stats: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb:                               Options.ttl: 0\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: [db/version_set.cc:3757] Recovered from manifest file:/var/lib/ceph/mon/ceph-a/store.db/MANIFEST-046283 succeeded,manifest_file_number is 46283, next_file_number is 46285, last_sequence is 21118261, log_number is 46279,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: [db/version_set.cc:3766] Column family [default] (ID 0), log number is 46279\r\n\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1592369428159803, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [46284]}\r\ndebug 2020-06-17T04:50:28.157+0000 7fae40dc56c0  4 rocksdb: [db/db_impl_open.cc:583] Recovering log #46284 mode 2\r\ndebug 2020-06-17T04:50:28.161+0000 7fae40dc56c0  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1592369428162991, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 46285, \"file_size\": 49966, \"table_properties\": {\"data_size\": 48994, \"index_size\": 93, \"filter_size\": 69, \"raw_key_size\": 556, \"raw_average_key_size\": 23, \"raw_value_size\": 48445, \"raw_average_value_size\": 2018, \"num_data_blocks\": 3, \"num_entries\": 24, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\"}}\r\ndebug 2020-06-17T04:50:28.161+0000 7fae40dc56c0  4 rocksdb: [db/version_set.cc:3036] Creating manifest 46286\r\n\r\ndebug 2020-06-17T04:50:28.169+0000 7fae40dc56c0  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1592369428172188, \"job\": 1, \"event\": \"recovery_finished\"}\r\ndebug 2020-06-17T04:50:28.181+0000 7fae40dc56c0  4 rocksdb: DB pointer 0x55e0e4be5200\r\ndebug 2020-06-17T04:50:28.181+0000 7fae27cdd700  4 rocksdb: [db/db_impl.cc:777] ------- DUMPING STATS -------\r\ndebug 2020-06-17T04:50:28.181+0000 7fae27cdd700  4 rocksdb: [db/db_impl.cc:778] \r\n** DB Stats **\r\nUptime(secs): 0.0 total, 0.0 interval\r\nCumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\r\nCumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\r\nCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\r\nInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\r\nInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 MB, 0.00 MB/s\r\nInterval stall: 00:00:0.000 H:M:S, 0.0 percent\r\n\r\n** Compaction Stats [default] **\r\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  L0      2/0   440.10 KB   0.5      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n  L6      1/0   26.82 MB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\r\n Sum      3/0   27.25 MB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n\r\n** Compaction Stats [default] **\r\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\nUptime(secs): 0.0 total, 0.0 interval\r\nFlush(GB): cumulative 0.000, interval 0.000\r\nAddFile(GB): cumulative 0.000, interval 0.000\r\nAddFile(Total Files): cumulative 0, interval 0\r\nAddFile(L0 Files): cumulative 0, interval 0\r\nAddFile(Keys): cumulative 0, interval 0\r\nCumulative compaction: 0.00 GB write, 1.83 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\r\nInterval compaction: 0.00 GB write, 1.83 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\r\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\r\n\r\n** File Read Latency Histogram By Level [default] **\r\n\r\n** Compaction Stats [default] **\r\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  L0      2/0   440.10 KB   0.5      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n  L6      1/0   26.82 MB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\r\n Sum      3/0   27.25 MB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\r\n\r\n** Compaction Stats [default] **\r\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\nUptime(secs): 0.0 total, 0.0 interval\r\nFlush(GB): cumulative 0.000, interval 0.000\r\nAddFile(GB): cumulative 0.000, interval 0.000\r\nAddFile(Total Files): cumulative 0, interval 0\r\nAddFile(L0 Files): cumulative 0, interval 0\r\nAddFile(Keys): cumulative 0, interval 0\r\nCumulative compaction: 0.00 GB write, 1.82 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\r\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\r\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\r\n\r\n** File Read Latency Histogram By Level [default] **\r\n\r\ndebug 2020-06-17T04:50:28.185+0000 7fae40dc56c0  0 starting mon.a rank 0 at public addrs v1:10.43.6.158:6789/0 at bind addrs [v2:10.42.0.126:3300/0,v1:10.42.0.126:6789/0] mon_data /var/lib/ceph/mon/ceph-a fsid 4ef46c76-d7f5-4797-83d5-632d05d0c4d4\r\ndebug 2020-06-17T04:50:28.189+0000 7fae40dc56c0  1 mon.a@-1(???) e2 preinit fsid 4ef46c76-d7f5-4797-83d5-632d05d0c4d4\r\ndebug 2020-06-17T04:50:28.189+0000 7fae40dc56c0  0 mon.a@-1(???).mds e1 new map\r\ndebug 2020-06-17T04:50:28.189+0000 7fae40dc56c0  0 mon.a@-1(???).mds e1 print_map\r\ne1\r\nenable_multiple, ever_enabled_multiple: 0,0\r\ncompat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2,10=snaprealm v2}\r\nlegacy client fscid: -1\r\n \r\nNo filesystems configured\r\n\r\ndebug 2020-06-17T04:50:28.189+0000 7fae40dc56c0  0 mon.a@-1(???).osd e310 crush map has features 288514051259236352, adjusting msgr requires\r\ndebug 2020-06-17T04:50:28.189+0000 7fae40dc56c0  0 mon.a@-1(???).osd e310 crush map has features 288514051259236352, adjusting msgr requires\r\ndebug 2020-06-17T04:50:28.189+0000 7fae40dc56c0  0 mon.a@-1(???).osd e310 crush map has features 3314933000852226048, adjusting msgr requires\r\ndebug 2020-06-17T04:50:28.189+0000 7fae40dc56c0  0 mon.a@-1(???).osd e310 crush map has features 288514051259236352, adjusting msgr requires\r\ncluster 2020-06-17T04:49:59.214625+0000 mon.a (mon.0) 1 : cluster [INF] mon.a is new leader, mons a in quorum (ranks 0)\r\ncluster 2020-06-17T04:49:59.214807+0000 mon.a (mon.0) 2 : cluster [DBG] monmap e2: 1 mons at {a=v1:10.43.6.158:6789/0}\r\ncluster 2020-06-17T04:49:59.215015+0000 mon.a (mon.0) 3 : cluster [DBG] fsmap \r\ncluster 2020-06-17T04:49:59.215067+0000 mon.a (mon.0) 4 : cluster [DBG] osdmap e310: 2 total, 2 up, 2 in\r\ncluster 2020-06-17T04:49:59.219508+0000 mon.a (mon.0) 5 : cluster [DBG] mgrmap e233: no daemons active (since 19m)\r\ndebug 2020-06-17T04:50:28.189+0000 7fae40dc56c0  1 mon.a@-1(???).paxosservice(auth 501..530) refresh upgraded, format 0 -> 3\r\ndebug 2020-06-17T04:50:28.201+0000 7fae40dc56c0  0 mon.a@-1(probing) e2  my rank is now 0 (was -1)\r\ndebug 2020-06-17T04:50:28.201+0000 7fae40dc56c0  1 mon.a@0(probing) e2 win_standalone_election\r\ndebug 2020-06-17T04:50:28.201+0000 7fae40dc56c0  1 paxos.0).electionLogic(23) init, last seen epoch 23, mid-election, bumping\r\ndebug 2020-06-17T04:50:28.213+0000 7fae40dc56c0  1 mon.a@0(electing) e2 collect_metadata :  no unique device id for : fallback method has no model nor serial'\r\ndebug 2020-06-17T04:50:28.213+0000 7fae40dc56c0  0 log_channel(cluster) log [INF] : mon.a is new leader, mons a in quorum (ranks 0)\r\ndebug 2020-06-17T04:50:28.213+0000 7fae40dc56c0  0 log_channel(cluster) log [DBG] : monmap e2: 1 mons at {a=v1:10.43.6.158:6789/0}\r\ndebug 2020-06-17T04:50:28.213+0000 7fae40dc56c0  0 log_channel(cluster) log [DBG] : fsmap \r\ndebug 2020-06-17T04:50:28.213+0000 7fae40dc56c0  0 log_channel(cluster) log [DBG] : osdmap e310: 2 total, 2 up, 2 in\r\ndebug 2020-06-17T04:50:28.217+0000 7fae40dc56c0  0 log_channel(cluster) log [DBG] : mgrmap e233: no daemons active (since 19m)\r\ncluster 2020-06-17T04:50:28.216907+0000 mon.a (mon.0) 1 : cluster [INF] mon.a is new leader, mons a in quorum (ranks 0)\r\ncluster 2020-06-17T04:50:28.217049+0000 mon.a (mon.0) 2 : cluster [DBG] monmap e2: 1 mons at {a=v1:10.43.6.158:6789/0}\r\ncluster 2020-06-17T04:50:28.217233+0000 mon.a (mon.0) 3 : cluster [DBG] fsmap \r\ncluster 2020-06-17T04:50:28.217289+0000 mon.a (mon.0) 4 : cluster [DBG] osdmap e310: 2 total, 2 up, 2 in\r\ncluster 2020-06-17T04:50:28.221811+0000 mon.a (mon.0) 5 : cluster [DBG] mgrmap e233: no daemons active (since 19m)\r\ndebug 2020-06-17T04:50:47.449+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:50:47.449+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:50:47.453868+0000 mon.a (mon.0) 6 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:50:47.453929+0000 mon.a (mon.0) 7 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:50:57.413+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:50:57.413+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:50:57.416134+0000 mon.a (mon.0) 8 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:50:57.416189+0000 mon.a (mon.0) 9 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:51:07.413+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:51:07.413+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:51:07.415289+0000 mon.a (mon.0) 10 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:51:07.415346+0000 mon.a (mon.0) 11 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:51:17.457+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:51:17.457+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:51:17.459699+0000 mon.a (mon.0) 12 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:51:17.459760+0000 mon.a (mon.0) 13 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:51:27.442+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:51:27.442+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:51:27.446324+0000 mon.a (mon.0) 14 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:51:27.446386+0000 mon.a (mon.0) 15 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:51:37.410+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:51:37.410+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:51:37.410971+0000 mon.a (mon.0) 16 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:51:37.411041+0000 mon.a (mon.0) 17 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:51:47.438+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:51:47.438+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:51:47.440867+0000 mon.a (mon.0) 18 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:51:47.440922+0000 mon.a (mon.0) 19 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:51:57.390+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:51:57.390+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:51:57.392746+0000 mon.a (mon.0) 20 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:51:57.392802+0000 mon.a (mon.0) 21 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:52:07.406+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:52:07.406+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:52:07.409163+0000 mon.a (mon.0) 22 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:52:07.409218+0000 mon.a (mon.0) 23 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:52:17.446+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:52:17.446+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:52:17.449677+0000 mon.a (mon.0) 24 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:52:17.449730+0000 mon.a (mon.0) 25 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:52:27.370+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:52:27.370+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:52:27.375518+0000 mon.a (mon.0) 26 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:52:27.375587+0000 mon.a (mon.0) 27 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:52:37.430+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:52:37.430+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:52:37.434899+0000 mon.a (mon.0) 28 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:52:37.434953+0000 mon.a (mon.0) 29 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:52:47.375+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:52:47.375+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:52:47.380535+0000 mon.a (mon.0) 30 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:52:47.380589+0000 mon.a (mon.0) 31 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:52:57.447+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:52:57.447+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:52:57.453210+0000 mon.a (mon.0) 32 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:52:57.453281+0000 mon.a (mon.0) 33 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:53:07.439+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:53:07.439+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:53:07.445680+0000 mon.a (mon.0) 34 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:53:07.445735+0000 mon.a (mon.0) 35 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:53:17.447+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:53:17.447+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:53:17.452763+0000 mon.a (mon.0) 36 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:53:17.452822+0000 mon.a (mon.0) 37 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:53:27.455+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:53:27.455+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:53:27.459998+0000 mon.a (mon.0) 38 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:53:27.460068+0000 mon.a (mon.0) 39 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:53:37.415+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:53:37.415+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:53:37.421069+0000 mon.a (mon.0) 40 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:53:37.421141+0000 mon.a (mon.0) 41 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:53:47.427+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:53:47.427+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:53:47.433414+0000 mon.a (mon.0) 42 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:53:47.433484+0000 mon.a (mon.0) 43 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:53:57.427+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:53:57.427+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:53:57.433246+0000 mon.a (mon.0) 44 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:53:57.433316+0000 mon.a (mon.0) 45 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:54:07.403+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:54:07.403+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:54:07.408367+0000 mon.a (mon.0) 46 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:54:07.408423+0000 mon.a (mon.0) 47 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:54:17.412+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:54:17.412+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:54:17.415877+0000 mon.a (mon.0) 48 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:54:17.415935+0000 mon.a (mon.0) 49 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:54:27.404+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:54:27.404+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:54:27.409850+0000 mon.a (mon.0) 50 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:54:27.409913+0000 mon.a (mon.0) 51 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:54:37.372+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:54:37.372+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:54:37.374735+0000 mon.a (mon.0) 52 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:54:37.374790+0000 mon.a (mon.0) 53 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:54:47.412+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:54:47.412+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:54:47.416914+0000 mon.a (mon.0) 54 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:54:47.416968+0000 mon.a (mon.0) 55 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:54:57.432+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:54:57.432+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:54:57.436015+0000 mon.a (mon.0) 56 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:54:57.436071+0000 mon.a (mon.0) 57 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:55:07.388+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:55:07.388+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:55:07.391717+0000 mon.a (mon.0) 58 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:55:07.391775+0000 mon.a (mon.0) 59 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:55:17.436+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:55:17.436+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:55:17.440089+0000 mon.a (mon.0) 60 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:55:17.440150+0000 mon.a (mon.0) 61 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:55:27.428+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:55:27.428+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:55:27.433787+0000 mon.a (mon.0) 62 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:55:27.433841+0000 mon.a (mon.0) 63 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:55:37.388+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:55:37.388+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:55:37.391894+0000 mon.a (mon.0) 64 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:55:37.391965+0000 mon.a (mon.0) 65 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:55:47.381+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:55:47.381+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:55:47.384266+0000 mon.a (mon.0) 66 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:55:47.384330+0000 mon.a (mon.0) 67 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:55:57.365+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:55:57.365+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:55:57.368518+0000 mon.a (mon.0) 68 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:55:57.368582+0000 mon.a (mon.0) 69 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:56:07.417+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:56:07.417+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:56:07.420450+0000 mon.a (mon.0) 70 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:56:07.420504+0000 mon.a (mon.0) 71 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:56:17.425+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:56:17.425+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:56:17.428356+0000 mon.a (mon.0) 72 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:56:17.428420+0000 mon.a (mon.0) 73 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:56:27.421+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:56:27.421+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:56:27.424336+0000 mon.a (mon.0) 74 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:56:27.424407+0000 mon.a (mon.0) 75 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:56:37.405+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:56:37.405+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:56:37.408601+0000 mon.a (mon.0) 76 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:56:37.408656+0000 mon.a (mon.0) 77 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:56:47.385+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:56:47.385+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:56:47.387985+0000 mon.a (mon.0) 78 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:56:47.388050+0000 mon.a (mon.0) 79 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:56:57.373+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:56:57.373+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:56:57.378088+0000 mon.a (mon.0) 80 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:56:57.378142+0000 mon.a (mon.0) 81 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:57:07.418+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:57:07.418+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:57:07.421976+0000 mon.a (mon.0) 82 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:57:07.422030+0000 mon.a (mon.0) 83 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:57:17.374+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:57:17.378+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:57:17.378686+0000 mon.a (mon.0) 84 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:57:17.378741+0000 mon.a (mon.0) 85 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:57:27.386+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:57:27.386+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:57:27.390095+0000 mon.a (mon.0) 86 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:57:27.390149+0000 mon.a (mon.0) 87 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:57:37.362+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:57:37.362+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:57:37.363008+0000 mon.a (mon.0) 88 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:57:37.363062+0000 mon.a (mon.0) 89 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:57:47.426+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:57:47.426+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:57:47.429962+0000 mon.a (mon.0) 90 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:57:47.430018+0000 mon.a (mon.0) 91 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:57:57.406+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:57:57.406+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:57:57.408109+0000 mon.a (mon.0) 92 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:57:57.408167+0000 mon.a (mon.0) 93 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:58:07.378+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:58:07.378+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:58:07.384786+0000 mon.a (mon.0) 94 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:58:07.384842+0000 mon.a (mon.0) 95 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:58:17.358+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:58:17.358+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:58:17.366019+0000 mon.a (mon.0) 96 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:58:17.366080+0000 mon.a (mon.0) 97 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:58:27.426+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:58:27.426+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:58:27.431970+0000 mon.a (mon.0) 98 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:58:27.432041+0000 mon.a (mon.0) 99 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:58:37.427+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:58:37.427+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:58:37.430681+0000 mon.a (mon.0) 100 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:58:37.430739+0000 mon.a (mon.0) 101 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:58:47.371+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:58:47.371+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:58:47.374960+0000 mon.a (mon.0) 102 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:58:47.375016+0000 mon.a (mon.0) 103 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:58:57.367+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:58:57.367+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:58:57.371305+0000 mon.a (mon.0) 104 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:58:57.371359+0000 mon.a (mon.0) 105 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:59:07.451+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:59:07.451+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:59:07.454667+0000 mon.a (mon.0) 106 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:59:07.454723+0000 mon.a (mon.0) 107 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:59:17.447+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:59:17.447+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:59:17.452822+0000 mon.a (mon.0) 108 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:59:17.452876+0000 mon.a (mon.0) 109 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:59:27.387+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:59:27.387+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:59:27.393280+0000 mon.a (mon.0) 110 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:59:27.393341+0000 mon.a (mon.0) 111 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:59:37.451+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:59:37.451+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:59:37.457071+0000 mon.a (mon.0) 112 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:59:37.457125+0000 mon.a (mon.0) 113 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:59:47.367+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:59:47.367+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:59:47.371514+0000 mon.a (mon.0) 114 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:59:47.371571+0000 mon.a (mon.0) 115 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T04:59:57.395+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T04:59:57.395+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T04:59:57.399776+0000 mon.a (mon.0) 116 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T04:59:57.399846+0000 mon.a (mon.0) 117 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:00:07.416+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:00:07.416+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:00:07.420579+0000 mon.a (mon.0) 118 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:00:07.420634+0000 mon.a (mon.0) 119 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:00:17.416+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:00:17.416+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:00:17.420986+0000 mon.a (mon.0) 120 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:00:17.421039+0000 mon.a (mon.0) 121 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:00:27.456+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:00:27.456+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:00:27.458922+0000 mon.a (mon.0) 122 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:00:27.458976+0000 mon.a (mon.0) 123 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:00:28.180+0000 7fae27cdd700  4 rocksdb: [db/db_impl.cc:777] ------- DUMPING STATS -------\r\ndebug 2020-06-17T05:00:28.180+0000 7fae27cdd700  4 rocksdb: [db/db_impl.cc:778] \r\n** DB Stats **\r\nUptime(secs): 600.0 total, 600.0 interval\r\nCumulative writes: 366 writes, 1574 keys, 366 commit groups, 1.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\r\nCumulative WAL: 366 writes, 366 syncs, 1.00 writes per sync, written: 0.00 GB, 0.00 MB/s\r\nCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\r\nInterval writes: 366 writes, 1574 keys, 366 commit groups, 1.0 writes per commit group, ingest: 2.44 MB, 0.00 MB/s\r\nInterval WAL: 366 writes, 366 syncs, 1.00 writes per sync, written: 0.00 MB, 0.00 MB/s\r\nInterval stall: 00:00:0.000 H:M:S, 0.0 percent\r\n\r\n** Compaction Stats [default] **\r\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  L0      2/0   440.10 KB   0.5      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n  L6      1/0   26.82 MB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\r\n Sum      3/0   27.25 MB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\r\n\r\n** Compaction Stats [default] **\r\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\nUptime(secs): 600.0 total, 600.0 interval\r\nFlush(GB): cumulative 0.000, interval 0.000\r\nAddFile(GB): cumulative 0.000, interval 0.000\r\nAddFile(Total Files): cumulative 0, interval 0\r\nAddFile(L0 Files): cumulative 0, interval 0\r\nAddFile(Keys): cumulative 0, interval 0\r\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\r\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\r\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\r\n\r\n** File Read Latency Histogram By Level [default] **\r\n\r\n** Compaction Stats [default] **\r\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  L0      2/0   440.10 KB   0.5      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n  L6      1/0   26.82 MB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\r\n Sum      3/0   27.25 MB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\r\n\r\n** Compaction Stats [default] **\r\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     15.7      0.00              0.00         1    0.003       0      0\r\nUptime(secs): 600.0 total, 0.0 interval\r\nFlush(GB): cumulative 0.000, interval 0.000\r\nAddFile(GB): cumulative 0.000, interval 0.000\r\nAddFile(Total Files): cumulative 0, interval 0\r\nAddFile(L0 Files): cumulative 0, interval 0\r\nAddFile(Keys): cumulative 0, interval 0\r\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\r\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\r\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\r\n\r\n** File Read Latency Histogram By Level [default] **\r\n\r\ndebug 2020-06-17T05:00:37.428+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:00:37.428+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:00:37.431066+0000 mon.a (mon.0) 124 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:00:37.431131+0000 mon.a (mon.0) 125 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:00:47.388+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:00:47.388+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:00:47.394370+0000 mon.a (mon.0) 126 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:00:47.394425+0000 mon.a (mon.0) 127 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:00:57.404+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:00:57.404+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:00:57.408053+0000 mon.a (mon.0) 128 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:00:57.408115+0000 mon.a (mon.0) 129 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:01:07.432+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:01:07.432+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:01:07.436184+0000 mon.a (mon.0) 130 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:01:07.436238+0000 mon.a (mon.0) 131 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:01:17.424+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:01:17.424+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:01:17.430586+0000 mon.a (mon.0) 132 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:01:17.430649+0000 mon.a (mon.0) 133 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:01:27.421+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:01:27.421+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:01:27.424812+0000 mon.a (mon.0) 134 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:01:27.424866+0000 mon.a (mon.0) 135 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:01:37.405+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:01:37.405+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:01:37.407731+0000 mon.a (mon.0) 136 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:01:37.407785+0000 mon.a (mon.0) 137 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:01:47.417+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:01:47.417+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:01:47.421404+0000 mon.a (mon.0) 138 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:01:47.421474+0000 mon.a (mon.0) 139 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:01:57.449+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:01:57.449+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:01:57.451751+0000 mon.a (mon.0) 140 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:01:57.451805+0000 mon.a (mon.0) 141 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:02:07.437+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:02:07.437+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:02:07.442415+0000 mon.a (mon.0) 142 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:02:07.442470+0000 mon.a (mon.0) 143 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:02:17.429+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:02:17.429+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:02:17.431866+0000 mon.a (mon.0) 144 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:02:17.431934+0000 mon.a (mon.0) 145 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\ndebug 2020-06-17T05:02:27.437+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\ndebug 2020-06-17T05:02:27.437+0000 7fae31d61700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\naudit 2020-06-17T05:02:27.442503+0000 mon.a (mon.0) 146 : audit [DBG] from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch\r\naudit 2020-06-17T05:02:27.442561+0000 mon.a (mon.0) 147 : audit [DBG] from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished\r\n```\r\n\r\n</details>",
  "closed_at": "2020-07-21T19:12:57Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "comments": 8,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/5662/comments",
  "created_at": "2020-06-17T05:10:40Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/5662/events",
  "html_url": "https://github.com/rook/rook/issues/5662",
  "id": 640145861,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/5662/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU2NDAxNDU4NjE=",
  "number": 5662,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "Failure to recover Rook Ceph cluster after original kubernetes cluster was destroyed",
  "updated_at": "2020-07-21T19:12:57Z",
  "url": "https://api.github.com/repos/rook/rook/issues/5662",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/2391349?v=4",
    "events_url": "https://api.github.com/users/jaredallard/events{/privacy}",
    "followers_url": "https://api.github.com/users/jaredallard/followers",
    "following_url": "https://api.github.com/users/jaredallard/following{/other_user}",
    "gists_url": "https://api.github.com/users/jaredallard/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/jaredallard",
    "id": 2391349,
    "login": "jaredallard",
    "node_id": "MDQ6VXNlcjIzOTEzNDk=",
    "organizations_url": "https://api.github.com/users/jaredallard/orgs",
    "received_events_url": "https://api.github.com/users/jaredallard/received_events",
    "repos_url": "https://api.github.com/users/jaredallard/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/jaredallard/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/jaredallard/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/jaredallard"
  }
}