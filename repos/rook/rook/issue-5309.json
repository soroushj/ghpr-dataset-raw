{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
      "events_url": "https://api.github.com/users/travisn/events{/privacy}",
      "followers_url": "https://api.github.com/users/travisn/followers",
      "following_url": "https://api.github.com/users/travisn/following{/other_user}",
      "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/travisn",
      "id": 1048514,
      "login": "travisn",
      "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
      "organizations_url": "https://api.github.com/users/travisn/orgs",
      "received_events_url": "https://api.github.com/users/travisn/received_events",
      "repos_url": "https://api.github.com/users/travisn/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/travisn"
    }
  ],
  "author_association": "NONE",
  "body": "* Bug Report\r\nA new feature was added in 1.3 to allow expansion of OSDs running on top of PVs\r\nhttps://github.com/rook/rook/pull/4844\r\n\r\n**Deviation from expected behavior:**\r\nModifying the\r\n`.storageClassDeviceSets.volumeClaimTemplates.spec.resources.requests.storage`\r\nby increasing the size has no effect\r\nRefer to this discussion on Slack: https://rook-io.slack.com/archives/CK9CF5H2R/p1587544293418000\r\n\r\n**Expected behavior:**\r\nI would expect\r\n- That the underlying EBS PVC was expanded\r\n- That the corresponding OSD was expanded\r\nneither happens\r\n\r\nI can manually alter the OSD PVC to increase the size. Restarting a OSD pod does not change the OSD size, but I can confirm that the PVC was changed inside the pod. \r\n```\r\nloop0                                                                                                   7:0    0  125G  0 loop\r\nnvme1n1                                                                                               259:1    0  125G  0 disk\r\n`-ceph--6658e799--4dc7--41fb--bf2c--b11784d9a729-osd--block--064e7e25--45c4--4432--818d--c4bcc3ac73a8 254:0    0  100G  0 lvm\r\n```\r\n* Cluster CR (custom resource), typically called `cluster.yaml`, if necessary\r\n* Operator's logs, if necessary\r\noperator log\r\n```\r\n2020-04-22 08:22:53.992137 I | rookcmd: starting Rook v1.3.1 with arguments '/usr/local/bin/rook ceph operator'\r\n2020-04-22 08:22:53.992217 I | rookcmd: flag values: --add_dir_header=false, --alsologtostderr=false, --csi-cephfs-plugin-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin.yaml, --csi-cephfs-provisioner-dep-template-path=/etc/ceph-cs\r\ni/cephfs/csi-cephfsplugin-provisioner-dep.yaml, --csi-cephfs-provisioner-sts-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-sts.yaml, --csi-rbd-plugin-template-path=/etc/ceph-csi/rbd/csi-rbdplugin.yaml, --csi-rbd-provi\r\nsioner-dep-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-dep.yaml, --csi-rbd-provisioner-sts-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-sts.yaml, --enable-discovery-daemon=false, --enable-flex-driver=false, -\r\n-enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-flush-frequency=5s, --log-level=INFO, --log_backtrace_at=:0, --log_dir=, --log_file=, --log_file_max_size=1800, --logtostderr=true, --master=, --mon-healthchec\r\nk-interval=30s, --mon-out-timeout=10m0s, --operator-image=, --service-account=, --skip_headers=false, --skip_log_headers=false, --stderrthreshold=2, --v=0, --vmodule=\r\n2020-04-22 08:22:53.992226 I | cephcmd: starting operator\r\n2020-04-22 08:22:54.021599 I | operator: rook-provisioner ceph.rook.io/block started using ceph.rook.io flex vendor dir\r\n2020-04-22 08:22:54.021860 I | operator: rook-provisioner rook.io/block started using rook.io flex vendor dir\r\n2020-04-22 08:22:54.021879 I | operator: Watching all namespaces for cluster CRDs\r\n2020-04-22 08:22:54.021888 I | op-cluster: start watching clusters in all namespaces\r\nI0422 08:22:54.021901       6 leaderelection.go:242] attempting to acquire leader lease  rook-ceph/ceph.rook.io-block...\r\n2020-04-22 08:22:54.021923 I | op-cluster: Enabling hotplug orchestration: ROOK_DISABLE_DEVICE_HOTPLUG=false\r\nI0422 08:22:54.022122       6 leaderelection.go:242] attempting to acquire leader lease  rook-ceph/rook.io-block...\r\n2020-04-22 08:22:54.022431 I | operator: setting up the controller-runtime manager\r\n2020-04-22 08:22:54.024875 I | op-cluster: ConfigMap \"rook-ceph-operator-config\" changes detected. Updating configurations\r\n2020-04-22 08:22:54.034514 I | op-cluster: starting cluster in namespace rook-ceph\r\n2020-04-22 08:22:55.400278 I | ceph-csi: detecting the ceph csi image version for image \"quay.io/cephcsi/cephcsi:v2.0.1\"\r\n2020-04-22 08:22:55.430649 I | operator: starting the controller-runtime manager\r\n2020-04-22 08:22:55.750405 I | ceph-spec: \"ceph-block-pool-controller\": operator is not ready to run ceph command, cannot reconcile yet.\r\n2020-04-22 08:22:55.838627 I | ceph-spec: \"ceph-file-controller\": operator is not ready to run ceph command, cannot reconcile yet.\r\n2020-04-22 08:22:55.847485 I | ceph-spec: \"ceph-block-pool-controller\": operator is not ready to run ceph command, cannot reconcile yet.\r\n2020-04-22 08:22:58.033981 I | ceph-csi: Detected ceph CSI image version: \"v2.0.1\"\r\n2020-04-22 08:22:58.063054 I | ceph-csi: successfully created csi config map \"rook-ceph-csi-config\"\r\n2020-04-22 08:23:01.414036 I | ceph-csi: CSIDriver CRD already had been registered for \"rook-ceph.rbd.csi.ceph.com\"\r\n2020-04-22 08:23:01.423207 I | ceph-csi: CSIDriver CRD already had been registered for \"rook-ceph.cephfs.csi.ceph.com\"\r\n2020-04-22 08:23:01.423225 I | operator: successfully started Ceph CSI driver(s)\r\n2020-04-22 08:23:05.852786 I | ceph-spec: \"ceph-block-pool-controller\": operator is not ready to run ceph command, cannot reconcile yet.\r\n2020-04-22 08:23:05.973163 I | ceph-spec: \"ceph-block-pool-controller\": operator is not ready to run ceph command, cannot reconcile yet.\r\n2020-04-22 08:23:05.975947 I | ceph-spec: \"ceph-file-controller\": operator is not ready to run ceph command, cannot reconcile yet.\r\n2020-04-22 08:23:07.474542 I | op-cluster: detecting the ceph image version for image ceph/ceph:v14.2.8...\r\n2020-04-22 08:23:09.554972 I | op-cluster: Detected ceph image version: \"14.2.8-0 nautilus\"\r\n2020-04-22 08:23:09.568634 I | op-mon: parsing mon endpoints: c=100.70.91.227:6789,a=100.68.141.27:6789,b=100.69.167.202:6789\r\n2020-04-22 08:23:09.569071 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-04-22 08:23:09.569163 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-04-22 08:23:09.972285 I | op-cluster: cluster \"rook-ceph\": version \"14.2.8-0 nautilus\" detected for image \"ceph/ceph:v14.2.8\"\r\n2020-04-22 08:23:10.002996 I | op-config: CephCluster \"rook-ceph\" status: \"Progressing\". \"Cluster is checking if updates are needed\"\r\n2020-04-22 08:23:10.026314 I | op-mon: start running mons\r\n2020-04-22 08:23:10.037798 I | op-mon: parsing mon endpoints: c=100.70.91.227:6789,a=100.68.141.27:6789,b=100.69.167.202:6789\r\n2020-04-22 08:23:10.052792 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"100.70.91.227:6789\",\"100.68.141.27:6789\",\"100.69.167.202:6789\"]}] data:c=100.70.91.227:6789,a=1\r\n00.68.141.27:6789,b=100.69.167.202:6789 mapping:{\"node\":{\"a\":null,\"b\":null,\"c\":null}} maxMonId:2]\r\n2020-04-22 08:23:10.078595 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-04-22 08:23:10.078703 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\nI0422 08:23:11.339283       6 leaderelection.go:252] successfully acquired lease rook-ceph/ceph.rook.io-block\r\nI0422 08:23:11.339389       6 event.go:281] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"rook-ceph\", Name:\"ceph.rook.io-block\", UID:\"69e9b767-e96f-4f97-b1d1-03fcca49ff98\", APIVersion:\"v1\", ResourceVersion:\"20638367\", FieldPath\r\n:\"\"}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-f46b86b6b-rq42w_b8addf34-53b1-4bce-b9cd-2eb739aeca4d became leader\r\nI0422 08:23:11.339436       6 controller.go:780] Starting provisioner controller ceph.rook.io/block_rook-ceph-operator-f46b86b6b-rq42w_b8addf34-53b1-4bce-b9cd-2eb739aeca4d!\r\nI0422 08:23:11.537262       6 leaderelection.go:252] successfully acquired lease rook-ceph/rook.io-block\r\nI0422 08:23:11.537304       6 event.go:281] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"rook-ceph\", Name:\"rook.io-block\", UID:\"e2fc3e17-e48c-4895-89e1-0c2064be5bf2\", APIVersion:\"v1\", ResourceVersion:\"20638368\", FieldPath:\"\"})\r\n: type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-f46b86b6b-rq42w_a5e4e0e1-bb81-4c9e-a56f-bc94564cb9fc became leader\r\nI0422 08:23:11.537340       6 controller.go:780] Starting provisioner controller rook.io/block_rook-ceph-operator-f46b86b6b-rq42w_a5e4e0e1-bb81-4c9e-a56f-bc94564cb9fc!\r\n2020-04-22 08:23:11.734506 I | op-mon: targeting the mon count 3\r\nI0422 08:23:12.539678       6 controller.go:829] Started provisioner controller ceph.rook.io/block_rook-ceph-operator-f46b86b6b-rq42w_b8addf34-53b1-4bce-b9cd-2eb739aeca4d!\r\nI0422 08:23:13.337583       6 controller.go:829] Started provisioner controller rook.io/block_rook-ceph-operator-f46b86b6b-rq42w_a5e4e0e1-bb81-4c9e-a56f-bc94564cb9fc!\r\n2020-04-22 08:23:13.668986 I | op-mon: checking for basic quorum with existing mons\r\n2020-04-22 08:23:14.337933 I | op-mon: mon \"c\" endpoint are [v2:100.70.91.227:3300,v1:100.70.91.227:6789]\r\n2020-04-22 08:23:14.938292 I | op-mon: mon \"a\" endpoint are [v2:100.68.141.27:3300,v1:100.68.141.27:6789]\r\n2020-04-22 08:23:15.537990 I | op-mon: mon \"b\" endpoint are [v2:100.69.167.202:3300,v1:100.69.167.202:6789]\r\n2020-04-22 08:23:16.534364 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"100.70.91.227:6789\",\"100.68.141.27:6789\",\"100.69.167.202:6789\"]}] data:c=100.70.91.227:6789,a=1\r\n00.68.141.27:6789,b=100.69.167.202:6789 mapping:{\"node\":{\"a\":null,\"b\":null,\"c\":null}} maxMonId:2]\r\n2020-04-22 08:23:16.887281 I | ceph-block-pool-controller: creating pool \"ec-pool\" in namespace \"rook-ceph\"\r\n2020-04-22 08:23:17.360391 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-04-22 08:23:17.360544 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-04-22 08:23:17.533788 I | op-mon: parsing mon endpoints: c=100.70.91.227:6789,a=100.68.141.27:6789,b=100.69.167.202:6789\r\n2020-04-22 08:23:17.934555 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-04-22 08:23:17.934669 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-04-22 08:23:17.950813 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed\r\n2020-04-22 08:23:17.963373 I | op-mon: checking if we can stop the deployment rook-ceph-mon-c\r\n2020-04-22 08:23:19.279998 I | op-k8sutil: updating deployment \"rook-ceph-mon-c\"\r\n2020-04-22 08:23:19.819317 I | ceph-file-controller: filesystem r2-fs already exists\r\n2020-04-22 08:23:20.697107 I | cephclient: creating EC pool ec-pool succeeded\r\n2020-04-22 08:23:21.306498 I | op-k8sutil: finished waiting for updated deployment \"rook-ceph-mon-c\"\r\n2020-04-22 08:23:21.306523 I | op-mon: checking if we can continue the deployment rook-ceph-mon-c\r\n2020-04-22 08:23:21.306539 I | op-mon: waiting for mon quorum with [c a b]\r\n2020-04-22 08:23:21.333064 I | op-mon: mons running: [c a b]\r\n2020-04-22 08:23:21.811320 I | ceph-block-pool-controller: creating pool \"r2-pool\" in namespace \"rook-ceph\"\r\n2020-04-22 08:23:21.814994 I | op-mon: Monitors in quorum: [a b c]\r\n2020-04-22 08:23:21.823410 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed\r\n2020-04-22 08:23:21.838270 I | op-mon: checking if we can stop the deployment rook-ceph-mon-a\r\n2020-04-22 08:23:23.279591 I | op-k8sutil: updating deployment \"rook-ceph-mon-a\"\r\n2020-04-22 08:23:25.283619 I | ceph-file-controller: start running mdses for filesystem \"r2-fs\"\r\n2020-04-22 08:23:25.283659 I | cephclient: getting or creating ceph auth key \"mds.r2-fs-a\"\r\n2020-04-22 08:23:25.307912 I | op-k8sutil: finished waiting for updated deployment \"rook-ceph-mon-a\"\r\n2020-04-22 08:23:25.307931 I | op-mon: checking if we can continue the deployment rook-ceph-mon-a\r\n2020-04-22 08:23:25.307943 I | op-mon: waiting for mon quorum with [c a b]\r\n2020-04-22 08:23:25.334313 I | op-mon: mons running: [c a b]\r\n2020-04-22 08:23:25.868377 I | op-mds: deployment for mds rook-ceph-mds-r2-fs-a already exists. updating if needed\r\n2020-04-22 08:23:25.870598 I | cephclient: creating replicated pool r2-pool succeeded\r\n2020-04-22 08:23:25.882959 I | op-mon: Monitors in quorum: [a b c]\r\n2020-04-22 08:23:25.884427 I | op-k8sutil: deployment \"rook-ceph-mds-r2-fs-a\" did not change, nothing to update\r\n2020-04-22 08:23:25.884446 I | cephclient: getting or creating ceph auth key \"mds.r2-fs-b\"\r\n2020-04-22 08:23:25.891539 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed\r\n2020-04-22 08:23:25.913268 I | op-mon: checking if we can stop the deployment rook-ceph-mon-b\r\n2020-04-22 08:23:26.470445 I | op-mds: deployment for mds rook-ceph-mds-r2-fs-b already exists. updating if needed\r\n2020-04-22 08:23:26.486111 I | op-k8sutil: deployment \"rook-ceph-mds-r2-fs-b\" did not change, nothing to update\r\n2020-04-22 08:23:26.486141 I | cephclient: getting or creating ceph auth key \"mds.r2-fs-c\"\r\n2020-04-22 08:23:26.886681 I | op-k8sutil: updating deployment \"rook-ceph-mon-b\"\r\n2020-04-22 08:23:26.994236 I | op-mds: deployment for mds rook-ceph-mds-r2-fs-c already exists. updating if needed\r\n2020-04-22 08:23:27.054720 I | op-k8sutil: deployment \"rook-ceph-mds-r2-fs-c\" did not change, nothing to update\r\n2020-04-22 08:23:27.054749 I | cephclient: getting or creating ceph auth key \"mds.r2-fs-d\"\r\n2020-04-22 08:23:27.540994 I | op-mds: deployment for mds rook-ceph-mds-r2-fs-d already exists. updating if needed\r\n2020-04-22 08:23:27.854552 I | op-k8sutil: deployment \"rook-ceph-mds-r2-fs-d\" did not change, nothing to update\r\n2020-04-22 08:23:27.854580 I | cephclient: getting or creating ceph auth key \"mds.r2-fs-e\"\r\n2020-04-22 08:23:28.425506 I | op-mds: deployment for mds rook-ceph-mds-r2-fs-e already exists. updating if needed\r\n2020-04-22 08:23:28.654716 I | op-k8sutil: deployment \"rook-ceph-mds-r2-fs-e\" did not change, nothing to update\r\n2020-04-22 08:23:28.654747 I | cephclient: getting or creating ceph auth key \"mds.r2-fs-f\"\r\n2020-04-22 08:23:28.917531 I | op-k8sutil: finished waiting for updated deployment \"rook-ceph-mon-b\"\r\n2020-04-22 08:23:28.917561 I | op-mon: checking if we can continue the deployment rook-ceph-mon-b\r\n2020-04-22 08:23:28.917579 I | op-mon: waiting for mon quorum with [c a b]\r\n2020-04-22 08:23:29.118312 I | op-mon: mons running: [c a b]\r\n2020-04-22 08:23:29.582845 I | op-mon: Monitors in quorum: [a b c]\r\n2020-04-22 08:23:29.582870 I | op-mon: mons created: 3\r\n2020-04-22 08:23:29.732555 I | op-mds: deployment for mds rook-ceph-mds-r2-fs-f already exists. updating if needed\r\n2020-04-22 08:23:29.746725 I | op-k8sutil: deployment \"rook-ceph-mds-r2-fs-f\" did not change, nothing to update\r\n2020-04-22 08:23:29.984899 I | op-mon: waiting for mon quorum with [c a b]\r\n2020-04-22 08:23:30.318416 I | op-mon: mons running: [c a b]\r\n2020-04-22 08:23:30.774788 I | op-mon: Monitors in quorum: [a b c]\r\n2020-04-22 08:23:30.774821 I | cephclient: getting or creating ceph auth key \"client.csi-rbd-provisioner\"\r\n2020-04-22 08:23:31.419507 I | cephclient: getting or creating ceph auth key \"client.csi-rbd-node\"\r\n2020-04-22 08:23:31.856236 I | cephclient: getting or creating ceph auth key \"client.csi-cephfs-provisioner\"\r\n2020-04-22 08:23:32.300434 I | cephclient: getting or creating ceph auth key \"client.csi-cephfs-node\"\r\n2020-04-22 08:23:33.114324 I | ceph-csi: created kubernetes csi secrets for cluster \"rook-ceph\"\r\n2020-04-22 08:23:33.114352 I | cephclient: getting or creating ceph auth key \"client.crash\"\r\n2020-04-22 08:23:33.915104 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster \"rook-ceph\"\r\n2020-04-22 08:23:34.334486 I | cephclient: successfully enabled msgr2 protocol\r\n2020-04-22 08:23:34.334517 I | op-mgr: start running mgr\r\n2020-04-22 08:23:34.334533 I | cephclient: getting or creating ceph auth key \"mgr.a\"\r\n2020-04-22 08:23:34.941282 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed\r\n2020-04-22 08:23:34.951971 I | op-k8sutil: deployment \"rook-ceph-mgr-a\" did not change, nothing to update\r\n2020-04-22 08:23:35.539292 I | op-mgr: dashboard service already exists\r\n2020-04-22 08:23:38.088957 I | op-mgr: successful modules: prometheus\r\n2020-04-22 08:23:38.119861 I | op-mgr: successful modules: crash\r\n2020-04-22 08:23:39.836755 I | op-mgr: successful modules: orchestrator modules\r\n2020-04-22 08:23:41.425591 I | op-mgr: successful modules: http bind settings\r\n2020-04-22 08:23:42.084356 I | op-mgr: the dashboard secret was already generated\r\n2020-04-22 08:23:42.084390 I | op-mgr: setting ceph dashboard \"admin\" login creds\r\n2020-04-22 08:23:42.117617 I | op-mgr: successful modules: mgr module(s) from the spec\r\n2020-04-22 08:23:42.854132 I | op-mgr: successfully set ceph dashboard creds\r\n2020-04-22 08:23:45.744862 I | op-mgr: dashboard config has changed. restarting the dashboard module.\r\n2020-04-22 08:23:45.744886 I | op-mgr: restarting the mgr module\r\n2020-04-22 08:23:48.263302 I | op-mgr: successful modules: dashboard\r\n2020-04-22 08:23:48.296275 I | op-mgr: mgr metrics service already exists\r\n2020-04-22 08:23:48.296305 I | op-mgr: starting monitoring deployment\r\nW0422 08:23:48.297188       6 client_config.go:543] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\r\n2020-04-22 08:23:48.310654 I | op-mgr: servicemonitor enabled\r\nW0422 08:23:48.313702       6 client_config.go:543] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\r\n2020-04-22 08:23:48.395579 I | op-mgr: prometheusRule deployed\r\n2020-04-22 08:23:48.395606 I | op-osd: start running osds in namespace rook-ceph\r\n2020-04-22 08:23:48.395612 I | op-osd: start provisioning the osds on pvcs, if needed\r\n2020-04-22 08:23:48.401655 I | op-osd: successfully provisioned pvc \"zone-a-0-data-lzh28\" for VolumeClaimTemplates \"data\" for storageClassDeviceSet \"zone-a\" of set 0\r\n2020-04-22 08:23:48.407227 I | op-osd: successfully provisioned pvc \"zone-b-0-data-k5c6h\" for VolumeClaimTemplates \"data\" for storageClassDeviceSet \"zone-b\" of set 0\r\n2020-04-22 08:23:48.412931 I | op-osd: successfully provisioned pvc \"zone-c-0-data-9gtht\" for VolumeClaimTemplates \"data\" for storageClassDeviceSet \"zone-c\" of set 0\r\n2020-04-22 08:23:48.436287 I | op-osd: skip OSD prepare pod creation as OSD daemon already exists for \"zone-a-0-data-lzh28\"\r\n2020-04-22 08:23:48.607416 I | op-osd: skip OSD prepare pod creation as OSD daemon already exists for \"zone-b-0-data-k5c6h\"\r\n2020-04-22 08:23:49.808337 I | op-osd: skip OSD prepare pod creation as OSD daemon already exists for \"zone-c-0-data-9gtht\"\r\n2020-04-22 08:23:50.199519 I | op-osd: start osds after provisioning is completed, if needed\r\n2020-04-22 08:23:50.400784 I | op-osd: osd orchestration status for node zone-a-0-data-lzh28 is completed\r\n2020-04-22 08:23:50.400848 I | op-osd: starting 1 osd daemons on pvc zone-a-0-data-lzh28\r\n2020-04-22 08:23:50.400867 I | cephclient: getting or creating ceph auth key \"osd.0\"\r\n2020-04-22 08:23:51.220670 I | op-osd: deployment for osd 0 already exists. updating if needed\r\n2020-04-22 08:23:51.625318 I | op-k8sutil: deployment \"rook-ceph-osd-0\" did not change, nothing to update\r\n2020-04-22 08:23:51.625350 I | op-osd: started deployment for osd 0 on pvc\r\n2020-04-22 08:23:52.006657 I | op-osd: osd orchestration status for node zone-b-0-data-k5c6h is completed\r\n2020-04-22 08:23:52.006684 I | op-osd: starting 1 osd daemons on pvc zone-b-0-data-k5c6h\r\n2020-04-22 08:23:52.006718 I | cephclient: getting or creating ceph auth key \"osd.2\"\r\n2020-04-22 08:23:52.628732 I | op-osd: deployment for osd 2 already exists. updating if needed\r\n2020-04-22 08:23:53.018304 I | op-k8sutil: deployment \"rook-ceph-osd-2\" did not change, nothing to update\r\n2020-04-22 08:23:53.018330 I | op-osd: started deployment for osd 2 on pvc\r\n2020-04-22 08:23:53.405402 I | op-osd: osd orchestration status for node zone-c-0-data-9gtht is completed\r\n2020-04-22 08:23:53.405425 I | op-osd: starting 1 osd daemons on pvc zone-c-0-data-9gtht\r\n2020-04-22 08:23:53.405436 I | cephclient: getting or creating ceph auth key \"osd.1\"\r\n2020-04-22 08:23:54.020213 I | op-osd: deployment for osd 1 already exists. updating if needed\r\n2020-04-22 08:23:54.419998 I | op-k8sutil: deployment \"rook-ceph-osd-1\" did not change, nothing to update\r\n2020-04-22 08:23:54.420026 I | op-osd: started deployment for osd 1 on pvc\r\n2020-04-22 08:23:54.806609 I | op-osd: start provisioning the osds on nodes, if needed\r\n2020-04-22 08:23:54.806636 W | op-osd: skipping osd provisioning where no dataDirHostPath is set\r\n2020-04-22 08:23:55.206143 I | op-osd: completed running osds in namespace rook-ceph\r\n2020-04-22 08:23:55.206171 I | rbd-mirror: configure rbd-mirroring with 0 workers\r\n2020-04-22 08:23:55.214372 I | rbd-mirror: no extra daemons to remove\r\n2020-04-22 08:23:55.214390 I | op-cluster: Done creating rook instance in namespace rook-ceph\r\n2020-04-22 08:23:55.221464 I | op-config: CephCluster \"rook-ceph\" status: \"Ready\". \"Cluster created successfully\"\r\n2020-04-22 08:23:55.252694 I | op-client: start watching client resources in namespace \"rook-ceph\"\r\n2020-04-22 08:23:55.252720 I | op-bucket-prov: Ceph Bucket Provisioner launched\r\n2020-04-22 08:23:55.254289 I | op-cluster: ceph status check interval is 30s\r\nI0422 08:23:55.254324       6 manager.go:118] objectbucket.io/provisioner-manager \"msg\"=\"starting provisioner\"  \"name\"=\"ceph.rook.io/bucket\"\r\n2020-04-22 08:23:55.264022 I | op-cluster: finalizer already set on cluster rook-ceph\r\n2020-04-22 09:29:34.785904 I | op-cluster: The Cluster CR has changed. diff=\u00a0\u00a0v1.ClusterSpec{\r\n\u00a0\u00a0      CephVersion: v1.CephVersionSpec{Image: \"ceph/ceph:v14.2.8\"},\r\n\u00a0\u00a0      Storage: v1.StorageScopeSpec{\r\n\u00a0\u00a0              ... // 4 identical fields\r\n\u00a0\u00a0              Selection:     v1.Selection{},\r\n\u00a0\u00a0              VolumeSources: nil,\r\n\u00a0\u00a0              StorageClassDeviceSets: []v1.StorageClassDeviceSet{\r\n\u00a0\u00a0                      {\r\n\u00a0\u00a0                              ... // 3 identical fields\r\n\u00a0\u00a0                              Placement: v1.Placement{NodeAffinity: &v1.NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{NodeSelectorTerms: []v1.NodeSelectorTerm{{MatchExpressions: []v1.NodeSelectorRequir\r\nement{{Key: \"k8s.gdo.aws/zone\", Operator: \"In\", Values: []string{\"a\"}}, {Key: \"k8s.gdo.aws/role\", Operator: \"In\", Values: []string{\"storage\"}}}}}}}, Tolerations: []v1.Toleration{{Key: \"k8s.gdo.aws/role\", Operator: \"Equal\", Value: \"stor\r\nage\", Effect: \"NoSchedule\"}}},\r\n\u00a0\u00a0                              Config:    nil,\r\n\u00a0\u00a0                              VolumeClaimTemplates: []v1.PersistentVolumeClaim{\r\n\u00a0\u00a0                                      {\r\n\u00a0\u00a0                                              TypeMeta:   v1.TypeMeta{},\r\n\u00a0\u00a0                                              ObjectMeta: v1.ObjectMeta{Name: \"data\"},\r\n\u00a0\u00a0                                              Spec: v1.PersistentVolumeClaimSpec{\r\n\u00a0\u00a0                                                      AccessModes: []v1.PersistentVolumeAccessMode{\"ReadWriteOnce\"},\r\n\u00a0\u00a0                                                      Selector:    nil,\r\n\u00a0\u00a0                                                      Resources: v1.ResourceRequirements{\r\n\u00a0\u00a0                                                              Limits:   nil,\r\n-\u00a0                                                              Requests: v1.ResourceList{s\"storage\": {i: resource.int64Amount{value: 128849018880}, Format: \"BinarySI\"}},\r\n+\u00a0                                                              Requests: v1.ResourceList{\r\n+\u00a0                                                                      s\"storage\": {i: resource.int64Amount{value: 134217728000}, s: \"125Gi\", Format: \"BinarySI\"},\r\n+\u00a0                                                              },\r\n\u00a0\u00a0                                                      },\r\n\u00a0\u00a0                                                      VolumeName:       \"\",\r\n\u00a0\u00a0                                                      StorageClassName: &\"ebs-gp2\",\r\n\u00a0\u00a0                                                      ... // 2 identical fields\r\n\u00a0\u00a0                                              },\r\n\u00a0\u00a0                                              Status: v1.PersistentVolumeClaimStatus{},\r\n\u00a0\u00a0                                      },\r\n\u00a0\u00a0                              },\r\n\u00a0\u00a0                              Portable:            true,\r\n\u00a0\u00a0                              TuneSlowDeviceClass: false,\r\n\u00a0\u00a0                      },\r\n\u00a0\u00a0                      {\r\n\u00a0\u00a0                              ... // 3 identical fields\r\n\u00a0\u00a0                              Placement: v1.Placement{NodeAffinity: &v1.NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{NodeSelectorTerms: []v1.NodeSelectorTerm{{MatchExpressions: []v1.NodeSelectorRequir\r\nement{{Key: \"k8s.gdo.aws/zone\", Operator: \"In\", Values: []string{\"b\"}}, {Key: \"k8s.gdo.aws/role\", Operator: \"In\", Values: []string{\"storage\"}}}}}}}, Tolerations: []v1.Toleration{{Key: \"k8s.gdo.aws/role\", Operator: \"Equal\", Value: \"stor\r\nage\", Effect: \"NoSchedule\"}}},\r\n\u00a0\u00a0                              Config:    nil,\r\n\u00a0\u00a0                              VolumeClaimTemplates: []v1.PersistentVolumeClaim{\r\n\u00a0\u00a0                                      {\r\n\u00a0\u00a0                                              TypeMeta:   v1.TypeMeta{},\r\n\u00a0\u00a0                                              ObjectMeta: v1.ObjectMeta{Name: \"data\"},\r\n\u00a0\u00a0                                              Spec: v1.PersistentVolumeClaimSpec{\r\n\u00a0\u00a0                                                      AccessModes: []v1.PersistentVolumeAccessMode{\"ReadWriteOnce\"},\r\n\u00a0\u00a0                                                      Selector:    nil,\r\n\u00a0\u00a0                                                      Resources: v1.ResourceRequirements{\r\n\u00a0\u00a0                                                              Limits:   nil,\r\n-\u00a0                                                              Requests: v1.ResourceList{s\"storage\": {i: resource.int64Amount{value: 128849018880}, Format: \"BinarySI\"}},\r\n+\u00a0                                                              Requests: v1.ResourceList{\r\n+\u00a0                                                                      s\"storage\": {i: resource.int64Amount{value: 134217728000}, s: \"125Gi\", Format: \"BinarySI\"},\r\n+\u00a0                                                              },\r\n\u00a0\u00a0                                                      },\r\n\u00a0\u00a0                                                      VolumeName:       \"\",\r\n\u00a0\u00a0                                                      StorageClassName: &\"ebs-gp2\",\r\n\u00a0\u00a0                                                      ... // 2 identical fields\r\n\u00a0\u00a0                                              },\r\n\u00a0\u00a0                                              Status: v1.PersistentVolumeClaimStatus{},\r\n\u00a0\u00a0                                      },\r\n\u00a0\u00a0                              },\r\n\u00a0\u00a0                              Portable:            true,\r\n\u00a0\u00a0                              TuneSlowDeviceClass: false,\r\n\u00a0\u00a0                      },\r\n\u00a0\u00a0                      {\r\n\u00a0\u00a0                              ... // 3 identical fields\r\n\u00a0\u00a0                              Placement: v1.Placement{NodeAffinity: &v1.NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{NodeSelectorTerms: []v1.NodeSelectorTerm{{MatchExpressions: []v1.NodeSelectorRequir\r\nement{{Key: \"k8s.gdo.aws/zone\", Operator: \"In\", Values: []string{\"c\"}}, {Key: \"k8s.gdo.aws/role\", Operator: \"In\", Values: []string{\"storage\"}}}}}}}, Tolerations: []v1.Toleration{{Key: \"k8s.gdo.aws/role\", Operator: \"Equal\", Value: \"stor\r\nage\", Effect: \"NoSchedule\"}}},\r\n\u00a0\u00a0                              Config:    nil,\r\n\u00a0\u00a0                              VolumeClaimTemplates: []v1.PersistentVolumeClaim{\r\n\u00a0\u00a0                                      {\r\n\u00a0\u00a0                                              TypeMeta:   v1.TypeMeta{},\r\n\u00a0\u00a0                                              ObjectMeta: v1.ObjectMeta{Name: \"data\"},\r\n\u00a0\u00a0                                              Spec: v1.PersistentVolumeClaimSpec{\r\n\u00a0\u00a0                                                      AccessModes: []v1.PersistentVolumeAccessMode{\"ReadWriteOnce\"},\r\n\u00a0\u00a0                                                      Selector:    nil,\r\n\u00a0\u00a0                                                      Resources: v1.ResourceRequirements{\r\n\u00a0\u00a0                                                              Limits:   nil,\r\n-\u00a0                                                              Requests: v1.ResourceList{s\"storage\": {i: resource.int64Amount{value: 128849018880}, Format: \"BinarySI\"}},\r\n+\u00a0                                                              Requests: v1.ResourceList{\r\n+\u00a0                                                                      s\"storage\": {i: resource.int64Amount{value: 134217728000}, s: \"125Gi\", Format: \"BinarySI\"},\r\n+\u00a0                                                              },\r\n\u00a0\u00a0                                                      },\r\n\u00a0\u00a0                                                      VolumeName:       \"\",\r\n\u00a0\u00a0                                                      StorageClassName: &\"ebs-gp2\",\r\n\u00a0\u00a0                                                      ... // 2 identical fields\r\n\u00a0\u00a0                                              },\r\n\u00a0\u00a0                                              Status: v1.PersistentVolumeClaimStatus{},\r\n\u00a0\u00a0                                      },\r\n\u00a0\u00a0                              },\r\n\u00a0\u00a0                              Portable:            true,\r\n\u00a0\u00a0                              TuneSlowDeviceClass: false,\r\n\u00a0\u00a0                      },\r\n\u00a0\u00a0              },\r\n\u00a0\u00a0      },\r\n\u00a0\u00a0      Annotations: nil,\r\n\u00a0\u00a0      Placement:   v1.PlacementSpec{\"mon\": {PodAntiAffinity: &v1.PodAntiAffinity{PreferredDuringSchedulingIgnoredDuringExecution: []v1.WeightedPodAffinityTerm{{Weight: 100, PodAffinityTerm: v1.PodAffinityTerm{LabelSelector: &v1.Label\r\nSelector{MatchExpressions: []v1.LabelSelectorRequirement{{Key: \"app\", Operator: \"In\", Values: []string{\"rook-ceph-mon\"}}}}, TopologyKey: \"failure-domain.beta.kubernetes.io/zone\"}}}}}},\r\n\u00a0\u00a0      ... // 16 identical fields\r\n\u00a0\u00a0}\r\n2020-04-22 09:29:34.785942 I | op-cluster: update event for cluster \"rook-ceph\" is supported, orchestrating update now\r\n2020-04-22 09:29:34.792646 I | op-config: CephCluster \"rook-ceph\" status: \"Updating\". \"Cluster is updating\"\r\n2020-04-22 09:29:35.204361 I | op-cluster: ceph daemons running versions are: {Mon:map[ceph version 14.2.8 (2d095e947a02261ce61424021bb43bd3022d35cb) nautilus (stable):3] Mgr:map[ceph version 14.2.8 (2d095e947a02261ce61424021bb43bd3022\r\nd35cb) nautilus (stable):1] Osd:map[ceph version 14.2.8 (2d095e947a02261ce61424021bb43bd3022d35cb) nautilus (stable):3] Rgw:map[] Mds:map[ceph version 14.2.8 (2d095e947a02261ce61424021bb43bd3022d35cb) nautilus (stable):6] RbdMirror:map\r\n[] Overall:map[ceph version 14.2.8 (2d095e947a02261ce61424021bb43bd3022d35cb) nautilus (stable):13]}\r\n2020-04-22 09:29:35.211639 I | op-config: CephCluster \"rook-ceph\" status: \"Updating\". \"Cluster is updating\"\r\n2020-04-22 09:29:35.234548 I | op-mon: start running mons\r\n2020-04-22 09:29:35.244727 I | op-mon: parsing mon endpoints: c=100.70.91.227:6789,a=100.68.141.27:6789,b=100.69.167.202:6789\r\n2020-04-22 09:29:35.260128 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"100.70.91.227:6789\",\"100.68.141.27:6789\",\"100.69.167.202:6789\"]}] data:c=100.70.91.227:6789,a=1\r\n00.68.141.27:6789,b=100.69.167.202:6789 mapping:{\"node\":{\"a\":null,\"b\":null,\"c\":null}} maxMonId:2]\r\n2020-04-22 09:29:35.284410 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-04-22 09:29:35.284523 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-04-22 09:29:36.627373 I | op-mon: targeting the mon count 3\r\n2020-04-22 09:29:38.604886 I | op-mon: checking for basic quorum with existing mons\r\n2020-04-22 09:29:38.652526 I | op-mon: mon \"c\" endpoint are [v2:100.70.91.227:3300,v1:100.70.91.227:6789]\r\n2020-04-22 09:29:38.715031 I | op-mon: mon \"a\" endpoint are [v2:100.68.141.27:3300,v1:100.68.141.27:6789]\r\n2020-04-22 09:29:39.230997 I | op-mon: mon \"b\" endpoint are [v2:100.69.167.202:3300,v1:100.69.167.202:6789]\r\n2020-04-22 09:29:39.829290 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"100.70.91.227:6789\",\"100.68.141.27:6789\",\"100.69.167.202:6789\"]}] data:c=100.70.91.227:6789,a=1\r\n00.68.141.27:6789,b=100.69.167.202:6789 mapping:{\"node\":{\"a\":null,\"b\":null,\"c\":null}} maxMonId:2]\r\n2020-04-22 09:29:40.428372 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-04-22 09:29:40.428476 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-04-22 09:29:41.228575 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-04-22 09:29:41.228698 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-04-22 09:29:41.245773 I | op-mon: deployment for mon rook-ceph-mon-c already exists. updating if needed\r\n2020-04-22 09:29:41.258568 I | op-mon: checking if we can stop the deployment rook-ceph-mon-c\r\n2020-04-22 09:29:42.051562 I | op-k8sutil: updating deployment \"rook-ceph-mon-c\"\r\n2020-04-22 09:29:44.079694 I | op-k8sutil: finished waiting for updated deployment \"rook-ceph-mon-c\"\r\n2020-04-22 09:29:44.079727 I | op-mon: checking if we can continue the deployment rook-ceph-mon-c\r\n2020-04-22 09:29:44.079747 I | op-mon: waiting for mon quorum with [c a b]\r\n2020-04-22 09:29:44.135803 I | op-mon: mons running: [c a b]\r\n2020-04-22 09:29:44.651223 I | op-mon: Monitors in quorum: [a b c]\r\n2020-04-22 09:29:44.659028 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed\r\n2020-04-22 09:29:44.669598 I | op-mon: checking if we can stop the deployment rook-ceph-mon-a\r\n2020-04-22 09:29:45.454412 I | op-k8sutil: updating deployment \"rook-ceph-mon-a\"\r\n2020-04-22 09:29:47.481387 I | op-k8sutil: finished waiting for updated deployment \"rook-ceph-mon-a\"\r\n2020-04-22 09:29:47.481413 I | op-mon: checking if we can continue the deployment rook-ceph-mon-a\r\n2020-04-22 09:29:47.481427 I | op-mon: waiting for mon quorum with [c a b]\r\n2020-04-22 09:29:47.510057 I | op-mon: mons running: [c a b]\r\n2020-04-22 09:29:48.006448 I | op-mon: Monitors in quorum: [a b c]\r\n2020-04-22 09:29:48.016764 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed\r\n2020-04-22 09:29:48.033307 I | op-mon: checking if we can stop the deployment rook-ceph-mon-b\r\n2020-04-22 09:29:48.935908 I | op-k8sutil: updating deployment \"rook-ceph-mon-b\"\r\n2020-04-22 09:29:50.961305 I | op-k8sutil: finished waiting for updated deployment \"rook-ceph-mon-b\"\r\n2020-04-22 09:29:50.961337 I | op-mon: checking if we can continue the deployment rook-ceph-mon-b\r\n2020-04-22 09:29:50.961351 I | op-mon: waiting for mon quorum with [c a b]\r\n2020-04-22 09:29:50.990235 I | op-mon: mons running: [c a b]\r\n2020-04-22 09:29:51.538133 I | op-mon: Monitors in quorum: [a b c]\r\n2020-04-22 09:29:51.538158 I | op-mon: mons created: 3\r\n2020-04-22 09:29:51.932047 I | op-mon: waiting for mon quorum with [c a b]\r\n2020-04-22 09:29:51.960593 I | op-mon: mons running: [c a b]\r\n2020-04-22 09:29:52.439216 I | op-mon: Monitors in quorum: [a b c]\r\n2020-04-22 09:29:52.439254 I | cephclient: getting or creating ceph auth key \"client.csi-rbd-provisioner\"\r\n2020-04-22 09:29:52.879049 I | cephclient: getting or creating ceph auth key \"client.csi-rbd-node\"\r\n2020-04-22 09:29:53.317914 I | cephclient: getting or creating ceph auth key \"client.csi-cephfs-provisioner\"\r\n2020-04-22 09:29:53.769172 I | cephclient: getting or creating ceph auth key \"client.csi-cephfs-node\"\r\n2020-04-22 09:29:54.267539 I | ceph-csi: created kubernetes csi secrets for cluster \"rook-ceph\"\r\n2020-04-22 09:29:54.267561 I | cephclient: getting or creating ceph auth key \"client.crash\"\r\n2020-04-22 09:29:54.721768 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster \"rook-ceph\"\r\n2020-04-22 09:29:55.117520 I | cephclient: successfully enabled msgr2 protocol\r\n2020-04-22 09:29:55.117547 I | op-mgr: start running mgr\r\n2020-04-22 09:29:55.117560 I | cephclient: getting or creating ceph auth key \"mgr.a\"\r\n2020-04-22 09:29:55.606831 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed\r\n2020-04-22 09:29:55.617688 I | op-k8sutil: deployment \"rook-ceph-mgr-a\" did not change, nothing to update\r\n2020-04-22 09:29:56.141295 I | op-mgr: dashboard service already exists\r\n2020-04-22 09:29:57.515002 I | op-mgr: successful modules: crash\r\n2020-04-22 09:29:58.550937 I | op-mgr: successful modules: prometheus\r\n2020-04-22 09:30:00.398843 I | op-mgr: successful modules: orchestrator modules\r\n2020-04-22 09:30:02.147391 I | op-mgr: successful modules: http bind settings\r\n2020-04-22 09:30:02.539221 I | op-mgr: successful modules: mgr module(s) from the spec\r\n2020-04-22 09:30:03.597140 I | op-mgr: the dashboard secret was already generated\r\n2020-04-22 09:30:03.597187 I | op-mgr: setting ceph dashboard \"admin\" login creds\r\n2020-04-22 09:30:04.342646 I | op-mgr: successfully set ceph dashboard creds\r\n2020-04-22 09:30:07.319339 I | op-mgr: dashboard config has changed. restarting the dashboard module.\r\n2020-04-22 09:30:07.319364 I | op-mgr: restarting the mgr module\r\n2020-04-22 09:30:09.698337 I | op-mgr: successful modules: dashboard\r\n2020-04-22 09:30:09.740663 I | op-mgr: mgr metrics service already exists\r\n2020-04-22 09:30:09.740693 I | op-mgr: starting monitoring deployment\r\nW0422 09:30:09.741444       6 client_config.go:543] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\r\n2020-04-22 09:30:09.754420 I | op-mgr: servicemonitor enabled\r\nW0422 09:30:09.757315       6 client_config.go:543] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\r\n2020-04-22 09:30:09.832011 I | op-mgr: prometheusRule deployed\r\n2020-04-22 09:30:09.832047 I | op-osd: start running osds in namespace rook-ceph\r\n2020-04-22 09:30:09.832057 I | op-osd: start provisioning the osds on pvcs, if needed\r\n2020-04-22 09:30:09.838329 I | op-osd: successfully provisioned pvc \"zone-a-0-data-lzh28\" for VolumeClaimTemplates \"data\" for storageClassDeviceSet \"zone-a\" of set 0\r\n2020-04-22 09:30:09.844104 I | op-osd: successfully provisioned pvc \"zone-b-0-data-k5c6h\" for VolumeClaimTemplates \"data\" for storageClassDeviceSet \"zone-b\" of set 0\r\n2020-04-22 09:30:09.850087 I | op-osd: successfully provisioned pvc \"zone-c-0-data-9gtht\" for VolumeClaimTemplates \"data\" for storageClassDeviceSet \"zone-c\" of set 0\r\n2020-04-22 09:30:09.872741 I | op-osd: skip OSD prepare pod creation as OSD daemon already exists for \"zone-a-0-data-lzh28\"\r\n2020-04-22 09:30:09.917939 I | op-osd: skip OSD prepare pod creation as OSD daemon already exists for \"zone-b-0-data-k5c6h\"\r\n2020-04-22 09:30:10.517752 I | op-osd: skip OSD prepare pod creation as OSD daemon already exists for \"zone-c-0-data-9gtht\"\r\n2020-04-22 09:30:10.906920 I | op-osd: start osds after provisioning is completed, if needed\r\n2020-04-22 09:30:11.307266 I | op-osd: osd orchestration status for node zone-a-0-data-lzh28 is completed\r\n2020-04-22 09:30:11.307291 I | op-osd: starting 1 osd daemons on pvc zone-a-0-data-lzh28\r\n2020-04-22 09:30:11.307302 I | cephclient: getting or creating ceph auth key \"osd.0\"\r\n2020-04-22 09:30:12.325859 I | op-osd: deployment for osd 0 already exists. updating if needed\r\n2020-04-22 09:30:12.722887 I | op-k8sutil: deployment \"rook-ceph-osd-0\" did not change, nothing to update\r\n2020-04-22 09:30:12.722912 I | op-osd: started deployment for osd 0 on pvc\r\n2020-04-22 09:30:12.912051 I | op-osd: osd orchestration status for node zone-b-0-data-k5c6h is completed\r\n2020-04-22 09:30:12.912133 I | op-osd: starting 1 osd daemons on pvc zone-b-0-data-k5c6h\r\n2020-04-22 09:30:12.912166 I | cephclient: getting or creating ceph auth key \"osd.2\"\r\n2020-04-22 09:30:13.420041 I | op-osd: deployment for osd 2 already exists. updating if needed\r\n2020-04-22 09:30:13.722683 I | op-k8sutil: deployment \"rook-ceph-osd-2\" did not change, nothing to update\r\n2020-04-22 09:30:13.722713 I | op-osd: started deployment for osd 2 on pvc\r\n2020-04-22 09:30:14.114560 I | op-osd: osd orchestration status for node zone-c-0-data-9gtht is completed\r\n2020-04-22 09:30:14.114584 I | op-osd: starting 1 osd daemons on pvc zone-c-0-data-9gtht\r\n2020-04-22 09:30:14.114599 I | cephclient: getting or creating ceph auth key \"osd.1\"\r\n2020-04-22 09:30:15.125136 I | op-osd: deployment for osd 1 already exists. updating if needed\r\n2020-04-22 09:30:15.522042 I | op-k8sutil: deployment \"rook-ceph-osd-1\" did not change, nothing to update\r\n2020-04-22 09:30:15.522069 I | op-osd: started deployment for osd 1 on pvc\r\n2020-04-22 09:30:15.711894 I | op-osd: start provisioning the osds on nodes, if needed\r\n2020-04-22 09:30:15.711917 W | op-osd: skipping osd provisioning where no dataDirHostPath is set\r\n2020-04-22 09:30:16.210310 I | op-osd: completed running osds in namespace rook-ceph\r\n2020-04-22 09:30:16.210339 I | rbd-mirror: configure rbd-mirroring with 0 workers\r\n2020-04-22 09:30:16.219366 I | rbd-mirror: no extra daemons to remove\r\n2020-04-22 09:30:16.219385 I | op-cluster: Done creating rook instance in namespace rook-ceph\r\n2020-04-22 09:30:16.226578 I | op-config: CephCluster \"rook-ceph\" status: \"Ready\". \"Cluster updated successfully\"\r\n2020-04-22 09:30:16.260602 I | op-cluster: succeeded updating cluster in namespace \"rook-ceph\"\r\n```\r\n\r\n**Environment**:\r\n* Cloud provider or hardware configuration: AWS\r\n* Rook version (use `rook version` inside of a Rook Pod): 1.3.1\r\n* Storage backend version (e.g. for ceph do `ceph -v`): Ceph v14.2.8\r\n* Kubernetes version (use `kubectl version`): 1.16.7\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift): Kops\r\n* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox](https://rook.io/docs/rook/master/ceph-toolbox.html)): HEALTH_OK\r\n",
  "closed_at": "2020-06-09T07:26:33Z",
  "closed_by": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "comments": 16,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/5309/comments",
  "created_at": "2020-04-22T10:23:33Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/5309/events",
  "html_url": "https://github.com/rook/rook/issues/5309",
  "id": 604639958,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    },
    {
      "color": "ef5c55",
      "default": false,
      "description": "main ceph tag",
      "id": 479456042,
      "name": "ceph",
      "node_id": "MDU6TGFiZWw0Nzk0NTYwNDI=",
      "url": "https://api.github.com/repos/rook/rook/labels/ceph"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/5309/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU2MDQ2Mzk5NTg=",
  "number": 5309,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "Operator should expand OSD PVC size when the template size is increased in the cluster CR ",
  "updated_at": "2020-06-09T07:26:33Z",
  "url": "https://api.github.com/repos/rook/rook/issues/5309",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/9726307?v=4",
    "events_url": "https://api.github.com/users/paalkr/events{/privacy}",
    "followers_url": "https://api.github.com/users/paalkr/followers",
    "following_url": "https://api.github.com/users/paalkr/following{/other_user}",
    "gists_url": "https://api.github.com/users/paalkr/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/paalkr",
    "id": 9726307,
    "login": "paalkr",
    "node_id": "MDQ6VXNlcjk3MjYzMDc=",
    "organizations_url": "https://api.github.com/users/paalkr/orgs",
    "received_events_url": "https://api.github.com/users/paalkr/received_events",
    "repos_url": "https://api.github.com/users/paalkr/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/paalkr/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/paalkr/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/paalkr"
  }
}