{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\nWhen I specified `podAntiAffinity` for `storage.StorageClassDeviceSets.placement` to distribute each OSD pods over the different nodes, it doesn't work correctly.\r\n\r\nThere were three worker nodes in my Kubernetes cluster and the DeviceSet's count is also 3. So OSD pods should be spread over each worker node. However, it wasn't.\r\n\r\nThis issue is similar to #4397, but the root cause would be different.\r\n\r\n**Expected behavior:**\r\n\r\nOSD pods should be placed as specified `placement`.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n\r\n* Install Rook.\r\n* Prepare some local persistent volumes for each worker node.\r\n* Apply the following cluster.yaml\r\n\r\n**File(s) to submit**:\r\n\r\n* cluster.yaml\r\n```yaml\r\napiVersion: ceph.rook.io/v1\r\nkind: CephCluster\r\nmetadata:\r\n  name: rook-ceph\r\n  namespace: rook-ceph\r\nspec:\r\n  dataDirHostPath: /var/lib/rook\r\n  mon:\r\n    count: 1\r\n    allowMultiplePerNode: false\r\n  cephVersion:\r\n    image: ceph/ceph:v14.2.4-20190917\r\n    allowUnsupported: false\r\n  dashboard:\r\n    enabled: false\r\n    ssl: true\r\n  network:\r\n    hostNetwork: false\r\n  storage:\r\n    storageClassDeviceSets:\r\n    - name: set1\r\n      count: 3\r\n      placement:\r\n        podAntiAffinity:\r\n          preferredDuringSchedulingIgnoredDuringExecution:\r\n          - weight: 100\r\n            podAffinityTerm:\r\n              labelSelector:\r\n                matchExpressions:\r\n                - key: app\r\n                  operator: In\r\n                  values:\r\n                  - rook-ceph-osd\r\n              topologyKey: kubernetes.io/hostname\r\n      volumeClaimTemplates:\r\n      - metadata:\r\n          name: data\r\n        spec:\r\n          resources:\r\n            requests:\r\n              storage: 5Gi\r\n          storageClassName: local-storage\r\n          volumeMode: Block\r\n          accessModes:\r\n            - ReadWriteOnce\r\n```\r\n\r\n```\r\n$ kubectl get node\r\nNAME                     STATUS   ROLES    AGE   VERSION\r\nrook-poc-control-plane   Ready    master   12m   v1.16.3\r\nrook-poc-worker          Ready    <none>   12m   v1.16.3\r\nrook-poc-worker2         Ready    <none>   12m   v1.16.3\r\nrook-poc-worker3         Ready    <none>   12m   v1.16.3\r\n```\r\n\r\nHere rook-poc-worker{,2,3} is the worker nodes.\r\n\r\n```\r\n$ kubectl get pod -n rook-ceph -o wide\r\nNAME                                    READY   STATUS    RESTARTS   AGE     IP           NODE               NOMINATED NODE   READINESS GATES\r\nrook-ceph-osd-set1-0-data-qzw9v-jmb59   1/1     Running   0          9m30s   10.64.1.2    rook-poc-worker2   <none>           <none>\r\nrook-ceph-osd-set1-1-data-g7cvh-7jd5r   1/1     Running   0          9m30s   10.64.2.3    rook-poc-worker3   <none>           <none>\r\nrook-ceph-osd-set1-2-data-j8jk8-kkmkc   1/1     Running   0          9m29s   10.64.2.4    rook-poc-worker3   <none>           <none>\r\n```\r\n\r\nAlthough all OSD Pods should be in the different worker nodes from each other, we can see\r\nan OSD pod is in rook-poc-worker2 and two OSD pods are in rook-poc-worker3.\r\n\r\n**Additional Note**\r\n\r\nI found that OSD pods are bound to the corresponding worker nodes with `nodeSelector`.\r\nThe logic to add `nodeLabel` is as follows.\r\n\r\n```\r\nfor all OSDs\r\n  Make a job to launch an OSD prepare pod.\r\n  Run this pod and bind this to a worker node.\r\n  Start an OSD pod and bind it to the node that the corresponding OSD prepare pod existed.\r\n```\r\n\r\n`placement` works well iff all OSD prepare nodes exist at the same time.\r\nHowever, it doesn't since OSD prepare pods are launched via jobs and these are ephemeral.\r\n\r\nIMHO, this problem can be solved by changing osd-prepare pods to initContainer of OSD pod.\r\nIf there is no strong reason to divide OSD prepare pod and OSD pod in OSD on PVC,\r\n\r\nI'd like to fix this problem in this way. Any suggestion? @travisn ",
  "closed_at": "2020-01-23T08:24:41Z",
  "closed_by": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "comments": 13,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/4582/comments",
  "created_at": "2019-12-23T06:10:43Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/4582/events",
  "html_url": "https://github.com/rook/rook/issues/4582",
  "id": 541593819,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/4582/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1NDE1OTM4MTk=",
  "number": 4582,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "placement doesn't work with OSD on PVC",
  "updated_at": "2020-01-23T08:24:41Z",
  "url": "https://api.github.com/repos/rook/rook/issues/4582",
  "user": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/586654?v=4",
    "events_url": "https://api.github.com/users/zoetrope/events{/privacy}",
    "followers_url": "https://api.github.com/users/zoetrope/followers",
    "following_url": "https://api.github.com/users/zoetrope/following{/other_user}",
    "gists_url": "https://api.github.com/users/zoetrope/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/zoetrope",
    "id": 586654,
    "login": "zoetrope",
    "node_id": "MDQ6VXNlcjU4NjY1NA==",
    "organizations_url": "https://api.github.com/users/zoetrope/orgs",
    "received_events_url": "https://api.github.com/users/zoetrope/received_events",
    "repos_url": "https://api.github.com/users/zoetrope/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/zoetrope/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/zoetrope/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/zoetrope"
  }
}