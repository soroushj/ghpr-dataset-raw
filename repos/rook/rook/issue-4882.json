{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\nTolerations which are configured in the Cluster CRD [as suggested in the Rook Docs](https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#placement-configuration-settings) are not included in the OSD Deployments. Instead, only (built-in?) tolerations for NoExecute conditions are included. In addition, the prepare jobs are started without any tolerations at all.\r\n\r\n**Expected behavior:**\r\nThe Tolerations I configure in the Cluster CRD are merged with the built-in NoExecute tolerations of the OSDs themselves and included in the prepare jobs.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n1. Create a k8s cluster with workers with the ``role=storage`` label and a ``role=storage:NoSchedule`` taint.\r\n2. Deploy Rook up to the Operator step\r\n3. Configure a Cluster CRD with NoSchedule tolerations for the ``role=storage`` taint and nodeAffinity for the ``role=storage`` label, [as documented here](https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#placement-configuration-settings). In addition, configure the node and pod affinity in [the storage device class](https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#storage-class-device-sets). \r\n4. Observe that the OSD prepare jobs fail to schedule: \"0/9 nodes are available: 3 node(s) had taints that the pod didn't tolerate, 6 node(s) didn't match node selector.\"\r\n5. If you remove the taint from one of the nodes, the prepare jobs will schedule and execute; all OSDs will be scheduled to that node, because the OSD deployments lack the appropriate tolerations.\r\n\r\n**File(s) to submit**:\r\n\r\n#### Shortened versions of definitions\r\n\r\nThe files have been obtained using ``kubectl get -o yaml -n rook-ceph``. The files in this section have been manually shortened to include only what I think is the most important part. Full versions below (search for  ``click to expand``). ``# [\u2026]`` marks an omission.\r\n\r\n* ``CephCluster/rook-ceph``\r\n\r\n    ```yaml\r\n    apiVersion: ceph.rook.io/v1\r\n    kind: CephCluster\r\n    metadata:\r\n      creationTimestamp: \"2020-02-17T13:55:20Z\"\r\n      generation: 3\r\n      name: rook-ceph\r\n      namespace: rook-ceph\r\n      resourceVersion: \"24610\"\r\n      selfLink: /apis/ceph.rook.io/v1/namespaces/rook-ceph/cephclusters/rook-ceph\r\n      uid: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n    spec:\r\n      cephVersion:\r\n        image: ceph/ceph:v14.2.6\r\n      # [\u2026]\r\n      placement:\r\n        all:\r\n          nodeAffinity:\r\n            requiredDuringSchedulingIgnoredDuringExecution:\r\n              nodeSelectorTerms:\r\n              - matchExpressions:\r\n                - key: role\r\n                  operator: In\r\n                  values:\r\n                  - storage\r\n          tolerations:\r\n          - effect: NoSchedule\r\n            key: role\r\n            operator: Equal\r\n            value: storage\r\n      # [\u2026]\r\n      storage:\r\n        config: null\r\n        storageClassDeviceSets:\r\n        - count: 3\r\n          name: cinder\r\n          placement:\r\n            nodeAffinity:\r\n              requiredDuringSchedulingIgnoredDuringExecution:\r\n                nodeSelectorTerms:\r\n                - matchExpressions:\r\n                  - key: role\r\n                    operator: In\r\n                    values:\r\n                    - storage\r\n            podAntiAffinity:\r\n              preferredDuringSchedulingIgnoredDuringExecution:\r\n              - podAffinityTerm:\r\n                  labelSelector:\r\n                    matchExpressions:\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd-prepare\r\n                  topologyKey: kubernetes.io/hostname\r\n                weight: 100\r\n          # [\u2026]\r\n    status:\r\n      state: Creating\r\n      version:\r\n        image: ceph/ceph:v14.2.6\r\n        version: 14.2.6-0\r\n    ```\r\n\r\n* ``job/rook-ceph-osd-prepare-cinder-0-ceph-data-6bzwc``\r\n\r\n    ```yaml\r\n    apiVersion: batch/v1\r\n    kind: Job\r\n    metadata:\r\n      creationTimestamp: \"2020-02-17T14:03:25Z\"\r\n      labels:\r\n        app: rook-ceph-osd-prepare\r\n        ceph-version: 14.2.6-0\r\n        ceph.rook.io/pvc: cinder-0-ceph-data-6bzwc\r\n        rook-version: v1.2.3\r\n        rook_cluster: rook-ceph\r\n      name: rook-ceph-osd-prepare-cinder-0-ceph-data-6bzwc\r\n      namespace: rook-ceph\r\n      ownerReferences:\r\n      - apiVersion: ceph.rook.io/v1\r\n        blockOwnerDeletion: true\r\n        kind: CephCluster\r\n        name: rook-ceph\r\n        uid: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n      resourceVersion: \"28230\"\r\n      selfLink: /apis/batch/v1/namespaces/rook-ceph/jobs/rook-ceph-osd-prepare-cinder-0-ceph-data-6bzwc\r\n      uid: 6dfb443b-1220-4b87-9a9d-34eb22b54608\r\n    spec:\r\n      # [\u2026]\r\n      template:\r\n        # [\u2026]\r\n        spec:\r\n          affinity:\r\n            nodeAffinity:\r\n              requiredDuringSchedulingIgnoredDuringExecution:\r\n                nodeSelectorTerms:\r\n                - matchExpressions:\r\n                  - key: role\r\n                    operator: In\r\n                    values:\r\n                    - storage\r\n            podAntiAffinity:\r\n              preferredDuringSchedulingIgnoredDuringExecution:\r\n              - podAffinityTerm:\r\n                  labelSelector:\r\n                    matchExpressions:\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd-prepare\r\n                  topologyKey: kubernetes.io/hostname\r\n                weight: 100\r\n          # [\u2026]\r\n          # note that there are *no* tolerations here!\r\n    status:\r\n      active: 1\r\n      startTime: \"2020-02-17T14:03:25Z\"\r\n    ```\r\n\r\n* ``deployment.apps/rook-ceph-osd-0``\r\n\r\n    ```yaml\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    metadata:\r\n      annotations:\r\n        deployment.kubernetes.io/revision: \"1\"\r\n      creationTimestamp: \"2020-02-18T07:30:25Z\"\r\n      generation: 1\r\n      labels:\r\n        app: rook-ceph-osd\r\n        ceph-osd-id: \"0\"\r\n        ceph-version: 14.2.6-0\r\n        ceph.rook.io/pvc: cinder-0-ceph-data-6bzwc\r\n        failure-domain: cinder-0-ceph-data-6bzwc\r\n        portable: \"true\"\r\n        rook-version: v1.2.3\r\n        rook_cluster: rook-ceph\r\n      name: rook-ceph-osd-0\r\n      namespace: rook-ceph\r\n      ownerReferences:\r\n      - apiVersion: ceph.rook.io/v1\r\n        blockOwnerDeletion: true\r\n        kind: CephCluster\r\n        name: rook-ceph\r\n        uid: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n      resourceVersion: \"363337\"\r\n      selfLink: /apis/apps/v1/namespaces/rook-ceph/deployments/rook-ceph-osd-0\r\n      uid: 14ae07d1-4a2f-48bc-8830-c4d045f25ff0\r\n    spec:\r\n      # [\u2026]\r\n      template:\r\n        # [\u2026]\r\n        spec:\r\n          affinity:\r\n            nodeAffinity:\r\n              requiredDuringSchedulingIgnoredDuringExecution:\r\n                nodeSelectorTerms:\r\n                - matchExpressions:\r\n                  - key: role\r\n                    operator: In\r\n                    values:\r\n                    - storage\r\n            podAntiAffinity:\r\n              preferredDuringSchedulingIgnoredDuringExecution:\r\n              - podAffinityTerm:\r\n                  labelSelector:\r\n                    matchExpressions:\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd-prepare\r\n                  topologyKey: kubernetes.io/hostname\r\n                weight: 100\r\n          # [\u2026]\r\n          tolerations:\r\n          - effect: NoExecute\r\n            key: node.kubernetes.io/unreachable\r\n            operator: Exists\r\n            tolerationSeconds: 5\r\n          # [\u2026]\r\n    status:\r\n      availableReplicas: 1\r\n      conditions:\r\n      - lastTransitionTime: \"2020-02-18T07:30:35Z\"\r\n        lastUpdateTime: \"2020-02-18T07:30:35Z\"\r\n        message: Deployment has minimum availability.\r\n        reason: MinimumReplicasAvailable\r\n        status: \"True\"\r\n        type: Available\r\n      - lastTransitionTime: \"2020-02-18T07:30:25Z\"\r\n        lastUpdateTime: \"2020-02-18T07:30:35Z\"\r\n        message: ReplicaSet \"rook-ceph-osd-0-6d568f9c84\" has successfully progressed.\r\n        reason: NewReplicaSetAvailable\r\n        status: \"True\"\r\n        type: Progressing\r\n      observedGeneration: 1\r\n      readyReplicas: 1\r\n      replicas: 1\r\n      updatedReplicas: 1\r\n    ```\r\n\r\n    To obtain this file, I removed the taint from one node. This made the OSDs schedule. \r\n\r\n#### Full versions of files\r\n\r\n* <details><summary><code>cephcluster/rook-ceph</code> (click to expand)</summary>\r\n\r\n    ```yaml\r\n    apiVersion: ceph.rook.io/v1\r\n    kind: CephCluster\r\n    metadata:\r\n      creationTimestamp: \"2020-02-17T13:55:20Z\"\r\n      generation: 3\r\n      name: rook-ceph\r\n      namespace: rook-ceph\r\n      resourceVersion: \"24610\"\r\n      selfLink: /apis/ceph.rook.io/v1/namespaces/rook-ceph/cephclusters/rook-ceph\r\n      uid: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n    spec:\r\n      cephVersion:\r\n        image: ceph/ceph:v14.2.6\r\n      crashCollector:\r\n        disable: false\r\n      dashboard: {}\r\n      dataDirHostPath: /var/lib/rook\r\n      disruptionManagement: {}\r\n      external:\r\n        enable: false\r\n      mgr:\r\n        modules:\r\n        - enabled: true\r\n          name: pg_autoscaler\r\n      mon:\r\n        count: 3\r\n        volumeClaimTemplate:\r\n          metadata:\r\n            creationTimestamp: null\r\n          spec:\r\n            resources:\r\n              requests:\r\n                storage: 10Gi\r\n            storageClassName: csi-sc-cinderplugin\r\n          status: {}\r\n      monitoring: {}\r\n      network:\r\n        hostNetwork: false\r\n        provider: \"\"\r\n        selectors: null\r\n      placement:\r\n        all:\r\n          nodeAffinity:\r\n            requiredDuringSchedulingIgnoredDuringExecution:\r\n              nodeSelectorTerms:\r\n              - matchExpressions:\r\n                - key: role\r\n                  operator: In\r\n                  values:\r\n                  - storage\r\n          tolerations:\r\n          - effect: NoSchedule\r\n            key: role\r\n            operator: Equal\r\n            value: storage\r\n      rbdMirroring:\r\n        workers: 0\r\n      removeOSDsIfOutAndSafeToRemove: true\r\n      storage:\r\n        config: null\r\n        storageClassDeviceSets:\r\n        - count: 3\r\n          name: cinder\r\n          placement:\r\n            nodeAffinity:\r\n              requiredDuringSchedulingIgnoredDuringExecution:\r\n                nodeSelectorTerms:\r\n                - matchExpressions:\r\n                  - key: role\r\n                    operator: In\r\n                    values:\r\n                    - storage\r\n            podAntiAffinity:\r\n              preferredDuringSchedulingIgnoredDuringExecution:\r\n              - podAffinityTerm:\r\n                  labelSelector:\r\n                    matchExpressions:\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd-prepare\r\n                  topologyKey: kubernetes.io/hostname\r\n                weight: 100\r\n          portable: true\r\n          resources: {}\r\n          volumeClaimTemplates:\r\n          - metadata:\r\n              creationTimestamp: null\r\n              name: ceph-data\r\n            spec:\r\n              accessModes:\r\n              - ReadWriteOnce\r\n              resources:\r\n                requests:\r\n                  storage: 90Gi\r\n              storageClassName: csi-sc-cinderplugin\r\n              volumeMode: Block\r\n            status: {}\r\n    status:\r\n      state: Creating\r\n      version:\r\n        image: ceph/ceph:v14.2.6\r\n        version: 14.2.6-0\r\n    ```\r\n    </details>\r\n\r\n- <details><summary><code>job/rook-ceph-osd-prepare-cinder-0-ceph-data-6bzwc</code> (click to expand)</summary>\r\n\r\n    ```yaml\r\n    apiVersion: batch/v1\r\n    kind: Job\r\n    metadata:\r\n      creationTimestamp: \"2020-02-17T14:03:25Z\"\r\n      labels:\r\n        app: rook-ceph-osd-prepare\r\n        ceph-version: 14.2.6-0\r\n        ceph.rook.io/pvc: cinder-0-ceph-data-6bzwc\r\n        rook-version: v1.2.3\r\n        rook_cluster: rook-ceph\r\n      name: rook-ceph-osd-prepare-cinder-0-ceph-data-6bzwc\r\n      namespace: rook-ceph\r\n      ownerReferences:\r\n      - apiVersion: ceph.rook.io/v1\r\n        blockOwnerDeletion: true\r\n        kind: CephCluster\r\n        name: rook-ceph\r\n        uid: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n      resourceVersion: \"28230\"\r\n      selfLink: /apis/batch/v1/namespaces/rook-ceph/jobs/rook-ceph-osd-prepare-cinder-0-ceph-data-6bzwc\r\n      uid: 6dfb443b-1220-4b87-9a9d-34eb22b54608\r\n    spec:\r\n      backoffLimit: 6\r\n      completions: 1\r\n      parallelism: 1\r\n      selector:\r\n        matchLabels:\r\n          controller-uid: 6dfb443b-1220-4b87-9a9d-34eb22b54608\r\n      template:\r\n        metadata:\r\n          creationTimestamp: null\r\n          labels:\r\n            app: rook-ceph-osd-prepare\r\n            ceph.rook.io/pvc: cinder-0-ceph-data-6bzwc\r\n            controller-uid: 6dfb443b-1220-4b87-9a9d-34eb22b54608\r\n            job-name: rook-ceph-osd-prepare-cinder-0-ceph-data-6bzwc\r\n            rook_cluster: rook-ceph\r\n          name: rook-ceph-osd\r\n        spec:\r\n          affinity:\r\n            nodeAffinity:\r\n              requiredDuringSchedulingIgnoredDuringExecution:\r\n                nodeSelectorTerms:\r\n                - matchExpressions:\r\n                  - key: role\r\n                    operator: In\r\n                    values:\r\n                    - storage\r\n            podAntiAffinity:\r\n              preferredDuringSchedulingIgnoredDuringExecution:\r\n              - podAffinityTerm:\r\n                  labelSelector:\r\n                    matchExpressions:\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd-prepare\r\n                  topologyKey: kubernetes.io/hostname\r\n                weight: 100\r\n          containers:\r\n          - args:\r\n            - --\r\n            - /rook/rook\r\n            - ceph\r\n            - osd\r\n            - provision\r\n            command:\r\n            - /rook/tini\r\n            env:\r\n            - name: ROOK_NODE_NAME\r\n              value: cinder-0-ceph-data-6bzwc\r\n            - name: ROOK_CLUSTER_ID\r\n              value: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n            - name: ROOK_PRIVATE_IP\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: status.podIP\r\n            - name: ROOK_PUBLIC_IP\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: status.podIP\r\n            - name: ROOK_CLUSTER_NAME\r\n              value: rook-ceph\r\n            - name: ROOK_MON_ENDPOINTS\r\n              valueFrom:\r\n                configMapKeyRef:\r\n                  key: data\r\n                  name: rook-ceph-mon-endpoints\r\n            - name: ROOK_MON_SECRET\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: mon-secret\r\n                  name: rook-ceph-mon\r\n            - name: ROOK_ADMIN_SECRET\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: admin-secret\r\n                  name: rook-ceph-mon\r\n            - name: ROOK_CONFIG_DIR\r\n              value: /var/lib/rook\r\n            - name: ROOK_CEPH_CONFIG_OVERRIDE\r\n              value: /etc/rook/config/override.conf\r\n            - name: ROOK_FSID\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: fsid\r\n                  name: rook-ceph-mon\r\n            - name: NODE_NAME\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: spec.nodeName\r\n            - name: ROOK_CRUSHMAP_HOSTNAME\r\n              value: cinder-0-ceph-data-6bzwc\r\n            - name: CEPH_VOLUME_DEBUG\r\n              value: \"1\"\r\n            - name: CEPH_VOLUME_SKIP_RESTORECON\r\n              value: \"1\"\r\n            - name: DM_DISABLE_UDEV\r\n              value: \"1\"\r\n            - name: ROOK_CEPH_VERSION\r\n              value: ceph version 14.2.6-0 nautilus\r\n            - name: ROOK_DATA_DEVICES\r\n              value: /mnt/cinder-0-ceph-data-6bzwc\r\n            - name: ROOK_PVC_BACKED_OSD\r\n              value: \"true\"\r\n            image: ceph/ceph:v14.2.6\r\n            imagePullPolicy: IfNotPresent\r\n            name: provision\r\n            resources: {}\r\n            securityContext:\r\n              privileged: true\r\n              readOnlyRootFilesystem: false\r\n              runAsNonRoot: false\r\n              runAsUser: 0\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeMounts:\r\n            - mountPath: /var/lib/rook\r\n              name: rook-data\r\n            - mountPath: /etc/ceph\r\n              name: ceph-conf-emptydir\r\n            - mountPath: /var/log/ceph\r\n              name: rook-ceph-log\r\n            - mountPath: /var/lib/ceph/crash\r\n              name: rook-ceph-crash\r\n            - mountPath: /rook\r\n              name: rook-binaries\r\n            - mountPath: /dev\r\n              name: devices\r\n            - mountPath: /run/udev\r\n              name: udev\r\n            - mountPath: /mnt\r\n              name: cinder-0-ceph-data-6bzwc-bridge\r\n          dnsPolicy: ClusterFirst\r\n          initContainers:\r\n          - args:\r\n            - copy-binaries\r\n            - --copy-to-dir\r\n            - /rook\r\n            image: rook/ceph:v1.2.3\r\n            imagePullPolicy: IfNotPresent\r\n            name: copy-bins\r\n            resources: {}\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeMounts:\r\n            - mountPath: /rook\r\n              name: rook-binaries\r\n          - args:\r\n            - -a\r\n            - /cinder-0-ceph-data-6bzwc\r\n            - /mnt/cinder-0-ceph-data-6bzwc\r\n            command:\r\n            - cp\r\n            image: ceph/ceph:v14.2.6\r\n            imagePullPolicy: IfNotPresent\r\n            name: blkdevmapper\r\n            resources: {}\r\n            securityContext:\r\n              privileged: false\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeDevices:\r\n            - devicePath: /cinder-0-ceph-data-6bzwc\r\n              name: cinder-0-ceph-data-6bzwc\r\n            volumeMounts:\r\n            - mountPath: /mnt\r\n              name: cinder-0-ceph-data-6bzwc-bridge\r\n          restartPolicy: OnFailure\r\n          schedulerName: default-scheduler\r\n          securityContext: {}\r\n          serviceAccount: rook-ceph-osd\r\n          serviceAccountName: rook-ceph-osd\r\n          terminationGracePeriodSeconds: 30\r\n          volumes:\r\n          - hostPath:\r\n              path: /var/lib/rook\r\n              type: \"\"\r\n            name: rook-data\r\n          - emptyDir: {}\r\n            name: ceph-conf-emptydir\r\n          - hostPath:\r\n              path: /var/lib/rook/rook-ceph/log\r\n              type: \"\"\r\n            name: rook-ceph-log\r\n          - hostPath:\r\n              path: /var/lib/rook/rook-ceph/crash\r\n              type: \"\"\r\n            name: rook-ceph-crash\r\n          - emptyDir: {}\r\n            name: rook-binaries\r\n          - hostPath:\r\n              path: /dev\r\n              type: \"\"\r\n            name: devices\r\n          - hostPath:\r\n              path: /run/udev\r\n              type: \"\"\r\n            name: udev\r\n          - name: cinder-0-ceph-data-6bzwc\r\n            persistentVolumeClaim:\r\n              claimName: cinder-0-ceph-data-6bzwc\r\n          - emptyDir:\r\n              medium: Memory\r\n            name: cinder-0-ceph-data-6bzwc-bridge\r\n    status:\r\n      active: 1\r\n      startTime: \"2020-02-17T14:03:25Z\"\r\n    ```\r\n\r\n    </details>\r\n\r\n- <details><summary><code>deployment.apps/rook-ceph-osd-0</code> (click to expand)</summary>\r\n\r\n    ```yaml\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    metadata:\r\n      annotations:\r\n        deployment.kubernetes.io/revision: \"1\"\r\n      creationTimestamp: \"2020-02-18T07:30:25Z\"\r\n      generation: 1\r\n      labels:\r\n        app: rook-ceph-osd\r\n        ceph-osd-id: \"0\"\r\n        ceph-version: 14.2.6-0\r\n        ceph.rook.io/pvc: cinder-0-ceph-data-6bzwc\r\n        failure-domain: cinder-0-ceph-data-6bzwc\r\n        portable: \"true\"\r\n        rook-version: v1.2.3\r\n        rook_cluster: rook-ceph\r\n      name: rook-ceph-osd-0\r\n      namespace: rook-ceph\r\n      ownerReferences:\r\n      - apiVersion: ceph.rook.io/v1\r\n        blockOwnerDeletion: true\r\n        kind: CephCluster\r\n        name: rook-ceph\r\n        uid: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n      resourceVersion: \"363337\"\r\n      selfLink: /apis/apps/v1/namespaces/rook-ceph/deployments/rook-ceph-osd-0\r\n      uid: 14ae07d1-4a2f-48bc-8830-c4d045f25ff0\r\n    spec:\r\n      progressDeadlineSeconds: 600\r\n      replicas: 1\r\n      revisionHistoryLimit: 10\r\n      selector:\r\n        matchLabels:\r\n          app: rook-ceph-osd\r\n          ceph-osd-id: \"0\"\r\n          rook_cluster: rook-ceph\r\n      strategy:\r\n        type: Recreate\r\n      template:\r\n        metadata:\r\n          creationTimestamp: null\r\n          labels:\r\n            app: rook-ceph-osd\r\n            ceph-osd-id: \"0\"\r\n            ceph.rook.io/pvc: cinder-0-ceph-data-6bzwc\r\n            failure-domain: cinder-0-ceph-data-6bzwc\r\n            portable: \"true\"\r\n            rook_cluster: rook-ceph\r\n          name: rook-ceph-osd\r\n        spec:\r\n          affinity:\r\n            nodeAffinity:\r\n              requiredDuringSchedulingIgnoredDuringExecution:\r\n                nodeSelectorTerms:\r\n                - matchExpressions:\r\n                  - key: role\r\n                    operator: In\r\n                    values:\r\n                    - storage\r\n            podAntiAffinity:\r\n              preferredDuringSchedulingIgnoredDuringExecution:\r\n              - podAffinityTerm:\r\n                  labelSelector:\r\n                    matchExpressions:\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd\r\n                    - key: app\r\n                      operator: In\r\n                      values:\r\n                      - rook-ceph-osd-prepare\r\n                  topologyKey: kubernetes.io/hostname\r\n                weight: 100\r\n          containers:\r\n          - args:\r\n            - --\r\n            - /rook/rook\r\n            - ceph\r\n            - osd\r\n            - start\r\n            - --\r\n            - --foreground\r\n            - --id\r\n            - \"0\"\r\n            - --fsid\r\n            - 2410e0f2-3eea-4b96-9ac0-80d72698984c\r\n            - --cluster\r\n            - ceph\r\n            - --setuser\r\n            - ceph\r\n            - --setgroup\r\n            - ceph\r\n            - --setuser-match-path\r\n            - /var/lib/rook/osd0\r\n            - --crush-location=root=default host=cinder-0-ceph-data-6bzwc region=f1a zone=AZ1\r\n            - --default-log-to-file\r\n            - \"false\"\r\n            - --ms-learn-addr-from-peer=false\r\n            command:\r\n            - /rook/tini\r\n            env:\r\n            - name: ROOK_NODE_NAME\r\n              value: cinder-0-ceph-data-6bzwc\r\n            - name: ROOK_CLUSTER_ID\r\n              value: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n            - name: ROOK_PRIVATE_IP\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: status.podIP\r\n            - name: ROOK_PUBLIC_IP\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: status.podIP\r\n            - name: ROOK_CLUSTER_NAME\r\n              value: rook-ceph\r\n            - name: ROOK_MON_ENDPOINTS\r\n              valueFrom:\r\n                configMapKeyRef:\r\n                  key: data\r\n                  name: rook-ceph-mon-endpoints\r\n            - name: ROOK_MON_SECRET\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: mon-secret\r\n                  name: rook-ceph-mon\r\n            - name: ROOK_ADMIN_SECRET\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: admin-secret\r\n                  name: rook-ceph-mon\r\n            - name: ROOK_CONFIG_DIR\r\n              value: /var/lib/rook\r\n            - name: ROOK_CEPH_CONFIG_OVERRIDE\r\n              value: /etc/rook/config/override.conf\r\n            - name: ROOK_FSID\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: fsid\r\n                  name: rook-ceph-mon\r\n            - name: NODE_NAME\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: spec.nodeName\r\n            - name: ROOK_CRUSHMAP_HOSTNAME\r\n              value: cinder-0-ceph-data-6bzwc\r\n            - name: CEPH_VOLUME_DEBUG\r\n              value: \"1\"\r\n            - name: CEPH_VOLUME_SKIP_RESTORECON\r\n              value: \"1\"\r\n            - name: DM_DISABLE_UDEV\r\n              value: \"1\"\r\n            - name: TINI_SUBREAPER\r\n            - name: CONTAINER_IMAGE\r\n              value: ceph/ceph:v14.2.6\r\n            - name: POD_NAME\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: metadata.name\r\n            - name: POD_NAMESPACE\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: metadata.namespace\r\n            - name: NODE_NAME\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: spec.nodeName\r\n            - name: POD_MEMORY_LIMIT\r\n              valueFrom:\r\n                resourceFieldRef:\r\n                  divisor: \"0\"\r\n                  resource: limits.memory\r\n            - name: POD_MEMORY_REQUEST\r\n              valueFrom:\r\n                resourceFieldRef:\r\n                  divisor: \"0\"\r\n                  resource: requests.memory\r\n            - name: POD_CPU_LIMIT\r\n              valueFrom:\r\n                resourceFieldRef:\r\n                  divisor: \"1\"\r\n                  resource: limits.cpu\r\n            - name: POD_CPU_REQUEST\r\n              valueFrom:\r\n                resourceFieldRef:\r\n                  divisor: \"0\"\r\n                  resource: requests.cpu\r\n            - name: ROOK_OSD_UUID\r\n              value: 6627d1bb-6a8a-4794-8934-0fbbb1e52e3f\r\n            - name: ROOK_OSD_ID\r\n              value: \"0\"\r\n            - name: ROOK_OSD_STORE_TYPE\r\n              value: bluestore\r\n            - name: ROOK_CEPH_MON_HOST\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: mon_host\r\n                  name: rook-ceph-config\r\n            - name: CEPH_ARGS\r\n              value: -m $(ROOK_CEPH_MON_HOST)\r\n            - name: ROOK_PVC_BACKED_OSD\r\n              value: \"true\"\r\n            - name: ROOK_LV_PATH\r\n              value: /dev/ceph-acda6256-a2a1-48bf-b024-0190abd171a5/osd-block-6627d1bb-6a8a-4794-8934-0fbbb1e52e3f\r\n            - name: ROOK_LV_BACKED_PV\r\n              value: \"false\"\r\n            image: ceph/ceph:v14.2.6\r\n            imagePullPolicy: IfNotPresent\r\n            name: osd\r\n            resources: {}\r\n            securityContext:\r\n              privileged: true\r\n              readOnlyRootFilesystem: false\r\n              runAsUser: 0\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeMounts:\r\n            - mountPath: /var/lib/rook\r\n              name: rook-data\r\n            - mountPath: /etc/ceph\r\n              name: rook-config-override\r\n              readOnly: true\r\n            - mountPath: /var/log/ceph\r\n              name: rook-ceph-log\r\n            - mountPath: /var/lib/ceph/crash\r\n              name: rook-ceph-crash\r\n            - mountPath: /dev\r\n              name: devices\r\n            - mountPath: /run/udev\r\n              name: run-udev\r\n            - mountPath: /rook\r\n              name: rook-binaries\r\n            - mountPath: /mnt\r\n              name: cinder-0-ceph-data-6bzwc-bridge\r\n          dnsPolicy: ClusterFirst\r\n          hostPID: true\r\n          initContainers:\r\n          - args:\r\n            - ceph\r\n            - osd\r\n            - init\r\n            env:\r\n            - name: ROOK_NODE_NAME\r\n              value: cinder-0-ceph-data-6bzwc\r\n            - name: ROOK_CLUSTER_ID\r\n              value: 23a4a635-39d9-49fd-bef1-a8735549db4b\r\n            - name: ROOK_PRIVATE_IP\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: status.podIP\r\n            - name: ROOK_PUBLIC_IP\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: status.podIP\r\n            - name: ROOK_CLUSTER_NAME\r\n              value: rook-ceph\r\n            - name: ROOK_MON_ENDPOINTS\r\n              valueFrom:\r\n                configMapKeyRef:\r\n                  key: data\r\n                  name: rook-ceph-mon-endpoints\r\n            - name: ROOK_MON_SECRET\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: mon-secret\r\n                  name: rook-ceph-mon\r\n            - name: ROOK_ADMIN_SECRET\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: admin-secret\r\n                  name: rook-ceph-mon\r\n            - name: ROOK_CONFIG_DIR\r\n              value: /var/lib/rook\r\n            - name: ROOK_CEPH_CONFIG_OVERRIDE\r\n              value: /etc/rook/config/override.conf\r\n            - name: ROOK_FSID\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  key: fsid\r\n                  name: rook-ceph-mon\r\n            - name: NODE_NAME\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: spec.nodeName\r\n            - name: ROOK_CRUSHMAP_HOSTNAME\r\n              value: cinder-0-ceph-data-6bzwc\r\n            - name: CEPH_VOLUME_DEBUG\r\n              value: \"1\"\r\n            - name: CEPH_VOLUME_SKIP_RESTORECON\r\n              value: \"1\"\r\n            - name: DM_DISABLE_UDEV\r\n              value: \"1\"\r\n            - name: TINI_SUBREAPER\r\n            - name: ROOK_OSD_ID\r\n              value: \"0\"\r\n            - name: ROOK_CEPH_VERSION\r\n              value: ceph version 14.2.6-0 nautilus\r\n            - name: ROOK_IS_DEVICE\r\n              value: \"true\"\r\n            image: rook/ceph:v1.2.3\r\n            imagePullPolicy: IfNotPresent\r\n            name: config-init\r\n            resources: {}\r\n            securityContext:\r\n              privileged: true\r\n              readOnlyRootFilesystem: false\r\n              runAsUser: 0\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeMounts:\r\n            - mountPath: /var/lib/rook\r\n              name: rook-data\r\n            - mountPath: /etc/ceph\r\n              name: rook-config-override\r\n              readOnly: true\r\n            - mountPath: /var/log/ceph\r\n              name: rook-ceph-log\r\n            - mountPath: /var/lib/ceph/crash\r\n              name: rook-ceph-crash\r\n          - args:\r\n            - copy-binaries\r\n            - --copy-to-dir\r\n            - /rook\r\n            image: rook/ceph:v1.2.3\r\n            imagePullPolicy: IfNotPresent\r\n            name: copy-bins\r\n            resources: {}\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeMounts:\r\n            - mountPath: /rook\r\n              name: rook-binaries\r\n          - args:\r\n            - --verbose\r\n            - --recursive\r\n            - ceph:ceph\r\n            - /var/log/ceph\r\n            - /var/lib/ceph/crash\r\n            - /var/lib/rook/osd0\r\n            command:\r\n            - chown\r\n            image: ceph/ceph:v14.2.6\r\n            imagePullPolicy: IfNotPresent\r\n            name: chown-container-data-dir\r\n            resources: {}\r\n            securityContext:\r\n              privileged: true\r\n              readOnlyRootFilesystem: false\r\n              runAsUser: 0\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeMounts:\r\n            - mountPath: /var/lib/rook\r\n              name: rook-data\r\n            - mountPath: /etc/ceph\r\n              name: rook-config-override\r\n              readOnly: true\r\n            - mountPath: /var/log/ceph\r\n              name: rook-ceph-log\r\n            - mountPath: /var/lib/ceph/crash\r\n              name: rook-ceph-crash\r\n            - mountPath: /dev\r\n              name: devices\r\n            - mountPath: /run/udev\r\n              name: run-udev\r\n            - mountPath: /rook\r\n              name: rook-binaries\r\n            - mountPath: /mnt\r\n              name: cinder-0-ceph-data-6bzwc-bridge\r\n          - args:\r\n            - -a\r\n            - /cinder-0-ceph-data-6bzwc\r\n            - /mnt/cinder-0-ceph-data-6bzwc\r\n            command:\r\n            - cp\r\n            image: ceph/ceph:v14.2.6\r\n            imagePullPolicy: IfNotPresent\r\n            name: blkdevmapper\r\n            resources: {}\r\n            securityContext:\r\n              privileged: false\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeDevices:\r\n            - devicePath: /cinder-0-ceph-data-6bzwc\r\n              name: cinder-0-ceph-data-6bzwc\r\n            volumeMounts:\r\n            - mountPath: /mnt\r\n              name: cinder-0-ceph-data-6bzwc-bridge\r\n          restartPolicy: Always\r\n          schedulerName: default-scheduler\r\n          securityContext: {}\r\n          serviceAccount: rook-ceph-osd\r\n          serviceAccountName: rook-ceph-osd\r\n          terminationGracePeriodSeconds: 30\r\n          tolerations:\r\n          - effect: NoExecute\r\n            key: node.kubernetes.io/unreachable\r\n            operator: Exists\r\n            tolerationSeconds: 5\r\n          volumes:\r\n          - hostPath:\r\n              path: /var/lib/rook\r\n              type: \"\"\r\n            name: rook-data\r\n          - configMap:\r\n              defaultMode: 420\r\n              items:\r\n              - key: config\r\n                mode: 292\r\n                path: ceph.conf\r\n              name: rook-config-override\r\n            name: rook-config-override\r\n          - hostPath:\r\n              path: /var/lib/rook/rook-ceph/log\r\n              type: \"\"\r\n            name: rook-ceph-log\r\n          - hostPath:\r\n              path: /var/lib/rook/rook-ceph/crash\r\n              type: \"\"\r\n            name: rook-ceph-crash\r\n          - hostPath:\r\n              path: /dev\r\n              type: \"\"\r\n            name: devices\r\n          - name: cinder-0-ceph-data-6bzwc\r\n            persistentVolumeClaim:\r\n              claimName: cinder-0-ceph-data-6bzwc\r\n          - emptyDir:\r\n              medium: Memory\r\n            name: cinder-0-ceph-data-6bzwc-bridge\r\n          - hostPath:\r\n              path: /run/udev\r\n              type: \"\"\r\n            name: run-udev\r\n          - emptyDir: {}\r\n            name: rook-binaries\r\n    status:\r\n      availableReplicas: 1\r\n      conditions:\r\n      - lastTransitionTime: \"2020-02-18T07:30:35Z\"\r\n        lastUpdateTime: \"2020-02-18T07:30:35Z\"\r\n        message: Deployment has minimum availability.\r\n        reason: MinimumReplicasAvailable\r\n        status: \"True\"\r\n        type: Available\r\n      - lastTransitionTime: \"2020-02-18T07:30:25Z\"\r\n        lastUpdateTime: \"2020-02-18T07:30:35Z\"\r\n        message: ReplicaSet \"rook-ceph-osd-0-6d568f9c84\" has successfully progressed.\r\n        reason: NewReplicaSetAvailable\r\n        status: \"True\"\r\n        type: Progressing\r\n      observedGeneration: 1\r\n      readyReplicas: 1\r\n      replicas: 1\r\n      updatedReplicas: 1\r\n    ```\r\n\r\n    </details>\r\n\r\n- <details><summary><code>deployment.apps/rook-ceph-operator</code> (click to expand)</summary>\r\n\r\n    ```yaml\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    metadata:\r\n      annotations:\r\n        deployment.kubernetes.io/revision: \"4\"\r\n      creationTimestamp: \"2020-02-17T12:55:50Z\"\r\n      generation: 4\r\n      labels:\r\n        operator: rook\r\n        storage-backend: ceph\r\n      name: rook-ceph-operator\r\n      namespace: rook-ceph\r\n      resourceVersion: \"362493\"\r\n      selfLink: /apis/apps/v1/namespaces/rook-ceph/deployments/rook-ceph-operator\r\n      uid: df8a725d-5c0c-42f7-ab10-070708057e23\r\n    spec:\r\n      progressDeadlineSeconds: 600\r\n      replicas: 1\r\n      revisionHistoryLimit: 10\r\n      selector:\r\n        matchLabels:\r\n          app: rook-ceph-operator\r\n      strategy:\r\n        rollingUpdate:\r\n          maxSurge: 25%\r\n          maxUnavailable: 25%\r\n        type: RollingUpdate\r\n      template:\r\n        metadata:\r\n          creationTimestamp: null\r\n          labels:\r\n            app: rook-ceph-operator\r\n        spec:\r\n          affinity:\r\n            nodeAffinity:\r\n              requiredDuringSchedulingIgnoredDuringExecution:\r\n                nodeSelectorTerms:\r\n                - matchExpressions:\r\n                  - key: role\r\n                    operator: In\r\n                    values:\r\n                    - storage\r\n          containers:\r\n          - args:\r\n            - ceph\r\n            - operator\r\n            env:\r\n            - name: ROOK_CURRENT_NAMESPACE_ONLY\r\n              value: \"false\"\r\n            - name: AGENT_TOLERATIONS\r\n              value: |\r\n                - effect: NoSchedule\r\n                  key: role\r\n                  value: storage\r\n                  operator: Equal\r\n            - name: DISCOVER_TOLERATIONS\r\n              value: |\r\n                - effect: NoSchedule\r\n                  key: role\r\n                  value: storage\r\n                  operator: Equal\r\n            - name: ROOK_ALLOW_MULTIPLE_FILESYSTEMS\r\n              value: \"false\"\r\n            - name: ROOK_LOG_LEVEL\r\n              value: INFO\r\n            - name: ROOK_CEPH_STATUS_CHECK_INTERVAL\r\n              value: 60s\r\n            - name: ROOK_MON_HEALTHCHECK_INTERVAL\r\n              value: 45s\r\n            - name: ROOK_MON_OUT_TIMEOUT\r\n              value: 600s\r\n            - name: ROOK_DISCOVER_DEVICES_INTERVAL\r\n              value: 60m\r\n            - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED\r\n              value: \"false\"\r\n            - name: ROOK_ENABLE_SELINUX_RELABELING\r\n              value: \"true\"\r\n            - name: ROOK_ENABLE_FSGROUP\r\n              value: \"true\"\r\n            - name: ROOK_DISABLE_DEVICE_HOTPLUG\r\n              value: \"false\"\r\n            - name: DISCOVER_DAEMON_UDEV_BLACKLIST\r\n              value: (?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+\r\n            - name: ROOK_ENABLE_FLEX_DRIVER\r\n              value: \"false\"\r\n            - name: ROOK_ENABLE_DISCOVERY_DAEMON\r\n              value: \"false\"\r\n            - name: ROOK_CSI_ENABLE_CEPHFS\r\n              value: \"true\"\r\n            - name: ROOK_CSI_ENABLE_RBD\r\n              value: \"true\"\r\n            - name: ROOK_CSI_ENABLE_GRPC_METRICS\r\n              value: \"true\"\r\n            - name: CSI_ENABLE_SNAPSHOTTER\r\n              value: \"true\"\r\n            - name: CSI_FORCE_CEPHFS_KERNEL_CLIENT\r\n              value: \"true\"\r\n            - name: CSI_PROVISIONER_NODE_AFFINITY\r\n              value: role=storage\r\n            - name: CSI_PROVISIONER_TOLERATIONS\r\n              value: |\r\n                - effect: NoSchedule\r\n                  key: role\r\n                  value: storage\r\n                  operator: Equal\r\n            - name: CSI_PLUGIN_TOLERATIONS\r\n              value: |\r\n                - effect: NoSchedule\r\n                  key: node-role.kubernetes.io/controlplane\r\n                  operator: Exists\r\n                - effect: NoExecute\r\n                  key: node-role.kubernetes.io/etcd\r\n                  operator: Exists\r\n                - effect: NoSchedule\r\n                  key: role\r\n                  value: storage\r\n                  operator: Equal\r\n            - name: ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS\r\n              value: \"5\"\r\n            - name: NODE_NAME\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: spec.nodeName\r\n            - name: POD_NAME\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: metadata.name\r\n            - name: POD_NAMESPACE\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: metadata.namespace\r\n            image: rook/ceph:v1.2.3\r\n            imagePullPolicy: IfNotPresent\r\n            name: rook-ceph-operator\r\n            resources: {}\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeMounts:\r\n            - mountPath: /var/lib/rook\r\n              name: rook-config\r\n            - mountPath: /etc/ceph\r\n              name: default-config-dir\r\n          dnsPolicy: ClusterFirst\r\n          restartPolicy: Always\r\n          schedulerName: default-scheduler\r\n          securityContext: {}\r\n          serviceAccount: rook-ceph-system\r\n          serviceAccountName: rook-ceph-system\r\n          terminationGracePeriodSeconds: 30\r\n          tolerations:\r\n          - effect: NoSchedule\r\n            key: role\r\n            operator: Equal\r\n            value: storage\r\n          volumes:\r\n          - emptyDir: {}\r\n            name: rook-config\r\n          - emptyDir: {}\r\n            name: default-config-dir\r\n    status:\r\n      availableReplicas: 1\r\n      conditions:\r\n      - lastTransitionTime: \"2020-02-17T12:55:50Z\"\r\n        lastUpdateTime: \"2020-02-17T13:55:21Z\"\r\n        message: ReplicaSet \"rook-ceph-operator-7955dd6476\" has successfully progressed.\r\n        reason: NewReplicaSetAvailable\r\n        status: \"True\"\r\n        type: Progressing\r\n      - lastTransitionTime: \"2020-02-18T07:28:57Z\"\r\n        lastUpdateTime: \"2020-02-18T07:28:57Z\"\r\n        message: Deployment has minimum availability.\r\n        reason: MinimumReplicasAvailable\r\n        status: \"True\"\r\n        type: Available\r\n      observedGeneration: 4\r\n      readyReplicas: 1\r\n      replicas: 1\r\n      updatedReplicas: 1\r\n    ```\r\n\r\n    </details>\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release): Ubuntu 18.04 LTS 64-bit\r\n* Kernel (e.g. `uname -a`): ``Linux managed-k8s-worker-0 4.15.0-88-generic #88-Ubuntu SMP Tue Feb 11 20:11:34 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux``\r\n* Cloud provider or hardware configuration: OpenStack with KVM hypervisor\r\n* Rook version (use `rook version` inside of a Rook Pod): \r\n  ```\r\n  [root@rook-ceph-operator-7955dd6476-rd6l7 /]# rook version\r\n  rook: v1.2.3\r\n  go: go1.11\r\n  ```\r\n\r\n* Storage backend version (e.g. for ceph do `ceph -v`):\r\n  ```\r\n  [root@rook-ceph-operator-7955dd6476-rd6l7 /]# ceph -v\r\n  ceph version 14.2.6 (f0aa067ac7a02ee46ea48aa26c6e298b5ea272e9) nautilus (stable)\r\n  ```\r\n* Kubernetes version (use `kubectl version`): ``Server Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}``\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift): \"vanilla\" k8s.\r\n\r\n#### Related issues\r\n\r\n- https://github.com/rook/rook/issues/2384 seems related, but got closed for inactivity\r\n- https://github.com/rook/rook/issues/1895 fixed in 0.8, this is 1.2+",
  "closed_at": "2020-03-13T09:53:24Z",
  "closed_by": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "comments": 4,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/4882/comments",
  "created_at": "2020-02-18T07:43:49Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/4882/events",
  "html_url": "https://github.com/rook/rook/issues/4882",
  "id": 566705955,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    },
    {
      "color": "006b75",
      "default": false,
      "description": null,
      "id": 479938403,
      "name": "docs",
      "node_id": "MDU6TGFiZWw0Nzk5Mzg0MDM=",
      "url": "https://api.github.com/repos/rook/rook/labels/docs"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/4882/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1NjY3MDU5NTU=",
  "number": 4882,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "Tolerations from Cluster CR are not included in OSD Deployments",
  "updated_at": "2020-03-13T09:53:24Z",
  "url": "https://api.github.com/repos/rook/rook/issues/4882",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/271710?v=4",
    "events_url": "https://api.github.com/users/horazont/events{/privacy}",
    "followers_url": "https://api.github.com/users/horazont/followers",
    "following_url": "https://api.github.com/users/horazont/following{/other_user}",
    "gists_url": "https://api.github.com/users/horazont/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/horazont",
    "id": 271710,
    "login": "horazont",
    "node_id": "MDQ6VXNlcjI3MTcxMA==",
    "organizations_url": "https://api.github.com/users/horazont/orgs",
    "received_events_url": "https://api.github.com/users/horazont/received_events",
    "repos_url": "https://api.github.com/users/horazont/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/horazont/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/horazont/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/horazont"
  }
}