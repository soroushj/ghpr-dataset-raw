{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/3718398?v=4",
    "events_url": "https://api.github.com/users/galexrt/events{/privacy}",
    "followers_url": "https://api.github.com/users/galexrt/followers",
    "following_url": "https://api.github.com/users/galexrt/following{/other_user}",
    "gists_url": "https://api.github.com/users/galexrt/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/galexrt",
    "id": 3718398,
    "login": "galexrt",
    "node_id": "MDQ6VXNlcjM3MTgzOTg=",
    "organizations_url": "https://api.github.com/users/galexrt/orgs",
    "received_events_url": "https://api.github.com/users/galexrt/received_events",
    "repos_url": "https://api.github.com/users/galexrt/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/galexrt/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/galexrt/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/galexrt"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars1.githubusercontent.com/u/3718398?v=4",
      "events_url": "https://api.github.com/users/galexrt/events{/privacy}",
      "followers_url": "https://api.github.com/users/galexrt/followers",
      "following_url": "https://api.github.com/users/galexrt/following{/other_user}",
      "gists_url": "https://api.github.com/users/galexrt/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/galexrt",
      "id": 3718398,
      "login": "galexrt",
      "node_id": "MDQ6VXNlcjM3MTgzOTg=",
      "organizations_url": "https://api.github.com/users/galexrt/orgs",
      "received_events_url": "https://api.github.com/users/galexrt/received_events",
      "repos_url": "https://api.github.com/users/galexrt/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/galexrt/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/galexrt/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/galexrt"
    }
  ],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\nOperator, after a restart / reschedule destroys a healthy cluster\r\n\r\n**Expected behavior:**\r\nShould not happen\r\n\r\n**How to reproduce it (minimal and precise):**\r\nWe tried to deploy a fresh v1.0.4 ceph cluster with our battletested config which is running 3 clusters over a year now. It works all out until the operator restarts. The logs speak for itself, the mons get deleted.\r\n\r\n**File(s) to submit**:\r\n\r\n* Cluster CR (custom resource), typically called `cluster.yaml`, if necessary\r\n* Operator's logs, if necessary\r\n* Crashing pod(s) logs, if necessary\r\n\r\n To get logs, use `kubectl -n <namespace> logs <pod name>`\r\nWhen pasting logs, always surround them with backticks or use the `insert code` button from the Github UI.\r\nRead [Github documentation if you need help](https://help.github.com/en/articles/creating-and-highlighting-code-blocks).\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release):\r\nCoreos\r\n* Kernel (e.g. `uname -a`):\r\n4.14.96-coreos\r\n* Cloud provider or hardware configuration:\r\nHardware / VMware\r\n* Rook version (use `rook version` inside of a Rook Pod):\r\nv1.0.4\r\n* Storage backend version (e.g. for ceph do `ceph -v`):\r\nceph version 14.2.1 (d555a9489eb35f84f2e1ef49b77e19da9d113972) nautilus (stable)\r\n* Kubernetes version (use `kubectl version`):\r\n1.13.4\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift):\r\nsystemd deployed\r\n\r\nInteresting parts of the log files:\r\n```\r\n2019-08-06 04:47:07.698574 I | op-mon: loaded: maxMonID=4, mons=map[a:0xc0012ebb60], mapping=&{Node:map[d:0xc0012ef5c0 e:0xc0012ef5f0 a:0xc0012ef4d0 b:0xc0012ef560 c:0xc0012ef590] Port:map[]}\r\n2019-08-06 04:47:07.704874 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists\r\n2019-08-06 04:47:07.708128 I | op-mon: saved mon endpoints to config map map[data:a=100.71.255.48:6789 maxMonId:4 mapping:{\"node\":{\"a\":{\"Name\":\"k8s-worker-00.lxstage.domain.com\",\"Hostname\":\"k8s-worker-00.lxstage.domain.com\",\"Address\":\"172.22.254.105\"},\"b\":{\"Name\":\"k8s-worker-101.lxstage.domain.com\",\"Hostname\":\"k8s-worker-101.lxstage.domain.com\",\"Address\":\"172.22.254.183\"},\"c\":{\"Name\":\"k8s-worker-102.lxstage.domain.com\",\"Hostname\":\"k8s-worker-102.lxstage.domain.com\",\"Address\":\"172.22.254.186\"},\"d\":{\"Name\":\"k8s-worker-103.lxstage.domain.com\",\"Hostname\":\"k8s-worker-103.lxstage.domain.com\",\"Address\":\"172.22.254.185\"},\"e\":{\"Name\":\"k8s-worker-104.lxstage.domain.com\",\"Hostname\":\"k8s-worker-104.lxstage.domain.com\",\"Address\":\"172.22.254.187\"}},\"port\":{}}]\r\n```\r\nThis indicates that we have 5 running mons. No need to update anything... and later:\r\n\r\n```\r\n2019-08-06 04:47:09.796587 I | op-mon: targeting the mon count 5\r\n2019-08-06 04:47:09.914452 D | op-mon: there are 22 nodes available for 1 mons\r\n2019-08-06 04:47:10.108140 D | op-mon: mon pod on node k8s-worker-00.lxstage.domain.com\r\n2019-08-06 04:47:10.108370 I | op-mon: Found 4 running nodes without mons\r\n2019-08-06 04:47:10.108382 D | op-mon: mon a already assigned to a node, no need to assign\r\n2019-08-06 04:47:10.108388 D | op-mon: mon f assigned to node k8s-worker-101.lxstage.domain.com\r\n2019-08-06 04:47:10.108393 D | op-mon: using IP 172.22.254.183 for node k8s-worker-101.lxstage.domain.com\r\n2019-08-06 04:47:10.108400 D | op-mon: mon g assigned to node k8s-worker-102.lxstage.domain.com\r\n2019-08-06 04:47:10.108407 D | op-mon: using IP 172.22.254.186 for node k8s-worker-102.lxstage.domain.com\r\n2019-08-06 04:47:10.108411 D | op-mon: mon h assigned to node k8s-worker-103.lxstage.domain.com\r\n2019-08-06 04:47:10.108416 D | op-mon: using IP 172.22.254.185 for node k8s-worker-103.lxstage.domain.com\r\n2019-08-06 04:47:10.108422 D | op-mon: mon i assigned to node k8s-worker-104.lxstage.domain.com\r\n2019-08-06 04:47:10.108427 D | op-mon: using IP 172.22.254.187 for node k8s-worker-104.lxstage.domain.com\r\n2019-08-06 04:47:10.108436 D | op-mon: mons have been assigned to nodes\r\n2019-08-06 04:47:10.108442 I | op-mon: creating mon f\r\n2019-08-06 04:47:10.108454 D | op-k8sutil: creating service rook-ceph-mon-a\r\n2019-08-06 04:47:10.343842 D | op-k8sutil: updating service %s\r\n2019-08-06 04:47:10.706629 I | op-mon: mon a endpoint are [v2:100.71.255.48:3300,v1:100.71.255.48:6789]\r\n2019-08-06 04:47:10.706661 D | op-k8sutil: creating service rook-ceph-mon-f\r\n2019-08-06 04:47:11.196724 I | op-mon: mon f endpoint are [v2:100.71.47.109:3300,v1:100.71.47.109:6789]\r\n2019-08-06 04:47:11.508132 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists\r\n```\r\n\r\nwhy is it updating anything? I see he want's to add 2 new mons which is nonsense.. we don't have the nodes in our tolerations for that...\r\n\r\n\r\ncluster config: (It's healthy again after we applied a backup from yesterday)\r\n```\r\napiVersion: ceph.rook.io/v1\r\nkind: CephCluster\r\nmetadata:\r\n  annotations:\r\n  creationTimestamp: \"2019-08-05T15:05:28Z\"\r\n  finalizers:\r\n  - cephcluster.ceph.rook.io\r\n  generation: 56\r\n  name: rook-ceph\r\n  namespace: rook-ceph-stage-primary\r\n  resourceVersion: \"307846765\"\r\n  selfLink: /apis/ceph.rook.io/v1/namespaces/rook-ceph-stage-primary/cephclusters/rook-ceph\r\n  uid: 76235f05-b792-11e9-9b32-0050568460f6\r\nspec:\r\n  annotations:\r\n    all:\r\n      kubernetes.io/egress-bandwidth: 100000M\r\n      kubernetes.io/ingress-bandwidth: 100000M\r\n  cephVersion:\r\n    image: ceph/ceph:v14.2.1-20190430\r\n  dashboard:\r\n    enabled: true\r\n    port: 7000\r\n    ssl: false\r\n  dataDirHostPath: /opt/rook/rook-ceph-stage-primary\r\n  mon:\r\n    allowMultiplePerNode: false\r\n    count: 5\r\n    preferredCount: 0\r\n  network:\r\n    hostNetwork: false\r\n  placement:\r\n    mgr:\r\n      nodeAffinity:\r\n        requiredDuringSchedulingIgnoredDuringExecution:\r\n          nodeSelectorTerms:\r\n          - matchExpressions:\r\n            - key: rook-namespace\r\n              operator: NotIn\r\n              values:\r\n              - rook-ceph-stage-primary\r\n    mon:\r\n      nodeAffinity:\r\n        requiredDuringSchedulingIgnoredDuringExecution:\r\n          nodeSelectorTerms:\r\n          - matchExpressions:\r\n            - key: rook-namespace\r\n              operator: In\r\n              values:\r\n              - rook-ceph-stage-primary\r\n          - matchExpressions:\r\n            - key: node-role.kubernetes.io/quorum\r\n              operator: In\r\n              values:\r\n              - \"\"\r\n      tolerations:\r\n      - effect: PreferNoSchedule\r\n        key: node-role.kubernetes.io/quorum\r\n        operator: Exists\r\n    osd:\r\n      nodeAffinity:\r\n        requiredDuringSchedulingIgnoredDuringExecution:\r\n          nodeSelectorTerms:\r\n          - matchExpressions:\r\n            - key: rook-namespace\r\n              operator: In\r\n              values:\r\n              - rook-ceph-stage-primary\r\n      tolerations:\r\n      - effect: PreferNoSchedule\r\n        key: node-role.kubernetes.io/storage\r\n        operator: Exists\r\n  rbdMirroring:\r\n    workers: 0\r\n  resources:\r\n    all:\r\n      limits:\r\n        cpu: 500m\r\n        memory: 1Gi\r\n      requests:\r\n        cpu: 500m\r\n        memory: 1Gi\r\n    mgr:\r\n      limits:\r\n        cpu: \"2\"\r\n        memory: 1Gi\r\n      requests:\r\n        cpu: \"2\"\r\n        memory: 1Gi\r\n  storage:\r\n    config:\r\n      databaseSizeMB: \"1024\"\r\n      journalSizeMB: \"1024\"\r\n      osdsPerDevice: \"6\"\r\n      storeType: bluestore\r\n    deviceFilter: ^nvme\r\n    nodes:\r\n    - config: null\r\n      location: datacenter=rz1,rack=r1\r\n      name: k8s-worker-101.lxstage.domain.com\r\n      resources: {}\r\n    - config: null\r\n      location: datacenter=rz2,rack=r2\r\n      name: k8s-worker-102.lxstage.domain.com\r\n      resources: {}\r\n    - config: null\r\n      location: datacenter=rz1,rack=r1\r\n      name: k8s-worker-103.lxstage.domain.com\r\n      resources: {}\r\n    - config: null\r\n      location: datacenter=rz2,rack=r2\r\n      name: k8s-worker-104.lxstage.domain.com\r\n      resources: {}\r\n    useAllDevices: false\r\nstatus:\r\n  ceph:\r\n    health: HEALTH_OK\r\n    lastChanged: \"2019-08-06T06:57:14Z\"\r\n    lastChecked: \"2019-08-06T07:18:11Z\"\r\n    previousHealth: HEALTH_WARN\r\n  state: Created\r\n\r\n```\r\n\r\nHere's the complete operator log: https://gist.github.com/mmack/1a998cf876dd9638bfe2e0e8a1bd55eb",
  "closed_at": "2019-08-14T20:31:16Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "comments": 11,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/3575/comments",
  "created_at": "2019-08-06T07:33:17Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/3575/events",
  "html_url": "https://github.com/rook/rook/issues/3575",
  "id": 477217227,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/3575/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU0NzcyMTcyMjc=",
  "number": 3575,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "[1.0.4] Operator destroys healthy ceph cluster after restart",
  "updated_at": "2019-08-14T23:29:51Z",
  "url": "https://api.github.com/repos/rook/rook/issues/3575",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/100767?v=4",
    "events_url": "https://api.github.com/users/mmack/events{/privacy}",
    "followers_url": "https://api.github.com/users/mmack/followers",
    "following_url": "https://api.github.com/users/mmack/following{/other_user}",
    "gists_url": "https://api.github.com/users/mmack/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/mmack",
    "id": 100767,
    "login": "mmack",
    "node_id": "MDQ6VXNlcjEwMDc2Nw==",
    "organizations_url": "https://api.github.com/users/mmack/orgs",
    "received_events_url": "https://api.github.com/users/mmack/received_events",
    "repos_url": "https://api.github.com/users/mmack/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/mmack/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/mmack/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/mmack"
  }
}