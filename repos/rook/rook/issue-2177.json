{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\n\r\nI have a rook/cephfs volume that used to work fine, but now takes ~10 minutes per container, which hits all manner of timeouts and the pod now fails to start.\r\n\r\nI believe (due to the below) the trigger was copying a bunch of existing data into the cephfs volume - it now contains ~60k files.\r\n\r\nI'm new to ceph, but I see the mds periodically flooded with thousands of setxattr calls.  This triggers ceph health warnings about operations taking >30s, and occasionally DoSes dockerd (kubelet complains PLEG is not responding and temporarily goes NotReady, causing the manager to churn osd/mon pods).\r\n\r\nFrom digging through code and internet discussions, I've learned that:\r\n- the kubelet will recursively walk a filesystem doing chown/chmod if fsGroup is set.  See https://github.com/kubernetes/kubernetes/blob/43ce5fbf41411babfacd1f5dda9e30fb19691f1e/pkg/volume/flexvolume/mounter.go#L96\r\n- docker will do a recursive selinux relabel if the platform supports selinux (mine does - coreos, and I note the host's ceph mount options include `seclabel`).  I haven't tracked down the specific code yet.\r\n\r\nGiven that I'm seeing lots of setxattr calls, I believe I'm hitting the latter (selinux relabel).\r\n\r\n**Expected behavior:**\r\n\r\ncephfs volumes continue to be equally usable regardless of the number of files in the filesystem.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n\r\nCreate a rook/cephfs volume that contains lots of files.  Attempt to start a pod on a host that supports selinux (eg coreos).  To make it worse, use multiple containers (my test example has 3) in the pod that each mount the cephfs volume.\r\n\r\nI know the above isn't very precise, I can try to put some specific commands together if there's difficulty reproducing.\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release):\r\n```\r\nNAME=\"Container Linux by CoreOS\"\r\nID=coreos\r\nVERSION=1883.1.0\r\nVERSION_ID=1883.1.0\r\nBUILD_ID=2018-09-10-2343\r\nPRETTY_NAME=\"Container Linux by CoreOS 1883.1.0 (Rhyolite)\"\r\nANSI_COLOR=\"38;5;75\"\r\nHOME_URL=\"https://coreos.com/\"\r\nBUG_REPORT_URL=\"https://issues.coreos.com\"\r\nCOREOS_BOARD=\"amd64-usr\"\r\n```\r\n\r\n* Kernel (e.g. `uname -a`):\r\n```\r\nLinux localhost 4.14.69-coreos #1 SMP Mon Sep 10 22:45:12 UTC 2018 x86_64 Intel(R) Celeron(R) CPU N2820 @ 2.13GHz GenuineIntel GNU/Linux\r\n```\r\n\r\n* Cloud provider or hardware configuration:\r\nbaremetal\r\n\r\n* Rook version (use `rook version` inside of a Rook Pod): v0.8.2\r\n\r\n* Kubernetes version (use `kubectl version`):\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.10\", GitCommit:\"098570796b32895c38a9a1c9286425fb1ececa18\", GitTreeState:\"clean\", BuildDate:\"2018-08-02T17:11:51Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/arm\"}\r\n\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift):\r\nbaremetal / kubeadm\r\n\r\n* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox](https://rook.io/docs/Rook/master/toolbox.html)):\r\n\r\nIt was HEALTH_OK during the above issues - right at this moment I'm mid upgrade to 0.8.3 so there's a lot of unrelated churn...",
  "closed_at": "2018-11-14T06:32:57Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "comments": 7,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/2177/comments",
  "created_at": "2018-10-02T03:46:23Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/2177/events",
  "html_url": "https://github.com/rook/rook/issues/2177",
  "id": 365729465,
  "labels": [],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/2177/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUzNjU3Mjk0NjU=",
  "number": 2177,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "cephfs volume times out during container start, due to recursive selinux relabelling",
  "updated_at": "2020-10-15T22:51:25Z",
  "url": "https://api.github.com/repos/rook/rook/issues/2177",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/224224?v=4",
    "events_url": "https://api.github.com/users/anguslees/events{/privacy}",
    "followers_url": "https://api.github.com/users/anguslees/followers",
    "following_url": "https://api.github.com/users/anguslees/following{/other_user}",
    "gists_url": "https://api.github.com/users/anguslees/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/anguslees",
    "id": 224224,
    "login": "anguslees",
    "node_id": "MDQ6VXNlcjIyNDIyNA==",
    "organizations_url": "https://api.github.com/users/anguslees/orgs",
    "received_events_url": "https://api.github.com/users/anguslees/received_events",
    "repos_url": "https://api.github.com/users/anguslees/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/anguslees/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/anguslees/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/anguslees"
  }
}