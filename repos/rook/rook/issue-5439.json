{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
      "events_url": "https://api.github.com/users/leseb/events{/privacy}",
      "followers_url": "https://api.github.com/users/leseb/followers",
      "following_url": "https://api.github.com/users/leseb/following{/other_user}",
      "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/leseb",
      "id": 912735,
      "login": "leseb",
      "node_id": "MDQ6VXNlcjkxMjczNQ==",
      "organizations_url": "https://api.github.com/users/leseb/orgs",
      "received_events_url": "https://api.github.com/users/leseb/received_events",
      "repos_url": "https://api.github.com/users/leseb/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/leseb"
    }
  ],
  "author_association": "NONE",
  "body": "\r\n**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\nmds and rgw ( and crashcollector) in CreateContainerConfigError because of missing `rook-ceph-config`.\r\nCluster status is connected and rbd provionner works fine and I'm able to use block storage on pods\r\n\r\n**Expected behavior:**\r\nShould work ?\r\n\r\n**How to reproduce it (minimal and precise):**\r\n```\r\ngit clone git@github.com:rook/rook\r\ncd rook\r\ngit checkout release-1.3 # (commit 145ae30c412748199287342aabd09179eb421a7a)\r\ncd cluster/example/kubernetes/ceph\r\n# In fact this is ansible script that does the following\r\nkubectl apply -f common.yaml\r\nkubectl apply -f operator.yaml\r\nkubectl apply -f rook-ceph-mon.yaml\r\nkubectl apply -f rook-ceph-mon-endpoints.yaml\r\nkubectl apply -f cluster-external.yaml\r\n```\r\n\r\n**File(s) to submit**:\r\n\r\n* Cluster CR (custom resource), typically called `cluster.yaml`, if necessary\r\n```\r\napiVersion: v1\r\ndata:\r\n  admin-secret: <admin-keyring-base64>\r\n  cluster-name: cm9vay1jZXBo\r\n  fsid: NjczOTEzMzQtMDQ5Zi00MTk3LTgwN2EtODQ3ZDAxMWFmMGZl\r\n  mon-secret: bW9uLXNlY3JldA==\r\nkind: Secret\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"data\":{\"admin-secret\":\"<admin-keyring-base64>\",\"cluster-name\":\"cm9vay1jZXBo\",\"fsid\":\"NjczOTEzMzQtMDQ5Zi00MTk3LTgwN2EtODQ3ZDAxMWFmMGZl\",\"mon-secret\":\"bW9uLXNlY3JldA==\"},\"kind\":\"Secret\",\"\r\n  creationTimestamp: \"2020-05-08T15:02:00Z\"\r\n  managedFields:\r\n  - apiVersion: v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:data:\r\n        .: {}\r\n        f:admin-secret: {}\r\n        f:cluster-name: {}\r\n        f:fsid: {}\r\n        f:mon-secret: {}\r\n      f:metadata:\r\n        f:annotations:\r\n          .: {}\r\n          f:kubectl.kubernetes.io/last-applied-configuration: {}\r\n      f:type: {}\r\n    manager: kubectl\r\n    operation: Update\r\n    time: \"2020-05-08T15:02:00Z\"\r\n  name: rook-ceph-mon\r\n  namespace: rook-ceph\r\n  resourceVersion: \"21125\"\r\n  selfLink: /api/v1/namespaces/rook-ceph/secrets/rook-ceph-mon\r\n  uid: 85ca3784-1aef-47cd-9eb0-6657db098f77\r\ntype: Opaque\r\n```\r\n\r\n```\r\napiVersion: v1\r\ndata:\r\n  data: node1=<ipnode1>:6789,node2=<ipnode2>:6789,node3=<ipnode3>:6789,node4=<ipnode4>:6789,node5=<ipnode5>:6789\r\n  mapping: '{}'\r\n  maxMonId: \"2\"\r\nkind: ConfigMap\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"data\":{\"data\":\"node1=<ipnode1>:6789,node2=<ipnode2>:6789,node3=<ipnode3>:6789,node4=<ipnode4>:6789,node5=<ipnode5>:6789\",\"mapping\":\"{}\",\"maxMonId\":\"2\"},\"kind\":\"ConfigMap\",\"metadata\":{\"a\r\n  creationTimestamp: \"2020-05-08T15:01:59Z\"\r\n  managedFields:\r\n  - apiVersion: v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:data:\r\n        .: {}\r\n        f:data: {}\r\n        f:mapping: {}\r\n        f:maxMonId: {}\r\n      f:metadata:\r\n        f:annotations:\r\n          .: {}\r\n          f:kubectl.kubernetes.io/last-applied-configuration: {}\r\n    manager: kubectl\r\n    operation: Update\r\n    time: \"2020-05-08T15:01:59Z\"\r\n  name: rook-ceph-mon-endpoints\r\n  namespace: rook-ceph\r\n  resourceVersion: \"21123\"\r\n  selfLink: /api/v1/namespaces/rook-ceph/configmaps/rook-ceph-mon-endpoints\r\n  uid: 4579fce2-ece9-4c68-a465-37019a0267fb\r\n\r\n```\r\n```\r\napiVersion: ceph.rook.io/v1\r\nkind: CephCluster\r\nmetadata:\r\n  name: rook-ceph-external\r\n  namespace: rook-ceph\r\nspec:\r\n  external:\r\n    enable: true\r\n  dataDirHostPath: /var/lib/rook\r\n  cephVersion:\r\n    image: ceph/ceph:v14.2.8\r\n```\r\n\r\n\r\n```\r\nNAME                                              READY   STATUS                       RESTARTS   AGE\r\ncsi-cephfsplugin-5d225                            3/3     Running                      0          9m48s\r\ncsi-cephfsplugin-d7dsj                            3/3     Running                      0          9m48s\r\ncsi-cephfsplugin-jjq4d                            3/3     Running                      0          9m48s\r\ncsi-cephfsplugin-ph559                            3/3     Running                      0          9m48s\r\ncsi-cephfsplugin-provisioner-95ccff866-5mzfh      5/5     Running                      0          9m48s\r\ncsi-cephfsplugin-provisioner-95ccff866-kvx9r      5/5     Running                      0          9m48s\r\ncsi-cephfsplugin-szphg                            3/3     Running                      0          9m48s\r\ncsi-rbdplugin-6rchm                               3/3     Running                      0          9m48s\r\ncsi-rbdplugin-94dmn                               3/3     Running                      0          9m48s\r\ncsi-rbdplugin-g2ngf                               3/3     Running                      0          9m49s\r\ncsi-rbdplugin-provisioner-557c75f46b-72cvc        6/6     Running                      0          9m49s\r\ncsi-rbdplugin-provisioner-557c75f46b-p6pml        6/6     Running                      0          9m49s\r\ncsi-rbdplugin-qpvjk                               3/3     Running                      0          9m49s\r\ncsi-rbdplugin-szzdr                               3/3     Running                      0          9m49s\r\nrook-ceph-crashcollector-node1-54c7c6d488-v4zdw   0/1     CreateContainerConfigError   0          4m11s\r\nrook-ceph-crashcollector-node4-684c695fdb-wsjls   0/1     CreateContainerConfigError   0          4m10s\r\nrook-ceph-mds-arch-cloud-a-849f54c646-vdqqc       0/1     CreateContainerConfigError   0          4m12s\r\nrook-ceph-mds-arch-cloud-b-6d68d59894-tbmb6       0/1     CreateContainerConfigError   0          4m10s\r\nrook-ceph-operator-867c9d46b6-zzszg               1/1     Running                      0          150m\r\nrook-ceph-rgw-arch-cloud-a-74547c4cc4-6jd67       0/1     CreateContainerConfigError   0          3m24s\r\nrook-discover-hkxpn                               1/1     Running                      0          150m\r\nrook-discover-n4qvq                               1/1     Running                      0          150m\r\nrook-discover-pbst2                               1/1     Running                      0          150m\r\nrook-discover-t4ctk                               1/1     Running                      0          150m\r\nrook-discover-zgstg                               1/1     Running                      0          150m\r\n\r\n```\r\n\r\n\r\n```\r\nName:         rook-ceph-rgw-arch-cloud-a-74547c4cc4-6jd67\r\nNamespace:    rook-ceph\r\nPriority:     0\r\nNode:         node1/<ipnode1>\r\nStart Time:   Fri, 08 May 2020 18:44:20 +0200\r\nLabels:       app=rook-ceph-rgw\r\n              ceph_daemon_id=arch-cloud\r\n              pod-template-hash=74547c4cc4\r\n              rgw=arch-cloud\r\n              rook_cluster=rook-ceph\r\n              rook_object_store=arch-cloud\r\nAnnotations:  <none>\r\nStatus:       Pending\r\nIP:           10.1.129.10\r\nIPs:\r\n  IP:           10.1.129.10\r\nControlled By:  ReplicaSet/rook-ceph-rgw-arch-cloud-a-74547c4cc4\r\nInit Containers:\r\n  chown-container-data-dir:\r\n    Container ID:  containerd://7be954b7cc6ce7008d9f2fc9984d9fe0be2ef451e8bbbd63f93849da42a280b1\r\n    Image:         ceph/ceph:v14.2.8\r\n    Image ID:      docker.io/ceph/ceph@sha256:8ca7af554c245f40b8c9e7fa9846652f85f9fab43782c01a8bc9b8958b59d1a6\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      chown\r\n    Args:\r\n      --verbose\r\n      --recursive\r\n      ceph:ceph\r\n      /var/log/ceph\r\n      /var/lib/ceph/crash\r\n      /var/lib/ceph/rgw/ceph-arch-cloud\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 08 May 2020 18:44:22 +0200\r\n      Finished:     Fri, 08 May 2020 18:44:22 +0200\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /etc/ceph from rook-config-override (ro)\r\n      /etc/ceph/keyring-store/ from rook-ceph-rgw-arch-cloud-a-keyring (ro)\r\n      /var/lib/ceph/crash from rook-ceph-crash (rw)\r\n      /var/lib/ceph/rgw/ceph-arch-cloud from ceph-daemon-data (rw)\r\n      /var/log/ceph from rook-ceph-log (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jflzk (ro)\r\nContainers:\r\n  rgw:\r\n    Container ID:\r\n    Image:         ceph/ceph:v14.2.8\r\n    Image ID:\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      radosgw\r\n    Args:\r\n      --fsid=67391334-049f-4197-807a-847d011af0fe\r\n      --keyring=/etc/ceph/keyring-store/keyring\r\n      --log-to-stderr=true\r\n      --err-to-stderr=true\r\n      --mon-cluster-log-to-stderr=true\r\n      --log-stderr-prefix=debug\r\n      --default-log-to-file=false\r\n      --default-mon-cluster-log-to-file=false\r\n      --mon-host=$(ROOK_CEPH_MON_HOST)\r\n      --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)\r\n      --id=rgw.arch.cloud.a\r\n      --setuser=ceph\r\n      --setgroup=ceph\r\n      --foreground\r\n      --rgw-frontends=beast port=8080\r\n      --host=$(POD_NAME)\r\n      --rgw-mime-types-file=/etc/ceph/rgw/mime.types\r\n    State:          Waiting\r\n      Reason:       CreateContainerConfigError\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Liveness:       http-get http://:8080/swift/healthcheck delay=10s timeout=1s period=10s #success=1 #failure=3\r\n    Environment:\r\n      CONTAINER_IMAGE:                ceph/ceph:v14.2.8\r\n      POD_NAME:                       rook-ceph-rgw-arch-cloud-a-74547c4cc4-6jd67 (v1:metadata.name)\r\n      POD_NAMESPACE:                  rook-ceph (v1:metadata.namespace)\r\n      NODE_NAME:                       (v1:spec.nodeName)\r\n      POD_MEMORY_LIMIT:               node allocatable (limits.memory)\r\n      POD_MEMORY_REQUEST:             0 (requests.memory)\r\n      POD_CPU_LIMIT:                  node allocatable (limits.cpu)\r\n      POD_CPU_REQUEST:                0 (requests.cpu)\r\n      ROOK_CEPH_MON_HOST:             <set to the key 'mon_host' in secret 'rook-ceph-config'>             Optional: false\r\n      ROOK_CEPH_MON_INITIAL_MEMBERS:  <set to the key 'mon_initial_members' in secret 'rook-ceph-config'>  Optional: false\r\n    Mounts:\r\n      /etc/ceph from rook-config-override (ro)\r\n      /etc/ceph/keyring-store/ from rook-ceph-rgw-arch-cloud-a-keyring (ro)\r\n      /etc/ceph/rgw from rook-ceph-rgw-arch-cloud-mime-types (ro)\r\n      /var/lib/ceph/crash from rook-ceph-crash (rw)\r\n      /var/lib/ceph/rgw/ceph-arch-cloud from ceph-daemon-data (rw)\r\n      /var/log/ceph from rook-ceph-log (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jflzk (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             False\r\n  ContainersReady   False\r\n  PodScheduled      True\r\nVolumes:\r\n  rook-config-override:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      rook-config-override\r\n    Optional:  false\r\n  rook-ceph-rgw-arch-cloud-a-keyring:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  rook-ceph-rgw-arch-cloud-a-keyring\r\n    Optional:    false\r\n  rook-ceph-log:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/lib/rook/rook-ceph/log\r\n    HostPathType:\r\n  rook-ceph-crash:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/lib/rook/rook-ceph/crash\r\n    HostPathType:\r\n  ceph-daemon-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:\r\n    SizeLimit:  <unset>\r\n  rook-ceph-rgw-arch-cloud-mime-types:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      rook-ceph-rgw-arch-cloud-mime-types\r\n    Optional:  false\r\n  default-token-jflzk:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  default-token-jflzk\r\n    Optional:    false\r\nQoS Class:       BestEffort\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io/unreachable:NoExecute for 5s\r\nEvents:\r\n  Type     Reason     Age                     From                    Message\r\n  ----     ------     ----                    ----                    -------\r\n\r\n```\r\n\r\n\r\n```\r\nname:         rook-ceph-mds-arch-cloud-b-6d68d59894-tbmb6\r\nNamespace:    rook-ceph\r\nPriority:     0\r\nNode:         node4/<ipnode4>\r\nStart Time:   Fri, 08 May 2020 18:43:33 +0200\r\nLabels:       app=rook-ceph-mds\r\n              ceph_daemon_id=arch-cloud-b\r\n              mds=arch-cloud-b\r\n              pod-template-hash=6d68d59894\r\n              rook_cluster=rook-ceph\r\n              rook_file_system=arch-cloud\r\nAnnotations:  <none>\r\nStatus:       Pending\r\nIP:           10.1.130.8\r\nIPs:\r\n  IP:           10.1.130.8\r\nControlled By:  ReplicaSet/rook-ceph-mds-arch-cloud-b-6d68d59894\r\nInit Containers:\r\n  chown-container-data-dir:\r\n    Container ID:  containerd://4733901097e795a5fd4201d108ffe18f7cfa1afb57785905fe8c4ab1b9973e90\r\n    Image:         ceph/ceph:v14.2.8\r\n    Image ID:      docker.io/ceph/ceph@sha256:8ca7af554c245f40b8c9e7fa9846652f85f9fab43782c01a8bc9b8958b59d1a6\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      chown\r\n    Args:\r\n      --verbose\r\n      --recursive\r\n      ceph:ceph\r\n      /var/log/ceph\r\n      /var/lib/ceph/crash\r\n      /var/lib/ceph/mds/ceph-arch-cloud-b\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 08 May 2020 18:44:10 +0200\r\n      Finished:     Fri, 08 May 2020 18:44:10 +0200\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /etc/ceph from rook-config-override (ro)\r\n      /etc/ceph/keyring-store/ from rook-ceph-mds-arch-cloud-b-keyring (ro)\r\n      /var/lib/ceph/crash from rook-ceph-crash (rw)\r\n      /var/lib/ceph/mds/ceph-arch-cloud-b from ceph-daemon-data (rw)\r\n      /var/log/ceph from rook-ceph-log (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jflzk (ro)\r\nContainers:\r\n  mds:\r\n    Container ID:\r\n    Image:         ceph/ceph:v14.2.8\r\n    Image ID:\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      ceph-mds\r\n    Args:\r\n      --fsid=67391334-049f-4197-807a-847d011af0fe\r\n      --keyring=/etc/ceph/keyring-store/keyring\r\n      --log-to-stderr=true\r\n      --err-to-stderr=true\r\n      --mon-cluster-log-to-stderr=true\r\n      --log-stderr-prefix=debug\r\n      --default-log-to-file=false\r\n      --default-mon-cluster-log-to-file=false\r\n      --mon-host=$(ROOK_CEPH_MON_HOST)\r\n      --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)\r\n      --id=arch-cloud-b\r\n      --setuser=ceph\r\n      --setgroup=ceph\r\n      --foreground\r\n    State:          Waiting\r\n      Reason:       CreateContainerConfigError\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Liveness:       exec [env -i sh -c ceph --admin-daemon /run/ceph/ceph-mds.arch-cloud-b.asok status] delay=10s timeout=1s period=10s #success=1 #failure=3\r\n    Environment:\r\n      CONTAINER_IMAGE:                ceph/ceph:v14.2.8\r\n      POD_NAME:                       rook-ceph-mds-arch-cloud-b-6d68d59894-tbmb6 (v1:metadata.name)\r\n      POD_NAMESPACE:                  rook-ceph (v1:metadata.namespace)\r\n      NODE_NAME:                       (v1:spec.nodeName)\r\n      POD_MEMORY_LIMIT:               node allocatable (limits.memory)\r\n      POD_MEMORY_REQUEST:             0 (requests.memory)\r\n      POD_CPU_LIMIT:                  node allocatable (limits.cpu)\r\n      POD_CPU_REQUEST:                0 (requests.cpu)\r\n      ROOK_CEPH_MON_HOST:             <set to the key 'mon_host' in secret 'rook-ceph-config'>             Optional: false\r\n      ROOK_CEPH_MON_INITIAL_MEMBERS:  <set to the key 'mon_initial_members' in secret 'rook-ceph-config'>  Optional: false\r\n    Mounts:\r\n      /etc/ceph from rook-config-override (ro)\r\n      /etc/ceph/keyring-store/ from rook-ceph-mds-arch-cloud-b-keyring (ro)\r\n      /var/lib/ceph/crash from rook-ceph-crash (rw)\r\n      /var/lib/ceph/mds/ceph-arch-cloud-b from ceph-daemon-data (rw)\r\n      /var/log/ceph from rook-ceph-log (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jflzk (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             False\r\n  ContainersReady   False\r\n  PodScheduled      True\r\nVolumes:\r\n  rook-config-override:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      rook-config-override\r\n    Optional:  false\r\n  rook-ceph-mds-arch-cloud-b-keyring:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  rook-ceph-mds-arch-cloud-b-keyring\r\n    Optional:    false\r\n  rook-ceph-log:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/lib/rook/rook-ceph/log\r\n    HostPathType:\r\n  rook-ceph-crash:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/lib/rook/rook-ceph/crash\r\n    HostPathType:\r\n  ceph-daemon-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:\r\n    SizeLimit:  <unset>\r\n  default-token-jflzk:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  default-token-jflzk\r\n    Optional:    false\r\nQoS Class:       BestEffort\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io/unreachable:NoExecute for 5s\r\nEvents:\r\n  Type     Reason     Age                  From                    Message\r\n  ----     ------     ----                 ----                    -------\r\n  Normal   Scheduled  <unknown>            default-scheduler       Successfully assigned rook-ceph/rook-ceph-mds-arch-cloud-b-6d68d59894-tbmb6 to node4\r\n  Normal   Pulling    3m44s                kubelet, node4  Pulling image \"ceph/ceph:v14.2.8\"\r\n  Normal   Pulled     3m18s                kubelet, node4  Successfully pulled image \"ceph/ceph:v14.2.8\"\r\n  Normal   Created    3m8s                 kubelet, node4  Created container chown-container-data-dir\r\n  Normal   Started    3m8s                 kubelet, node4  Started container chown-container-data-dir\r\n  Warning  Failed     80s (x10 over 3m6s)  kubelet, node4  Error: secret \"rook-ceph-config\" not found\r\n  Normal   Pulled     69s (x11 over 3m6s)  kubelet, node4  Container image \"ceph/ceph:v14.2.8\" already present on machine\r\n\r\n```\r\n\r\n\r\nconfigmap list\r\n```\r\nNAME                                  DATA   AGE\r\nlocal-device-node3                    1      151m\r\nlocal-device-node5                    1      151m\r\nlocal-device-node2                    1      151m\r\nlocal-device-node1                    1      151m\r\nlocal-device-node4                    1      152m\r\nrook-ceph-csi-config                  1      12m\r\nrook-ceph-mon-endpoints               3      108m\r\nrook-ceph-operator-config             6      153m\r\nrook-ceph-rgw-arch-cloud-mime-types   1      6m31s\r\nrook-config-override                  1      12m\r\n```\r\nsecret list:\r\n```\r\nNAME                                  DATA   AGE\r\nlocal-device-node3                    1      151m\r\nlocal-device-node5                    1      151m\r\nlocal-device-node2                    1      151m\r\nlocal-device-node1                    1      151m\r\nlocal-device-node4                    1      152m\r\nrook-ceph-csi-config                  1      12m\r\nrook-ceph-mon-endpoints               3      108m\r\nrook-ceph-operator-config             6      153m\r\nrook-ceph-rgw-arch-cloud-mime-types   1      6m31s\r\nrook-config-override                  1      12m\r\n```\r\n\r\n* Operator's logs, if necessary\r\n```\r\n2020-05-08 14:17:33.424010 I | rookcmd: starting Rook v1.3.2 with arguments '/usr/local/bin/rook ceph operator'\r\n2020-05-08 14:17:33.424138 I | rookcmd: flag values: --add_dir_header=false, --alsologtostderr=false, --csi-cephfs-plugin-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin.yaml, --csi-cephfs-provisioner-dep-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-dep.yaml, --csi-cephfs-provisioner-sts-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-sts.yaml, --csi-rbd-plugin-template-path=/etc/ceph-csi/rbd/csi-rbdplugin.yaml, --csi-rbd-provisioner-dep-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-dep.yaml, --csi-rbd-provisioner-sts-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-st\r\n2020-05-08 14:17:33.424146 I | cephcmd: starting operator\r\n2020-05-08 14:17:33.634709 I | op-discover: rook-discover daemonset started\r\n2020-05-08 14:17:33.636603 I | operator: rook-provisioner ceph.rook.io/block started using ceph.rook.io flex vendor dir\r\n2020-05-08 14:17:33.636927 I | operator: rook-provisioner rook.io/block started using rook.io flex vendor dir\r\n2020-05-08 14:17:33.636944 I | operator: Watching all namespaces for cluster CRDs\r\n2020-05-08 14:17:33.636961 I | op-cluster: start watching clusters in all namespaces\r\n2020-05-08 14:17:33.636989 I | op-cluster: Enabling hotplug orchestration: ROOK_DISABLE_DEVICE_HOTPLUG=false\r\nI0508 14:17:33.637373       7 leaderelection.go:242] attempting to acquire leader lease  rook-ceph/ceph.rook.io-block...\r\nI0508 14:17:33.637583       7 leaderelection.go:242] attempting to acquire leader lease  rook-ceph/rook.io-block...\r\n2020-05-08 14:17:33.637784 I | operator: setting up the controller-runtime manager\r\n2020-05-08 14:17:33.639492 I | op-cluster: ConfigMap \"rook-ceph-operator-config\" changes detected. Updating configurations\r\nI0508 14:17:33.692372       7 leaderelection.go:252] successfully acquired lease rook-ceph/ceph.rook.io-block\r\nI0508 14:17:33.692526       7 event.go:281] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"rook-ceph\", Name:\"ceph.rook.io-block\", UID:\"668e41d9-45f5-43c5-a933-d098a6229bf4\", APIVersion:\"v1\", ResourceVersion:\"4538\", FieldPath:\"\"}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-867c9d46b6-zzszg_130e4f84-9e80-40c5-9c5f-816f68da92ac became leader\r\nI0508 14:17:33.692634       7 controller.go:780] Starting provisioner controller ceph.rook.io/block_rook-ceph-operator-867c9d46b6-zzszg_130e4f84-9e80-40c5-9c5f-816f68da92ac!\r\nI0508 14:17:33.700781       7 leaderelection.go:252] successfully acquired lease rook-ceph/rook.io-block\r\nI0508 14:17:33.700999       7 controller.go:780] Starting provisioner controller rook.io/block_rook-ceph-operator-867c9d46b6-zzszg_f954183d-9fd7-40a0-ad10-fd5cc887b83e!\r\nI0508 14:17:33.701028       7 event.go:281] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"rook-ceph\", Name:\"rook.io-block\", UID:\"b4f9470a-d285-4c28-9096-8491faaed9a0\", APIVersion:\"v1\", ResourceVersion:\"4539\", FieldPath:\"\"}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-867c9d46b6-zzszg_f954183d-9fd7-40a0-ad10-fd5cc887b83e became leader\r\nI0508 14:17:34.092847       7 controller.go:829] Started provisioner controller ceph.rook.io/block_rook-ceph-operator-867c9d46b6-zzszg_130e4f84-9e80-40c5-9c5f-816f68da92ac!\r\n2020-05-08 14:17:34.342605 I | operator: starting the controller-runtime manager\r\nI0508 14:17:35.301159       7 controller.go:829] Started provisioner controller rook.io/block_rook-ceph-operator-867c9d46b6-zzszg_f954183d-9fd7-40a0-ad10-fd5cc887b83e!\r\n2020-05-08 16:37:08.849621 I | op-cluster: starting cluster in namespace rook-ceph\r\n2020-05-08 16:37:08.908241 I | ceph-csi: detecting the ceph csi image version for image \"quay.io/cephcsi/cephcsi:v2.1.0\"\r\n2020-05-08 16:37:52.471077 I | ceph-csi: Detected ceph CSI image version: \"v2.1.0\"\r\n2020-05-08 16:37:52.756743 I | ceph-csi: successfully created csi config map \"rook-ceph-csi-config\"\r\n2020-05-08 16:37:56.375226 I | ceph-csi: CSIDriver object created for driver \"rook-ceph.rbd.csi.ceph.com\"\r\n2020-05-08 16:37:56.618035 I | ceph-csi: CSIDriver object created for driver \"rook-ceph.cephfs.csi.ceph.com\"\r\n2020-05-08 16:37:56.618065 I | operator: successfully started Ceph CSI driver(s)\r\n2020-05-08 16:37:57.878049 I | op-config: CephCluster \"rook-ceph\" status: \"Connecting\". \"Cluster is connecting\"\r\n2020-05-08 16:37:57.980481 I | op-mon: parsing mon endpoints: node1=<ipnode1>:6789,node2=<ipnode2>:6789,node3=<ipnode3>:6789,node4=<ipnode4>:6789,node5=<ipnode5>:6789\r\n2020-05-08 16:37:57.980637 I | op-cluster: detecting the ceph image version for image ceph/ceph:v14.2.8...\r\n2020-05-08 16:38:42.382110 I | op-cluster: Detected ceph image version: \"14.2.8-0 nautilus\"\r\n2020-05-08 16:38:42.691683 I | op-mon: parsing mon endpoints: node1=<ipnode1>:6789,node2=<ipnode2>:6789,node3=<ipnode3>:6789,node4=<ipnode4>:6789,node5=<ipnode5>:6789\r\n2020-05-08 16:38:42.691926 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-05-08 16:38:42.691979 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-05-08 16:38:43.729172 I | op-cluster: cluster \"rook-ceph\": version \"14.2.8-0 nautilus\" detected for image \"ceph/ceph:v14.2.8\"\r\n2020-05-08 16:38:44.215496 I | op-cluster: creating 'rook-ceph-config' configmap.\r\n2020-05-08 16:38:44.646493 I | op-cluster: external cluster identity established\r\n2020-05-08 16:38:44.646533 I | cephclient: getting or creating ceph auth key \"client.csi-rbd-provisioner\"\r\n2020-05-08 16:38:46.348624 I | cephclient: getting or creating ceph auth key \"client.csi-rbd-node\"\r\n2020-05-08 16:38:47.988099 I | cephclient: getting or creating ceph auth key \"client.csi-cephfs-provisioner\"\r\n2020-05-08 16:38:50.236928 I | cephclient: getting or creating ceph auth key \"client.csi-cephfs-node\"\r\n2020-05-08 16:38:55.707919 I | ceph-csi: created kubernetes csi secrets for cluster \"rook-ceph\"\r\n2020-05-08 16:38:55.970580 I | ceph-csi: successfully created csi config map \"rook-ceph-csi-config\"\r\n2020-05-08 16:38:56.236566 I | op-cluster: successfully updated csi config map\r\n2020-05-08 16:38:56.236599 I | cephclient: getting or creating ceph auth key \"client.crash\"\r\n2020-05-08 16:38:57.850398 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster \"rook-ceph\"\r\n2020-05-08 16:38:57.946519 I | op-config: CephCluster \"rook-ceph\" status: \"Connected\". \"Cluster connected successfully\"\r\n2020-05-08 16:38:58.100133 I | op-client: start watching client resources in namespace \"rook-ceph\"\r\n2020-05-08 16:38:58.100164 I | op-bucket-prov: Ceph Bucket Provisioner launched\r\n2020-05-08 16:38:58.101267 I | op-cluster: ceph status check interval is 60s\r\nI0508 16:38:58.101325       7 manager.go:118] objectbucket.io/provisioner-manager \"msg\"=\"starting provisioner\"  \"name\"=\"ceph.rook.io/bucket\"\r\n2020-05-08 16:38:58.312694 I | op-cluster: added finalizer to cluster rook-ceph-external\r\n2020-05-08 16:43:12.104653 I | op-mon: parsing mon endpoints: node1=<ipnode1>:6789,node2=<ipnode2>:6789,node3=<ipnode3>:6789,node4=<ipnode4>:6789,node5=<ipnode5>:6789\r\n2020-05-08 16:43:12.727398 I | ceph-spec: adding finalizer \"cephobjectstore.ceph.rook.io\" on \"arch-cloud\"\r\n2020-05-08 16:43:12.735339 E | ceph-object-controller: failed to reconcile failed to add finalizer: failed to add finalizer \"cephobjectstore.ceph.rook.io\" on \"arch-cloud\": Operation cannot be fulfilled on cephobjectstores.ceph.rook.io \"arch-cloud\": the object has been modified; please apply your changes to the latest version and try again\r\n2020-05-08 16:43:13.502136 I | op-mon: parsing mon endpoints: node1=<ipnode1>:6789,node2=<ipnode2>:6789,node3=<ipnode3>:6789,node4=<ipnode4>:6789,node5=<ipnode5>:6789\r\n2020-05-08 16:43:14.075974 I | ceph-spec: adding finalizer \"cephfilesystem.ceph.rook.io\" on \"arch-cloud\"\r\n2020-05-08 16:43:14.083356 E | ceph-file-controller: failed to reconcile failed to add finalizer: failed to add finalizer \"cephfilesystem.ceph.rook.io\" on \"arch-cloud\": Operation cannot be fulfilled on cephfilesystems.ceph.rook.io \"arch-cloud\": the object has been modified; please apply your changes to the latest version and try again\r\n2020-05-08 16:43:14.259124 I | op-mon: parsing mon endpoints: node1=<ipnode1>:6789,node2=<ipnode2>:6789,node3=<ipnode3>:6789,node4=<ipnode4>:6789,node5=<ipnode5>:6789\r\n2020-05-08 16:43:14.432865 I | ceph-spec: adding finalizer \"cephblockpool.ceph.rook.io\" on \"replicated-block\"\r\n2020-05-08 16:43:14.439181 E | ceph-block-pool-controller: failed to reconcile failed to add finalizer: failed to add finalizer \"cephblockpool.ceph.rook.io\" on \"replicated-block\": Operation cannot be fulfilled on cephblockpools.ceph.rook.io \"replicated-block\": the object has been modified; please apply your changes to the latest version and try again\r\n2020-05-08 16:43:14.882323 I | ceph-spec: adding finalizer \"cephobjectstore.ceph.rook.io\" on \"arch-cloud\"\r\n2020-05-08 16:43:15.603755 I | op-mon: parsing mon endpoints: node1=<ipnode1>:6789,node2=<ipnode2>:6789,node3=<ipnode3>:6789,node4=<ipnode4>:6789,node5=<ipnode5>:6789\r\n2020-05-08 16:43:15.943081 I | ceph-object-controller: reconciling object store deployments\r\n2020-05-08 16:43:15.967756 I | ceph-spec: adding finalizer \"cephblockpool.ceph.rook.io\" on \"replicated-block\"\r\n2020-05-08 16:43:16.222910 I | ceph-spec: adding finalizer \"cephfilesystem.ceph.rook.io\" on \"arch-cloud\"\r\n2020-05-08 16:43:16.453349 I | ceph-object-controller: ceph object store gateway service running at 10.1.246.245:80\r\n2020-05-08 16:43:16.453370 I | ceph-object-controller: reconciling object store pools\r\n2020-05-08 16:43:16.496339 I | ceph-block-pool-controller: creating pool \"replicated-block\" in namespace \"rook-ceph\"\r\n2020-05-08 16:43:18.229576 I | ceph-file-controller: Creating filesystem arch-cloud\r\n2020-05-08 16:43:23.461625 I | cephclient: creating replicated pool arch-cloud-metadata succeeded\r\n2020-05-08 16:43:23.465163 I | cephclient: creating replicated pool replicated-block succeeded\r\n2020-05-08 16:43:24.619420 I | cephclient: creating replicated pool arch-cloud.rgw.control succeeded\r\n2020-05-08 16:43:27.631668 I | cephclient: creating replicated pool arch-cloud-data0 succeeded\r\n2020-05-08 16:43:27.631718 I | cephclient: creating filesystem \"arch-cloud\" with metadata pool \"arch-cloud-metadata\" and data pools [arch-cloud-data0]\r\n2020-05-08 16:43:28.964061 I | ceph-file-controller: created filesystem arch-cloud on 1 data pool(s) and metadata pool arch-cloud-metadata\r\n2020-05-08 16:43:29.607030 I | cephclient: setting allow_standby_replay for filesystem \"arch-cloud\"\r\n2020-05-08 16:43:30.745735 I | ceph-file-controller: start running mdses for filesystem \"arch-cloud\"\r\n2020-05-08 16:43:30.745772 I | cephclient: getting or creating ceph auth key \"mds.arch-cloud-a\"\r\n2020-05-08 16:43:31.537565 I | op-mds: setting mds config flags\r\n2020-05-08 16:43:31.597404 I | cephclient: getting or creating ceph auth key \"mds.arch-cloud-b\"\r\n2020-05-08 16:43:31.920729 I | cephclient: creating replicated pool arch-cloud.rgw.meta succeeded\r\n2020-05-08 16:43:32.172478 I | ceph-spec: will reconcile based on patch {\"metadata\":{\"managedFields\":[{\"apiVersion\":\"apps/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:banzaicloud.com/last-applied\":{}},\"f:labels\":{\".\":{},\"f:app\":{},\"f:ceph-version\":{},\"f:ceph_daemon_id\":{},\"f:mds\":{},\"f:rook-version\":{},\"f:rook_cluster\":{},\"f:rook_file_system\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"c0265ed5-e5f5-4b97-91bb-1e84ddf99ab6\\\"}\":{\".\":{},\"f:apiVersion\":{},\"f:blockOwnerDeletion\":{},\"f:controller\":{},\"f:kind\":{},\"f:name\":{},\"f:uid\":{}}}},\"f:spec\":{\"f:progressDeadlineSeconds\":{},\"f:replicas\":{},\"f\r\n2020-05-08 16:43:32.736716 I | ceph-spec: will reconcile based on patch {\"metadata\":{\"managedFields\":[{\"apiVersion\":\"apps/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:banzaicloud.com/last-applied\":{}},\"f:labels\":{\".\":{},\"f:app\":{},\"f:ceph-version\":{},\"f:ceph_daemon_id\":{},\"f:mds\":{},\"f:rook-version\":{},\"f:rook_cluster\":{},\"f:rook_file_system\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"c0265ed5-e5f5-4b97-91bb-1e84ddf99ab6\\\"}\":{\".\":{},\"f:apiVersion\":{},\"f:blockOwnerDeletion\":{},\"f:controller\":{},\"f:kind\":{},\"f:name\":{},\"f:uid\":{}}}},\"f:spec\":{\"f:progressDeadlineSeconds\":{},\"f:replicas\":{},\"f\r\n2020-05-08 16:43:32.978128 I | op-mds: setting mds config flags\r\n2020-05-08 16:43:33.560589 I | ceph-spec: will reconcile based on patch {\"metadata\":{\"managedFields\":[{\"apiVersion\":\"apps/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:banzaicloud.com/last-applied\":{}},\"f:labels\":{\".\":{},\"f:app\":{},\"f:ceph-version\":{},\"f:ceph_daemon_id\":{},\"f:mds\":{},\"f:rook-version\":{},\"f:rook_cluster\":{},\"f:rook_file_system\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"c0265ed5-e5f5-4b97-91bb-1e84ddf99ab6\\\"}\":{\".\":{},\"f:apiVersion\":{},\"f:blockOwnerDeletion\":{},\"f:controller\":{},\"f:kind\":{},\"f:name\":{},\"f:uid\":{}}}},\"f:spec\":{\"f:progressDeadlineSeconds\":{},\"f:replicas\":{},\"f\r\n2020-05-08 16:43:33.884110 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:43:34.188686 I | ceph-spec: will reconcile based on patch {\"metadata\":{\"managedFields\":[{\"apiVersion\":\"apps/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:banzaicloud.com/last-applied\":{}},\"f:labels\":{\".\":{},\"f:app\":{},\"f:ceph-version\":{},\"f:ceph_daemon_id\":{},\"f:mds\":{},\"f:rook-version\":{},\"f:rook_cluster\":{},\"f:rook_file_system\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"c0265ed5-e5f5-4b97-91bb-1e84ddf99ab6\\\"}\":{\".\":{},\"f:apiVersion\":{},\"f:blockOwnerDeletion\":{},\"f:controller\":{},\"f:kind\":{},\"f:name\":{},\"f:uid\":{}}}},\"f:spec\":{\"f:progressDeadlineSeconds\":{},\"f:replicas\":{},\"f\r\n2020-05-08 16:43:34.228961 E | ceph-crashcollector-controller: node reconcile failed on op \"unchanged\": Operation cannot be fulfilled on deployments.apps \"rook-ceph-crashcollector-node4\": the object has been modified; please apply your changes to the latest version and try again\r\n2020-05-08 16:43:34.409112 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:43:34.947231 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:43:41.734546 I | cephclient: creating replicated pool arch-cloud.rgw.log succeeded\r\n2020-05-08 16:43:44.818639 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:43:47.412751 I | cephclient: creating replicated pool arch-cloud.rgw.buckets.index succeeded\r\n2020-05-08 16:43:53.574680 I | cephclient: creating replicated pool arch-cloud.rgw.buckets.non-ec succeeded\r\n2020-05-08 16:43:55.717609 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:44:02.653763 I | cephclient: creating replicated pool .rgw.root succeeded\r\n2020-05-08 16:44:07.091022 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:44:10.893897 I | cephclient: creating replicated pool arch-cloud.rgw.buckets.data succeeded\r\n2020-05-08 16:44:10.893918 I | ceph-object-controller: reconciling object store realms\r\n2020-05-08 16:44:15.192122 I | ceph-object-controller: RGW: realm=f4041927-7148-4c36-a80b-18a75c5ea6c9, zonegroup=0dfb0a67-de9a-4e1f-b402-c5ccd3ce87ed, zone=e2ed78e5-f5b9-4c51-8fd8-174df52e52f5\r\n2020-05-08 16:44:15.192160 I | ceph-object-controller: creating object store \"arch-cloud\" in namespace \"rook-ceph\"\r\n2020-05-08 16:44:15.192191 I | cephclient: getting or creating ceph auth key \"client.rgw.arch.cloud.a\"\r\n2020-05-08 16:44:15.954783 I | ceph-object-controller: setting rgw config flags\r\n2020-05-08 16:44:17.590725 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:44:19.561493 I | ceph-object-controller: object store \"arch-cloud\" deployment \"rook-ceph-rgw-arch-cloud-a\" started\r\n2020-05-08 16:44:19.872784 I | ceph-object-controller: created object store \"arch-cloud\" in namespace \"rook-ceph\"\r\n2020-05-08 16:44:20.098369 I | ceph-spec: will reconcile based on patch {\"metadata\":{\"managedFields\":[{\"apiVersion\":\"apps/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:banzaicloud.com/last-applied\":{}},\"f:labels\":{\".\":{},\"f:app\":{},\"f:ceph-version\":{},\"f:ceph_daemon_id\":{},\"f:rgw\":{},\"f:rook-version\":{},\"f:rook_cluster\":{},\"f:rook_object_store\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"c5741a8c-913c-4d7b-95ef-4dc3107b8c14\\\"}\":{\".\":{},\"f:apiVersion\":{},\"f:blockOwnerDeletion\":{},\"f:controller\":{},\"f:kind\":{},\"f:name\":{},\"f:uid\":{}}}},\"f:spec\":{\"f:progressDeadlineSeconds\":{},\"f:replicas\":{},\"\r\n2020-05-08 16:44:20.231350 I | ceph-spec: will reconcile based on patch {\"metadata\":{\"managedFields\":[{\"apiVersion\":\"apps/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:banzaicloud.com/last-applied\":{}},\"f:labels\":{\".\":{},\"f:app\":{},\"f:ceph-version\":{},\"f:ceph_daemon_id\":{},\"f:rgw\":{},\"f:rook-version\":{},\"f:rook_cluster\":{},\"f:rook_object_store\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"c5741a8c-913c-4d7b-95ef-4dc3107b8c14\\\"}\":{\".\":{},\"f:apiVersion\":{},\"f:blockOwnerDeletion\":{},\"f:controller\":{},\"f:kind\":{},\"f:name\":{},\"f:uid\":{}}}},\"f:spec\":{\"f:progressDeadlineSeconds\":{},\"f:replicas\":{},\"\r\n2020-05-08 16:44:20.585388 I | ceph-spec: ceph-object-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:44:21.065257 I | ceph-spec: ceph-object-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:44:28.144844 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:44:31.150137 I | ceph-spec: ceph-object-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:44:38.656637 I | ceph-spec: ceph-file-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n2020-05-08 16:44:41.702242 I | ceph-spec: ceph-object-controller: CephCluster \"rook-ceph-external\" found but skipping reconcile since Ceph health is \"HEALTH_ERR\"\r\n\r\n```\r\n\r\n* Crashing pod(s) logs, if necessary\r\n```\r\nme:         rook-ceph-mds-arch-cloud-b-6d68d59894-tbmb6\r\nNamespace:    rook-ceph\r\nPriority:     0\r\nNode:         node4/<ipnode4>\r\nStart Time:   Fri, 08 May 2020 18:43:33 +0200\r\nLabels:       app=rook-ceph-mds\r\n              ceph_daemon_id=arch-cloud-b\r\n              mds=arch-cloud-b\r\n              pod-template-hash=6d68d59894\r\n              rook_cluster=rook-ceph\r\n              rook_file_system=arch-cloud\r\nAnnotations:  <none>\r\nStatus:       Pending\r\nIP:           10.1.130.8\r\nIPs:\r\n  IP:           10.1.130.8\r\nControlled By:  ReplicaSet/rook-ceph-mds-arch-cloud-b-6d68d59894\r\nInit Containers:\r\n  chown-container-data-dir:\r\n    Container ID:  containerd://4733901097e795a5fd4201d108ffe18f7cfa1afb57785905fe8c4ab1b9973e90\r\n    Image:         ceph/ceph:v14.2.8\r\n    Image ID:      docker.io/ceph/ceph@sha256:8ca7af554c245f40b8c9e7fa9846652f85f9fab43782c01a8bc9b8958b59d1a6\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      chown\r\n    Args:\r\n      --verbose\r\n      --recursive\r\n      ceph:ceph\r\n      /var/log/ceph\r\n      /var/lib/ceph/crash\r\n      /var/lib/ceph/mds/ceph-arch-cloud-b\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 08 May 2020 18:44:10 +0200\r\n      Finished:     Fri, 08 May 2020 18:44:10 +0200\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /etc/ceph from rook-config-override (ro)\r\n      /etc/ceph/keyring-store/ from rook-ceph-mds-arch-cloud-b-keyring (ro)\r\n      /var/lib/ceph/crash from rook-ceph-crash (rw)\r\n      /var/lib/ceph/mds/ceph-arch-cloud-b from ceph-daemon-data (rw)\r\n      /var/log/ceph from rook-ceph-log (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jflzk (ro)\r\nContainers:\r\n  mds:\r\n    Container ID:\r\n    Image:         ceph/ceph:v14.2.8\r\n    Image ID:\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      ceph-mds\r\n    Args:\r\n      --fsid=67391334-049f-4197-807a-847d011af0fe\r\n      --keyring=/etc/ceph/keyring-store/keyring\r\n      --log-to-stderr=true\r\n      --err-to-stderr=true\r\n      --mon-cluster-log-to-stderr=true\r\n      --log-stderr-prefix=debug\r\n      --default-log-to-file=false\r\n      --default-mon-cluster-log-to-file=false\r\n      --mon-host=$(ROOK_CEPH_MON_HOST)\r\n      --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)\r\n      --id=arch-cloud-b\r\n      --setuser=ceph\r\n      --setgroup=ceph\r\n      --foreground\r\n    State:          Waiting\r\n      Reason:       CreateContainerConfigError\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Liveness:       exec [env -i sh -c ceph --admin-daemon /run/ceph/ceph-mds.arch-cloud-b.asok status] delay=10s timeout=1s period=10s #success=1 #failure=3\r\n    Environment:\r\n      CONTAINER_IMAGE:                ceph/ceph:v14.2.8\r\n      POD_NAME:                       rook-ceph-mds-arch-cloud-b-6d68d59894-tbmb6 (v1:metadata.name)\r\n      POD_NAMESPACE:                  rook-ceph (v1:metadata.namespace)\r\n      NODE_NAME:                       (v1:spec.nodeName)\r\n      POD_MEMORY_LIMIT:               node allocatable (limits.memory)\r\n      POD_MEMORY_REQUEST:             0 (requests.memory)\r\n      POD_CPU_LIMIT:                  node allocatable (limits.cpu)\r\n      POD_CPU_REQUEST:                0 (requests.cpu)\r\n      ROOK_CEPH_MON_HOST:             <set to the key 'mon_host' in secret 'rook-ceph-config'>             Optional: false\r\n      ROOK_CEPH_MON_INITIAL_MEMBERS:  <set to the key 'mon_initial_members' in secret 'rook-ceph-config'>  Optional: false\r\n    Mounts:\r\n      /etc/ceph from rook-config-override (ro)\r\n      /etc/ceph/keyring-store/ from rook-ceph-mds-arch-cloud-b-keyring (ro)\r\n      /var/lib/ceph/crash from rook-ceph-crash (rw)\r\n      /var/lib/ceph/mds/ceph-arch-cloud-b from ceph-daemon-data (rw)\r\n      /var/log/ceph from rook-ceph-log (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jflzk (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             False\r\n  ContainersReady   False\r\n  PodScheduled      True\r\nVolumes:\r\n  rook-config-override:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      rook-config-override\r\n    Optional:  false\r\n  rook-ceph-mds-arch-cloud-b-keyring:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  rook-ceph-mds-arch-cloud-b-keyring\r\n    Optional:    false\r\n  rook-ceph-log:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/lib/rook/rook-ceph/log\r\n    HostPathType:\r\n  rook-ceph-crash:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/lib/rook/rook-ceph/crash\r\n    HostPathType:\r\n  ceph-daemon-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:\r\n    SizeLimit:  <unset>\r\n  default-token-jflzk:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  default-token-jflzk\r\n    Optional:    false\r\nQoS Class:       BestEffort\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io/unreachable:NoExecute for 5s\r\nEvents:\r\n  Type     Reason     Age                  From                    Message\r\n  ----     ------     ----                 ----                    -------\r\n  Normal   Scheduled  <unknown>            default-scheduler       Successfully assigned rook-ceph/rook-ceph-mds-arch-cloud-b-6d68d59894-tbmb6 to node4\r\n  Normal   Pulling    3m44s                kubelet, node4  Pulling image \"ceph/ceph:v14.2.8\"\r\n  Normal   Pulled     3m18s                kubelet, node4  Successfully pulled image \"ceph/ceph:v14.2.8\"\r\n  Normal   Created    3m8s                 kubelet, node4  Created container chown-container-data-dir\r\n  Normal   Started    3m8s                 kubelet, node4  Started container chown-container-data-dir\r\n  Warning  Failed     80s (x10 over 3m6s)  kubelet, node4  Error: secret \"rook-ceph-config\" not found\r\n  Normal   Pulled     69s (x11 over 3m6s)  kubelet, node4  Container image \"ceph/ceph:v14.2.8\" already present on machine\r\n```\r\n\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release): Archlinux\r\n* Kernel (e.g. `uname -a`): Linux node1 5.6.11-arch1-1 #1 SMP PREEMPT Wed, 06 May 2020 17:32:37 +0000 x86_64 GNU/Linux\r\n* Cloud provider or hardware configuration: bare-metal\r\n* Rook version (use `rook version` inside of a Rook Pod): v1.3.2\r\n* Storage backend version (e.g. for ceph do `ceph -v`): ceph version 14.2.8 (2d095e947a02261ce61424021bb43bd3022d35cb) nautilus (stable)\r\n* Kubernetes version (use `kubectl version`): v1.18.2\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift): custom installer (but kubernetes vanilla)\r\n* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox](https://rook.io/docs/rook/master/ceph-toolbox.html)): ERR since cephfs in error, OK before\r\n",
  "closed_at": "2020-05-11T20:19:28Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "comments": 0,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/5439/comments",
  "created_at": "2020-05-08T17:49:19Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/5439/events",
  "html_url": "https://github.com/rook/rook/issues/5439",
  "id": 614888055,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    },
    {
      "color": "ef5c55",
      "default": false,
      "description": "main ceph tag",
      "id": 479456042,
      "name": "ceph",
      "node_id": "MDU6TGFiZWw0Nzk0NTYwNDI=",
      "url": "https://api.github.com/repos/rook/rook/labels/ceph"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/5439/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU2MTQ4ODgwNTU=",
  "number": 5439,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "external cluster with management: mds and rgw pods in CreateContainerConfigError because missing `rook-ceph-config` secret",
  "updated_at": "2020-05-11T20:19:28Z",
  "url": "https://api.github.com/repos/rook/rook/issues/5439",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/11920285?v=4",
    "events_url": "https://api.github.com/users/Zempashi/events{/privacy}",
    "followers_url": "https://api.github.com/users/Zempashi/followers",
    "following_url": "https://api.github.com/users/Zempashi/following{/other_user}",
    "gists_url": "https://api.github.com/users/Zempashi/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/Zempashi",
    "id": 11920285,
    "login": "Zempashi",
    "node_id": "MDQ6VXNlcjExOTIwMjg1",
    "organizations_url": "https://api.github.com/users/Zempashi/orgs",
    "received_events_url": "https://api.github.com/users/Zempashi/received_events",
    "repos_url": "https://api.github.com/users/Zempashi/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/Zempashi/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/Zempashi/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/Zempashi"
  }
}