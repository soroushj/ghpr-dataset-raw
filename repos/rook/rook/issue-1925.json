{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/7062400?v=4",
    "events_url": "https://api.github.com/users/rootfs/events{/privacy}",
    "followers_url": "https://api.github.com/users/rootfs/followers",
    "following_url": "https://api.github.com/users/rootfs/following{/other_user}",
    "gists_url": "https://api.github.com/users/rootfs/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/rootfs",
    "id": 7062400,
    "login": "rootfs",
    "node_id": "MDQ6VXNlcjcwNjI0MDA=",
    "organizations_url": "https://api.github.com/users/rootfs/orgs",
    "received_events_url": "https://api.github.com/users/rootfs/received_events",
    "repos_url": "https://api.github.com/users/rootfs/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/rootfs/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/rootfs/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/rootfs"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars0.githubusercontent.com/u/7062400?v=4",
      "events_url": "https://api.github.com/users/rootfs/events{/privacy}",
      "followers_url": "https://api.github.com/users/rootfs/followers",
      "following_url": "https://api.github.com/users/rootfs/following{/other_user}",
      "gists_url": "https://api.github.com/users/rootfs/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/rootfs",
      "id": 7062400,
      "login": "rootfs",
      "node_id": "MDQ6VXNlcjcwNjI0MDA=",
      "organizations_url": "https://api.github.com/users/rootfs/orgs",
      "received_events_url": "https://api.github.com/users/rootfs/received_events",
      "repos_url": "https://api.github.com/users/rootfs/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/rootfs/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/rootfs/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/rootfs"
    }
  ],
  "author_association": "NONE",
  "body": "<!-- **Are you in the right place?**\r\n1. For issues or feature requests, please create an issue in this repository.\r\n2. For general technical and non-technical questions, we are happy to help you on our [Rook.io Slack](https://Rook-io.slack.com).\r\n3. Did you already search the existing open issues for anything similar? -->\r\n\r\n**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\n\r\n**Expected behavior:**\r\n\r\n**How to reproduce it (minimal and precise):**\r\nAdd taints to the storage nodes and modify cluster.yaml and operator.yaml to use these node storage\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release):\r\n```sh\r\nNAME=\"Ubuntu\"\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 16.04.4 LTS\"\r\nVERSION_ID=\"16.04\"\r\nHOME_URL=\"http://www.ubuntu.com/\"\r\nSUPPORT_URL=\"http://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\r\nVERSION_CODENAME=xenial\r\nUBUNTU_CODENAME=xenial\r\n```\r\n\r\n* Kernel (e.g. `uname -a`):\r\nLinux node1 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n* Cloud provider or hardware configuration:\r\nVirtualBox on a laptop\r\n\r\n* Rook version (use `rook version` inside of a Rook Pod):\r\nrook: v0.8.0\r\n\r\n* Kubernetes version (use `kubectl version`):\r\n```yaml\r\nclientVersion:\r\n  buildDate: 2018-07-17T18:53:20Z\r\n  compiler: gc\r\n  gitCommit: b1b29978270dc22fecc592ac55d903350454310a\r\n  gitTreeState: clean\r\n  gitVersion: v1.11.1\r\n  goVersion: go1.10.3\r\n  major: \"1\"\r\n  minor: \"11\"\r\n  platform: linux/amd64\r\nserverVersion:\r\n  buildDate: 2018-06-27T20:08:34Z\r\n  compiler: gc\r\n  gitCommit: 91e7b4fd31fcd3d5f436da26c980becec37ceefe\r\n  gitTreeState: clean\r\n  gitVersion: v1.11.0\r\n  goVersion: go1.10.2\r\n  major: \"1\"\r\n  minor: \"11\"\r\n  platform: linux/amd64\r\n\r\n```\r\n\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift):\r\nOn-premise (on a laptop with 6 VirtualBox machines), 1 master node, 2 workers, 3 storage nodes\r\n\r\n* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox](https://rook.io/docs/Rook/master/toolbox.html)):\r\n\r\n```sh\r\n[root@rook-ceph-tools /]# ceph health\r\nHEALTH_WARN noscrub,nodeep-scrub flag(s) set\r\n```\r\n```sh\r\n[root@rook-ceph-tools /]# ceph status\r\n  cluster:\r\n    id:     9f50ccc7-b276-4594-bfbe-06cecd4e9e36\r\n    health: HEALTH_WARN\r\n            noscrub,nodeep-scrub flag(s) set\r\n \r\n  services:\r\n    mon: 3 daemons, quorum rook-ceph-mon0,rook-ceph-mon2,rook-ceph-mon1\r\n    mgr: a(active)\r\n    osd: 3 osds: 3 up, 3 in\r\n         flags noscrub,nodeep-scrub\r\n \r\n  data:\r\n    pools:   0 pools, 0 pgs\r\n    objects: 0 objects, 0 bytes\r\n    usage:   26198 MB used, 117 GB / 143 GB avail\r\n    pgs:     \r\n```\r\n```sh\r\n[root@rook-ceph-tools /]# ceph quorum_status\r\n{\"election_epoch\":12,\"quorum\":[0,1,2],\"quorum_names\":[\"rook-ceph-mon0\",\"rook-ceph-mon2\",\"rook-ceph-mon1\"],\"quorum_leader_name\":\"rook-ceph-mon0\",\"monmap\":{\"epoch\":3,\"fsid\":\"9f50ccc7-b276-4594-bfbe-06cecd4e9e36\",\"modified\":\"2018-07-21 09:30:06.043301\",\"created\":\"2018-07-21 09:29:47.733522\",\"features\":{\"persistent\":[\"kraken\",\"luminous\"],\"optional\":[]},\"mons\":[{\"rank\":0,\"name\":\"rook-ceph-mon0\",\"addr\":\"10.102.3.68:6790/0\",\"public_addr\":\"10.102.3.68:6790/0\"},{\"rank\":1,\"name\":\"rook-ceph-mon2\",\"addr\":\"10.108.199.152:6790/0\",\"public_addr\":\"10.108.199.152:6790/0\"},{\"rank\":2,\"name\":\"rook-ceph-mon1\",\"addr\":\"10.110.190.32:6790/0\",\"public_addr\":\"10.110.190.32:6790/0\"}]}}\r\n```\r\n```sh\r\n[root@rook-ceph-tools /]# ceph mon_status\r\n{\"name\":\"rook-ceph-mon1\",\"rank\":2,\"state\":\"peon\",\"election_epoch\":12,\"quorum\":[0,1,2],\"features\":{\"required_con\":\"153140804152475648\",\"required_mon\":[\"kraken\",\"luminous\"],\"quorum_con\":\"2305244844532236283\",\"quorum_mon\":[\"kraken\",\"luminous\"]},\"outside_quorum\":[],\"extra_probe_peers\":[\"10.108.199.152:6790/0\"],\"sync_provider\":[],\"monmap\":{\"epoch\":3,\"fsid\":\"9f50ccc7-b276-4594-bfbe-06cecd4e9e36\",\"modified\":\"2018-07-21 09:30:06.043301\",\"created\":\"2018-07-21 09:29:47.733522\",\"features\":{\"persistent\":[\"kraken\",\"luminous\"],\"optional\":[]},\"mons\":[{\"rank\":0,\"name\":\"rook-ceph-mon0\",\"addr\":\"10.102.3.68:6790/0\",\"public_addr\":\"10.102.3.68:6790/0\"},{\"rank\":1,\"name\":\"rook-ceph-mon2\",\"addr\":\"10.108.199.152:6790/0\",\"public_addr\":\"10.108.199.152:6790/0\"},{\"rank\":2,\"name\":\"rook-ceph-mon1\",\"addr\":\"10.110.190.32:6790/0\",\"public_addr\":\"10.110.190.32:6790/0\"}]},\"feature_map\":{\"mon\":{\"group\":{\"features\":\"0x1ffddff8eea4fffb\",\"release\":\"luminous\",\"num\":1}},\"client\":{\"group\":{\"features\":\"0x1ffddff8eea4fffb\",\"release\":\"luminous\",\"num\":1}}}}\r\n```\r\n\r\n\r\n\r\n\r\n\r\nThe not all the osds pod are scheduled on the storage nodes.\r\n\r\n```sh\r\nroot@node1:/home/arkan# kubectl -n rook-ceph get all\r\nNAME                                       READY     STATUS      RESTARTS   AGE\r\npod/rook-ceph-mgr-a-54458c6f9-4dx7r        1/1       Running     0          21m\r\npod/rook-ceph-mon0-vzm79                   1/1       Running     0          22m\r\npod/rook-ceph-mon1-9scx9                   1/1       Running     0          22m\r\npod/rook-ceph-mon2-7zkv7                   1/1       Running     0          21m\r\npod/rook-ceph-osd-id-0-557c7fc59f-8dfdg    1/1       Running     0          21m\r\npod/rook-ceph-osd-id-1-58cc99d9bd-qvhpn    1/1       Running     0          21m\r\npod/rook-ceph-osd-id-2-7cc96bd666-gtpbq    1/1       Running     0          21m\r\npod/rook-ceph-osd-prepare-node2-kq5g4      0/1       Pending     0          21m\r\npod/rook-ceph-osd-prepare-node3-ms5xp      0/1       Pending     0          21m\r\npod/rook-ceph-osd-prepare-storage1-jx54w   0/1       Completed   0          9m\r\npod/rook-ceph-osd-prepare-storage2-j462s   0/1       Completed   0          9m\r\npod/rook-ceph-osd-prepare-storage3-5xjh2   0/1       Completed   0          9m\r\npod/rook-ceph-tools                        1/1       Running     0          22m\r\n\r\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\r\nservice/rook-ceph-mgr                      ClusterIP   10.101.24.98     <none>        9283/TCP   21m\r\nservice/rook-ceph-mgr-dashboard            ClusterIP   10.98.15.70      <none>        7000/TCP   21m\r\nservice/rook-ceph-mgr-dashboard-external   ClusterIP   10.108.39.235    <none>        7000/TCP   22m\r\nservice/rook-ceph-mon0                     ClusterIP   10.102.3.68      <none>        6790/TCP   22m\r\nservice/rook-ceph-mon1                     ClusterIP   10.110.190.32    <none>        6790/TCP   22m\r\nservice/rook-ceph-mon2                     ClusterIP   10.108.199.152   <none>        6790/TCP   21m\r\n\r\nNAME                                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\ndeployment.apps/rook-ceph-mgr-a      1         1         1            1           21m\r\ndeployment.apps/rook-ceph-osd-id-0   1         1         1            1           21m\r\ndeployment.apps/rook-ceph-osd-id-1   1         1         1            1           21m\r\ndeployment.apps/rook-ceph-osd-id-2   1         1         1            1           21m\r\n\r\nNAME                                            DESIRED   CURRENT   READY     AGE\r\nreplicaset.apps/rook-ceph-mgr-a-54458c6f9       1         1         1         21m\r\nreplicaset.apps/rook-ceph-mon0                  1         1         1         22m\r\nreplicaset.apps/rook-ceph-mon1                  1         1         1         22m\r\nreplicaset.apps/rook-ceph-mon2                  1         1         1         21m\r\nreplicaset.apps/rook-ceph-osd-id-0-557c7fc59f   1         1         1         21m\r\nreplicaset.apps/rook-ceph-osd-id-1-58cc99d9bd   1         1         1         21m\r\nreplicaset.apps/rook-ceph-osd-id-2-7cc96bd666   1         1         1         21m\r\n\r\nNAME                                       DESIRED   SUCCESSFUL   AGE\r\njob.batch/rook-ceph-osd-prepare-node2      1         0            21m\r\njob.batch/rook-ceph-osd-prepare-node3      1         0            21m\r\njob.batch/rook-ceph-osd-prepare-storage1   1         1            9m\r\njob.batch/rook-ceph-osd-prepare-storage2   1         1            9m\r\njob.batch/rook-ceph-osd-prepare-storage3   1         1            9m\r\n```\r\n\r\n```sh\r\nroot@node1:/home/arkan# kubectl -n rook-ceph-system get all\r\nNAME                                      READY     STATUS    RESTARTS   AGE\r\npod/rook-ceph-agent-8hmmt                 1/1       Running   0          23m\r\npod/rook-ceph-agent-8njp5                 1/1       Running   0          23m\r\npod/rook-ceph-agent-jdclt                 1/1       Running   0          23m\r\npod/rook-ceph-agent-l7xj7                 1/1       Running   0          23m\r\npod/rook-ceph-agent-mjw5t                 1/1       Running   0          23m\r\npod/rook-ceph-operator-56565dfdcf-sjwjp   1/1       Running   0          23m\r\npod/rook-discover-52tjd                   1/1       Running   0          23m\r\npod/rook-discover-gktzq                   1/1       Running   0          23m\r\npod/rook-discover-s62fd                   1/1       Running   0          23m\r\npod/rook-discover-w54r8                   1/1       Running   0          23m\r\npod/rook-discover-w8mfh                   1/1       Running   0          23m\r\n\r\nNAME                             DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\r\ndaemonset.apps/rook-ceph-agent   5         5         5         5            5           <none>          23m\r\ndaemonset.apps/rook-discover     5         5         5         5            5           <none>          23m\r\n\r\nNAME                                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\ndeployment.apps/rook-ceph-operator   1         1         1            1           23m\r\n\r\nNAME                                            DESIRED   CURRENT   READY     AGE\r\nreplicaset.apps/rook-ceph-operator-56565dfdcf   1         1         1         23m\r\n```\r\n\r\ncluster.yaml manifest\r\n```yaml\r\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: rook-ceph\r\n---\r\napiVersion: v1\r\nkind: ServiceAccount\r\nmetadata:\r\n  name: rook-ceph-cluster\r\n  namespace: rook-ceph\r\n---\r\nkind: Role\r\napiVersion: rbac.authorization.k8s.io/v1beta1\r\nmetadata:\r\n  name: rook-ceph-cluster\r\n  namespace: rook-ceph\r\nrules:\r\n- apiGroups: [\"\"]\r\n  resources: [\"configmaps\"]\r\n  verbs: [ \"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\" ]\r\n---\r\n# Allow the operator to create resources in this cluster's namespace\r\nkind: RoleBinding\r\napiVersion: rbac.authorization.k8s.io/v1beta1\r\nmetadata:\r\n  name: rook-ceph-cluster-mgmt\r\n  namespace: rook-ceph\r\nroleRef:\r\n  apiGroup: rbac.authorization.k8s.io\r\n  kind: ClusterRole\r\n  name: rook-ceph-cluster-mgmt\r\nsubjects:\r\n- kind: ServiceAccount\r\n  name: rook-ceph-system\r\n  namespace: rook-ceph-system\r\n---\r\n# Allow the pods in this namespace to work with configmaps\r\nkind: RoleBinding\r\napiVersion: rbac.authorization.k8s.io/v1beta1\r\nmetadata:\r\n  name: rook-ceph-cluster\r\n  namespace: rook-ceph\r\nroleRef:\r\n  apiGroup: rbac.authorization.k8s.io\r\n  kind: Role\r\n  name: rook-ceph-cluster\r\nsubjects:\r\n- kind: ServiceAccount\r\n  name: rook-ceph-cluster\r\n  namespace: rook-ceph\r\n---\r\napiVersion: ceph.rook.io/v1beta1\r\nkind: Cluster\r\nmetadata:\r\n  name: rook-ceph\r\n  namespace: rook-ceph\r\nspec:\r\n  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).\r\n  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.\r\n  # In Minikube, the '/data' directory is configured to persist across reboots. Use \"/data/rook\" in Minikube environment.\r\n  dataDirHostPath: \"/var/lib/rook\"\r\n  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)\r\n  serviceAccount: rook-ceph-cluster\r\n  # set the amount of mons to be started\r\n  mon:\r\n    count: 3\r\n    allowMultiplePerNode: true\r\n  # enable the ceph dashboard for viewing cluster status\r\n  dashboard:\r\n    enabled: true\r\n  network:\r\n    # toggle to use hostNetwork\r\n    hostNetwork: false\r\n  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.\r\n  # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and\r\n  # tolerate taints with a key of 'storage-node'.\r\n  placement:\r\n    all:\r\n      nodeAffinity:\r\n        requiredDuringSchedulingIgnoredDuringExecution:\r\n          nodeSelectorTerms:\r\n          - matchExpressions:\r\n            - key: role\r\n              operator: In\r\n              values:\r\n              - storage-node\r\n#      podAffinity:\r\n#      podAntiAffinity:\r\n      tolerations:\r\n      - key: storage-node\r\n        operator: Exists\r\n        effect: NoSchedule\r\n# The above placement information can also be specified for mon, osd, and mgr components\r\n#    mon:\r\n#    osd:\r\n#    mgr:\r\n  resources:\r\n# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory\r\n#    mgr:\r\n#      limits:\r\n#        cpu: \"500m\"\r\n#        memory: \"1024Mi\"\r\n#      requests:\r\n#        cpu: \"500m\"\r\n#        memory: \"1024Mi\"\r\n# The above example requests/limits can also be added to the mon and osd components\r\n#    mon:\r\n#    osd:\r\n  storage: # cluster level storage configuration and selection\r\n    useAllNodes: true\r\n    useAllDevices: false\r\n    deviceFilter:\r\n    location:\r\n    config:\r\n      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.\r\n      # Set the storeType explicitly only if it is required not to use the default.\r\n      # storeType: bluestore\r\n      databaseSizeMB: \"1024\" # this value can be removed for environments with normal sized disks (100 GB or larger)\r\n      journalSizeMB: \"1024\"  # this value can be removed for environments with normal sized disks (20 GB or larger)\r\n# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.\r\n#    directories:\r\n#    - path: /rook/storage-dir\r\n# Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named\r\n# nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.\r\n#    nodes:\r\n#    - name: \"172.17.4.101\"\r\n#      directories: # specific directories to use for storage can be specified for each node\r\n#      - path: \"/rook/storage-dir\"\r\n#      resources:\r\n#        limits:\r\n#          cpu: \"500m\"\r\n#          memory: \"1024Mi\"\r\n#        requests:\r\n#          cpu: \"500m\"\r\n#          memory: \"1024Mi\"\r\n#    - name: \"172.17.4.201\"\r\n#      devices: # specific devices to use for storage can be specified for each node\r\n#      - name: \"sdb\"\r\n#      - name: \"sdc\"\r\n#      config: # configuration can be specified at the node level which overrides the cluster level config\r\n#        storeType: filestore\r\n#    - name: \"172.17.4.301\"\r\n#      deviceFilter: \"^sd.\"\r\n```\r\n\r\noperator.yaml manifest\r\n```yaml\r\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: rook-ceph-system\r\n---\r\napiVersion: apiextensions.k8s.io/v1beta1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: clusters.ceph.rook.io\r\nspec:\r\n  group: ceph.rook.io\r\n  names:\r\n    kind: Cluster\r\n    listKind: ClusterList\r\n    plural: clusters\r\n    singular: cluster\r\n    shortNames:\r\n    - rcc\r\n  scope: Namespaced\r\n  version: v1beta1\r\n---\r\napiVersion: apiextensions.k8s.io/v1beta1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: filesystems.ceph.rook.io\r\nspec:\r\n  group: ceph.rook.io\r\n  names:\r\n    kind: Filesystem\r\n    listKind: FilesystemList\r\n    plural: filesystems\r\n    singular: filesystem\r\n    shortNames:\r\n    - rcfs\r\n  scope: Namespaced\r\n  version: v1beta1\r\n---\r\napiVersion: apiextensions.k8s.io/v1beta1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: objectstores.ceph.rook.io\r\nspec:\r\n  group: ceph.rook.io\r\n  names:\r\n    kind: ObjectStore\r\n    listKind: ObjectStoreList\r\n    plural: objectstores\r\n    singular: objectstore\r\n    shortNames:\r\n    - rco\r\n  scope: Namespaced\r\n  version: v1beta1\r\n---\r\napiVersion: apiextensions.k8s.io/v1beta1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: pools.ceph.rook.io\r\nspec:\r\n  group: ceph.rook.io\r\n  names:\r\n    kind: Pool\r\n    listKind: PoolList\r\n    plural: pools\r\n    singular: pool\r\n    shortNames:\r\n    - rcp\r\n  scope: Namespaced\r\n  version: v1beta1\r\n---\r\napiVersion: apiextensions.k8s.io/v1beta1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: volumes.rook.io\r\nspec:\r\n  group: rook.io\r\n  names:\r\n    kind: Volume\r\n    listKind: VolumeList\r\n    plural: volumes\r\n    singular: volume\r\n    shortNames:\r\n    - rv\r\n  scope: Namespaced\r\n  version: v1alpha2\r\n---\r\n# The cluster role for managing all the cluster-specific resources in a namespace\r\napiVersion: rbac.authorization.k8s.io/v1beta1\r\nkind: ClusterRole\r\nmetadata:\r\n  name: rook-ceph-cluster-mgmt\r\n  labels:\r\n    operator: rook\r\n    storage-backend: ceph\r\nrules:\r\n- apiGroups:\r\n  - \"\"\r\n  resources:\r\n  - secrets\r\n  - pods\r\n  - services\r\n  - configmaps\r\n  verbs:\r\n  - get\r\n  - list\r\n  - watch\r\n  - patch\r\n  - create\r\n  - update\r\n  - delete\r\n- apiGroups:\r\n  - extensions\r\n  resources:\r\n  - deployments\r\n  - daemonsets\r\n  - replicasets\r\n  verbs:\r\n  - get\r\n  - list\r\n  - watch\r\n  - create\r\n  - update\r\n  - delete\r\n---\r\n# The role for the operator to manage resources in the system namespace\r\napiVersion: rbac.authorization.k8s.io/v1beta1\r\nkind: Role\r\nmetadata:\r\n  name: rook-ceph-system\r\n  namespace: rook-ceph-system\r\n  labels:\r\n    operator: rook\r\n    storage-backend: ceph\r\nrules:\r\n- apiGroups:\r\n  - \"\"\r\n  resources:\r\n  - pods\r\n  - configmaps\r\n  verbs:\r\n  - get\r\n  - list\r\n  - watch\r\n  - patch\r\n  - create\r\n  - update\r\n  - delete\r\n- apiGroups:\r\n  - extensions\r\n  resources:\r\n  - daemonsets\r\n  verbs:\r\n  - get\r\n  - list\r\n  - watch\r\n  - create\r\n  - update\r\n  - delete\r\n---\r\n# The cluster role for managing the Rook CRDs\r\napiVersion: rbac.authorization.k8s.io/v1beta1\r\nkind: ClusterRole\r\nmetadata:\r\n  name: rook-ceph-global\r\n  labels:\r\n    operator: rook\r\n    storage-backend: ceph\r\nrules:\r\n- apiGroups:\r\n  - \"\"\r\n  resources:\r\n  # Pod access is needed for fencing\r\n  - pods\r\n  # Node access is needed for determining nodes where mons should run\r\n  - nodes\r\n  - nodes/proxy\r\n  verbs:\r\n  - get\r\n  - list\r\n  - watch\r\n- apiGroups:\r\n  - \"\"\r\n  resources:\r\n  - events\r\n    # PVs and PVCs are managed by the Rook provisioner\r\n  - persistentvolumes\r\n  - persistentvolumeclaims\r\n  verbs:\r\n  - get\r\n  - list\r\n  - watch\r\n  - patch\r\n  - create\r\n  - update\r\n  - delete\r\n- apiGroups:\r\n  - storage.k8s.io\r\n  resources:\r\n  - storageclasses\r\n  verbs:\r\n  - get\r\n  - list\r\n  - watch\r\n- apiGroups:\r\n  - batch\r\n  resources:\r\n  - jobs\r\n  verbs:\r\n  - get\r\n  - list\r\n  - watch\r\n  - create\r\n  - update\r\n  - delete   \r\n- apiGroups:\r\n  - ceph.rook.io\r\n  resources:\r\n  - \"*\"\r\n  verbs:\r\n  - \"*\"\r\n- apiGroups:\r\n  - rook.io\r\n  resources:\r\n  - \"*\"\r\n  verbs:\r\n  - \"*\"\r\n---\r\n# The rook system service account used by the operator, agent, and discovery pods\r\napiVersion: v1\r\nkind: ServiceAccount\r\nmetadata:\r\n  name: rook-ceph-system\r\n  namespace: rook-ceph-system\r\n  labels:\r\n    operator: rook\r\n    storage-backend: ceph\r\n---\r\n# Grant the operator, agent, and discovery agents access to resources in the rook-ceph-system namespace\r\nkind: RoleBinding\r\napiVersion: rbac.authorization.k8s.io/v1beta1\r\nmetadata:\r\n  name: rook-ceph-system\r\n  namespace: rook-ceph-system\r\n  labels:\r\n    operator: rook\r\n    storage-backend: ceph\r\nroleRef:\r\n  apiGroup: rbac.authorization.k8s.io\r\n  kind: Role\r\n  name: rook-ceph-system\r\nsubjects:\r\n- kind: ServiceAccount\r\n  name: rook-ceph-system\r\n  namespace: rook-ceph-system\r\n---\r\n# Grant the rook system daemons cluster-wide access to manage the Rook CRDs, PVCs, and storage classes\r\nkind: ClusterRoleBinding\r\napiVersion: rbac.authorization.k8s.io/v1beta1\r\nmetadata:\r\n  name: rook-ceph-global\r\n  namespace: rook-ceph-system\r\n  labels:\r\n    operator: rook\r\n    storage-backend: ceph\r\nroleRef:\r\n  apiGroup: rbac.authorization.k8s.io\r\n  kind: ClusterRole\r\n  name: rook-ceph-global\r\nsubjects:\r\n- kind: ServiceAccount\r\n  name: rook-ceph-system\r\n  namespace: rook-ceph-system\r\n---\r\n# The deployment for the rook operator\r\napiVersion: apps/v1beta1\r\nkind: Deployment\r\nmetadata:\r\n  name: rook-ceph-operator\r\n  namespace: rook-ceph-system\r\n  labels:\r\n    operator: rook\r\n    storage-backend: ceph\r\nspec:\r\n  replicas: 1\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: rook-ceph-operator\r\n    spec:\r\n      serviceAccountName: rook-ceph-system\r\n      containers:\r\n      - name: rook-ceph-operator\r\n        image: rook/ceph:v0.8.0\r\n        args: [\"ceph\", \"operator\"]\r\n        volumeMounts:\r\n        - mountPath: /var/lib/rook\r\n          name: rook-config\r\n        - mountPath: /etc/ceph\r\n          name: default-config-dir\r\n        env:\r\n        # To disable RBAC, uncomment the following:\r\n        # - name: RBAC_ENABLED\r\n        #  value: \"false\"\r\n        # Rook Agent toleration. Will tolerate all taints with all keys.\r\n        # Choose between NoSchedule, PreferNoSchedule and NoExecute:\r\n        - name: AGENT_TOLERATION\r\n          value: \"NoSchedule\"\r\n        # (Optional) Rook Agent toleration key. Set this to the key of the taint you want to tolerate\r\n        - name: AGENT_TOLERATION_KEY\r\n          value: \"storage-node\"\r\n        # Set the path where the Rook agent can find the flex volumes\r\n        # - name: FLEXVOLUME_DIR_PATH\r\n        #  value: \"<PathToFlexVolumes>\"\r\n        # Rook Discover toleration. Will tolerate all taints with all keys.\r\n        # Choose between NoSchedule, PreferNoSchedule and NoExecute:\r\n        - name: DISCOVER_TOLERATION\r\n          value: \"NoSchedule\"\r\n        # (Optional) Rook Discover toleration key. Set this to the key of the taint you want to tolerate\r\n        - name: DISCOVER_TOLERATION_KEY\r\n          value: \"storage-node\"\r\n        # Allow rook to create multiple file systems. Note: This is considered\r\n        # an experimental feature in Ceph as described at\r\n        # http://docs.ceph.com/docs/master/cephfs/experimental-features/#multiple-filesystems-within-a-ceph-cluster\r\n        # which might cause mons to crash as seen in https://github.com/rook/rook/issues/1027\r\n        - name: ROOK_ALLOW_MULTIPLE_FILESYSTEMS\r\n          value: \"false\"\r\n        # The logging level for the operator: INFO | DEBUG \r\n        - name: ROOK_LOG_LEVEL\r\n          value: \"INFO\"\r\n        # The interval to check if every mon is in the quorum.\r\n        - name: ROOK_MON_HEALTHCHECK_INTERVAL\r\n          value: \"45s\"\r\n        # The duration to wait before trying to failover or remove/replace the\r\n        # current mon with a new mon (useful for compensating flapping network).\r\n        - name: ROOK_MON_OUT_TIMEOUT\r\n          value: \"300s\"\r\n        # Whether to start pods as privileged that mount a host path, which includes the Ceph mon and osd pods.\r\n        # This is necessary to workaround the anyuid issues when running on OpenShift.\r\n        # For more details see https://github.com/rook/rook/issues/1314#issuecomment-355799641\r\n        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED\r\n          value: \"false\"\r\n        # The name of the node to pass with the downward API\r\n        - name: NODE_NAME\r\n          valueFrom:\r\n            fieldRef:\r\n              fieldPath: spec.nodeName\r\n        # The pod name to pass with the downward API\r\n        - name: POD_NAME\r\n          valueFrom:\r\n            fieldRef:\r\n              fieldPath: metadata.name\r\n        # The pod namespace to pass with the downward API\r\n        - name: POD_NAMESPACE\r\n          valueFrom:\r\n            fieldRef:\r\n              fieldPath: metadata.namespace\r\n      volumes:\r\n      - name: rook-config\r\n        emptyDir: {}\r\n      - name: default-config-dir\r\n        emptyDir: {}\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
  "closed_at": "2018-07-31T16:27:26Z",
  "closed_by": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/4313439?v=4",
    "events_url": "https://api.github.com/users/jbw976/events{/privacy}",
    "followers_url": "https://api.github.com/users/jbw976/followers",
    "following_url": "https://api.github.com/users/jbw976/following{/other_user}",
    "gists_url": "https://api.github.com/users/jbw976/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/jbw976",
    "id": 4313439,
    "login": "jbw976",
    "node_id": "MDQ6VXNlcjQzMTM0Mzk=",
    "organizations_url": "https://api.github.com/users/jbw976/orgs",
    "received_events_url": "https://api.github.com/users/jbw976/received_events",
    "repos_url": "https://api.github.com/users/jbw976/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/jbw976/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/jbw976/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/jbw976"
  },
  "comments": 8,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/1925/comments",
  "created_at": "2018-07-21T09:54:53Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/1925/events",
  "html_url": "https://github.com/rook/rook/issues/1925",
  "id": 343314696,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    },
    {
      "color": "ef5c55",
      "default": false,
      "description": "main ceph tag",
      "id": 479456042,
      "name": "ceph",
      "node_id": "MDU6TGFiZWw0Nzk0NTYwNDI=",
      "url": "https://api.github.com/repos/rook/rook/labels/ceph"
    },
    {
      "color": "ffdd66",
      "default": false,
      "description": "If a PR needs backport to latest release.",
      "id": 1009594971,
      "name": "needs-backport",
      "node_id": "MDU6TGFiZWwxMDA5NTk0OTcx",
      "url": "https://api.github.com/repos/rook/rook/labels/needs-backport"
    },
    {
      "color": "00aaaa",
      "default": false,
      "description": null,
      "id": 479820581,
      "name": "operator",
      "node_id": "MDU6TGFiZWw0Nzk4MjA1ODE=",
      "url": "https://api.github.com/repos/rook/rook/labels/operator"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/1925/labels{/name}",
  "locked": false,
  "milestone": {
    "closed_at": "2020-01-28T21:43:40Z",
    "closed_issues": 62,
    "created_at": "2017-11-06T17:57:26Z",
    "creator": {
      "avatar_url": "https://avatars3.githubusercontent.com/u/1463491?v=4",
      "events_url": "https://api.github.com/users/bassam/events{/privacy}",
      "followers_url": "https://api.github.com/users/bassam/followers",
      "following_url": "https://api.github.com/users/bassam/following{/other_user}",
      "gists_url": "https://api.github.com/users/bassam/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/bassam",
      "id": 1463491,
      "login": "bassam",
      "node_id": "MDQ6VXNlcjE0NjM0OTE=",
      "organizations_url": "https://api.github.com/users/bassam/orgs",
      "received_events_url": "https://api.github.com/users/bassam/received_events",
      "repos_url": "https://api.github.com/users/bassam/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/bassam/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/bassam/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/bassam"
    },
    "description": "* Multiple storage backends\r\n  * Full design\r\n  * Refactor code base and repositories to enable\r\n  * Consider support for Minio, potentially early support for other backends time permitting\r\n* Switch CRDs to API Aggregation\r\n* Run on arbitrary PVs\r\n* Remove Rook API and CLI\r\n* Migrate CI and release pipelines to a solution hosted by the CNCF\r\n* Run with Least Privileged and possibly without privileged containers\r\n* Shutdown / restart issues\r\n* Support Kubernetes 1.7+ only\r\n* Ceph features and improvements\r\n  * Adding/removing disks (lifecycle issues, failures, 1 OSD per pod, etc.)\r\n  * Placement group balancer support (ceph-mgr module)\r\n  * Mon reliability (restarts, failing over too fast, ip changes, etc.)\r\n  * Mimic support",
    "due_on": null,
    "html_url": "https://github.com/rook/rook/milestone/8",
    "id": 2891896,
    "labels_url": "https://api.github.com/repos/rook/rook/milestones/8/labels",
    "node_id": "MDk6TWlsZXN0b25lMjg5MTg5Ng==",
    "number": 8,
    "open_issues": 0,
    "state": "closed",
    "title": "0.8",
    "updated_at": "2020-01-28T21:43:40Z",
    "url": "https://api.github.com/repos/rook/rook/milestones/8"
  },
  "node_id": "MDU6SXNzdWUzNDMzMTQ2OTY=",
  "number": 1925,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "\"OSD Prepare running on nodes that are 'invalid'\" (e.g. placement not matching)",
  "updated_at": "2018-07-31T16:27:52Z",
  "url": "https://api.github.com/repos/rook/rook/issues/1925",
  "user": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/7836805?v=4",
    "events_url": "https://api.github.com/users/arkanmgerges/events{/privacy}",
    "followers_url": "https://api.github.com/users/arkanmgerges/followers",
    "following_url": "https://api.github.com/users/arkanmgerges/following{/other_user}",
    "gists_url": "https://api.github.com/users/arkanmgerges/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/arkanmgerges",
    "id": 7836805,
    "login": "arkanmgerges",
    "node_id": "MDQ6VXNlcjc4MzY4MDU=",
    "organizations_url": "https://api.github.com/users/arkanmgerges/orgs",
    "received_events_url": "https://api.github.com/users/arkanmgerges/received_events",
    "repos_url": "https://api.github.com/users/arkanmgerges/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/arkanmgerges/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/arkanmgerges/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/arkanmgerges"
  }
}