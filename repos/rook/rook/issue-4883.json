{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "<!-- **Are you in the right place?**\r\n1. For issues or feature requests, please create an issue in this repository.\r\n2. For general technical and non-technical questions, we are happy to help you on our [Rook.io Slack](https://slack.rook.io/).\r\n3. Did you already search the existing open issues for anything similar? -->\r\n\r\n**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:** erasureCoded-Pool `ec-data-pool-v2` for CRD `cephblockpools.ceph.rook.io` Status is failed\r\n\r\nCLI# `kubectl -n rook-ceph-conx0 get cephblockpools.ceph.rook.io ec-data-pool-v2 -o yaml`\r\n```YAML\r\napiVersion: ceph.rook.io/v1\r\nkind: CephBlockPool\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"ceph.rook.io/v1\",\"kind\":\"CephBlockPool\",\"metadata\":{\"annotations\":{},\"name\":\"ec-data-pool-v2\",\"namespace\":\"rook-ceph-conx0\"},\"spec\":{\"erasureCoded\":{\"codingChunks\":1,\"dataChunks\":2}}}\r\n  creationTimestamp: \"2020-02-18T17:38:01Z\"\r\n  generation: 4\r\n  name: ec-data-pool-v2\r\n  namespace: rook-ceph-conx0\r\n  resourceVersion: \"180651853\"\r\n  selfLink: /apis/ceph.rook.io/v1/namespaces/rook-ceph-conx0/cephblockpools/ec-data-pool-v2\r\n  uid: 694a67d2-5275-11ea-a8c2-005056a70090\r\nspec:\r\n  crushRoot: \"\"\r\n  deviceClass: \"\"\r\n  erasureCoded:\r\n    algorithm: \"\"\r\n    codingChunks: 1\r\n    dataChunks: 2\r\n  failureDomain: \"\"\r\n  replicated:\r\n    size: 0\r\n    targetSizeRatio: 0\r\nstatus:\r\n  phase: Failed\r\n```\r\n\r\n**Expected behavior:** erasureCoded-Pool `ec-data-pool-v2` for CRD `cephblockpools.ceph.rook.io` Status is ready\r\n\r\n**How to reproduce it (minimal and precise):**\r\nYAML: `storageclass-ec-k8s-conx0_revision-b.yaml` to create a CephBlockPool as erasureCoded\r\n```yaml\r\napiVersion: ceph.rook.io/v1\r\nkind: CephBlockPool\r\nmetadata:\r\n  name: ec-data-pool-v2\r\n  namespace: rook-ceph-conx0\r\nspec:\r\n  # Make sure you have enough nodes and OSDs running bluestore to support the replica size or erasure code chunks.\r\n  # For the below settings, you need at least 3 OSDs on different nodes (because the `failureDomain` is `host` by default).\r\n  erasureCoded:\r\n    dataChunks: 2\r\n    codingChunks: 1\r\n```\r\n\r\n1.) Create cephBlockPool \r\n```\r\n$ kubectl apply -f storageclass-ec-k8s-conx0_revision-b.yaml\r\ncephblockpool.ceph.rook.io/ec-data-pool-v2 created\r\n```\r\n\r\n2.) Status for CephBlockPool Name: ec-data-pool-v2\r\n```\r\n[2020-02-18 18:43:28 user@m01 rook-20200131]\r\n $ kubectl -n rook-ceph-conx0 get cephblockpools.ceph.rook.io ec-data-pool-v2\r\nNAME              AGE\r\nec-data-pool-v2   5m45s\r\n[2020-02-18 18:43:49 user@m01 rook-20200131]\r\n $ kubectl -n rook-ceph-conx0 describe cephblockpools.ceph.rook.io ec-data-pool-v2\r\nName:         ec-data-pool-v2\r\nNamespace:    rook-ceph-conx0\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration:\r\n                {\"apiVersion\":\"ceph.rook.io/v1\",\"kind\":\"CephBlockPool\",\"metadata\":{\"annotations\":{},\"name\":\"ec-data-pool-v2\",\"namespace\":\"rook-ceph-conx0\"...\r\nAPI Version:  ceph.rook.io/v1\r\nKind:         CephBlockPool\r\nMetadata:\r\n  Creation Timestamp:  2020-02-18T17:38:01Z\r\n  Generation:          4\r\n  Resource Version:    180651853\r\n  Self Link:           /apis/ceph.rook.io/v1/namespaces/rook-ceph-conx0/cephblockpools/ec-data-pool-v2\r\n  UID:                 694a67d2-5275-11ea-a8c2-005056a70090\r\nSpec:\r\n  Crush Root:\r\n  Device Class:\r\n  Erasure Coded:\r\n    Algorithm:\r\n    Coding Chunks:  1\r\n    Data Chunks:    2\r\n  Failure Domain:\r\n  Replicated:\r\n    Size:               0\r\n    Target Size Ratio:  0\r\nStatus:\r\n  Phase:  Failed\r\nEvents:   <none>\r\n[2020-02-18 18:43:54 user@m01 rook-20200131]\r\n $\r\n```\r\n\r\n**File(s) to submit**:\r\n\r\n* Operator's logs: \r\n[rook-ceph-operator-logs__cephblock-with-ec-failed.txt](https://github.com/rook/rook/files/4220973/rook-ceph__cephblock-with-ec-failed_rook-ceph-operator.txt)\r\n\r\n````\r\n $ kubectl -n rook-ceph-conx0 logs -l app=rook-ceph-operator -f\r\n2020-02-18 17:38:07.477201 I | cephclient: creating EC pool ec-data-pool-v2 succeeded, buf:\r\n2020-02-18 17:38:07.477225 I | op-pool: created pool \"ec-data-pool-v2\"\r\n2020-02-18 17:38:07.488173 D | clusterdisruption-controller: discovered NamespacedName: \"rook-ceph-conx0/rook-ceph-conx0\"\r\n2020-02-18 17:38:07.488214 D | clusterdisruption-controller: reconciling \"rook-ceph-conx0/rook-ceph-conx0\"\r\n2020-02-18 17:38:07.488492 E | op-pool: failed to update pool \"ec-data-pool-v2\". erasurecoded update not allowed\r\n2020-02-18 17:38:07.497834 D | clusterdisruption-controller: discovered NamespacedName: \"rook-ceph-conx0/rook-ceph-conx0\"\r\n2020-02-18 17:38:07.497870 D | clusterdisruption-controller: reconciling \"rook-ceph-conx0/rook-ceph-conx0\"\r\n2020-02-18 17:38:07.498052 E | op-pool: failed to update pool \"ec-data-pool-v2\". erasurecoded update not allowed\r\n2020-02-18 17:38:07.501155 E | op-pool: failed to update pool \"ec-data-pool-v2\". erasurecoded update not allowed\r\n````\r\n\r\n* Details of Ceph Pool `ec-data-pool-v2`\r\n```sh \r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]# ceph osd pool get ec-data-pool-v2 all\r\nsize: 3\r\nmin_size: 2\r\npg_num: 8\r\npgp_num: 8\r\ncrush_rule: ec-data-pool-v2\r\nhashpspool: true\r\nallow_ec_overwrites: true\r\nnodelete: false\r\nnopgchange: false\r\nnosizechange: false\r\nwrite_fadvise_dontneed: false\r\nnoscrub: false\r\nnodeep-scrub: false\r\nuse_gmt_hitset: 1\r\nerasure_code_profile: ec-data-pool-v2_ecprofile\r\nfast_read: 0\r\npg_autoscale_mode: warn\r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]# ceph osd erasure-code-profile get ec-data-pool-v2_ecprofile\r\ncrush-device-class=\r\ncrush-failure-domain=host\r\ncrush-root=default\r\njerasure-per-chunk-alignment=false\r\nk=2\r\nm=1\r\nplugin=jerasure\r\ntechnique=reed_sol_van\r\nw=8\r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]#\r\n```\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release):\r\n```sh\r\n $ cat /etc/os-release\r\nNAME=\"CentOS Linux\"\r\nVERSION=\"7 (Core)\"\r\nID=\"centos\"\r\nID_LIKE=\"rhel fedora\"\r\nVERSION_ID=\"7\"\r\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:/o:centos:centos:7\"\r\nHOME_URL=\"https://www.centos.org/\"\r\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\r\n\r\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT=\"centos\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n```\r\n* Kernel (e.g. `uname -a`):\r\n```sh\r\n $ uname -a\r\nLinux contn08 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n* Cloud provider or hardware configuration:\r\n* Rook version (use `rook version` inside of a Rook Pod): \r\n```sh\r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]# rook version\r\nrook: v1.2.4\r\ngo: go1.11\r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]#\r\n```\r\n* Storage backend version (e.g. for ceph do `ceph -v`):\r\n```sh\r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]# ceph -v\r\nceph version 14.2.6 (f0aa067ac7a02ee46ea48aa26c6e298b5ea272e9) nautilus (stable)\r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]#\r\n```\r\n\r\n* Kubernetes version (use `kubectl version`):\r\n```sh\r\n $ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.6\", GitCommit:\"72c30166b2105cd7d3350f2c28a219e6abcd79eb\", GitTreeState:\"clean\", BuildDate:\"2020-01-18T23:23:21Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift): Rancher 2.3.5\r\n\r\n* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox](https://rook.io/docs/rook/master/ceph-toolbox.html)):\r\n```sh\r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]# ceph health\r\nHEALTH_WARN too few PGs per OSD (2 < min 30)\r\n[root@rook-ceph-tools-9b7b66bbb-cgnrx /]#\r\n```\r\n",
  "closed_at": "2020-02-19T10:45:46Z",
  "closed_by": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "comments": 2,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/4883/comments",
  "created_at": "2020-02-18T18:40:41Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/4883/events",
  "html_url": "https://github.com/rook/rook/issues/4883",
  "id": 567084458,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    },
    {
      "color": "ef5c55",
      "default": false,
      "description": "main ceph tag",
      "id": 479456042,
      "name": "ceph",
      "node_id": "MDU6TGFiZWw0Nzk0NTYwNDI=",
      "url": "https://api.github.com/repos/rook/rook/labels/ceph"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/4883/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1NjcwODQ0NTg=",
  "number": 4883,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "cephblockpools: erasurecoded update not allowed",
  "updated_at": "2020-02-19T10:45:46Z",
  "url": "https://api.github.com/repos/rook/rook/issues/4883",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/53277782?v=4",
    "events_url": "https://api.github.com/users/lugg1/events{/privacy}",
    "followers_url": "https://api.github.com/users/lugg1/followers",
    "following_url": "https://api.github.com/users/lugg1/following{/other_user}",
    "gists_url": "https://api.github.com/users/lugg1/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/lugg1",
    "id": 53277782,
    "login": "lugg1",
    "node_id": "MDQ6VXNlcjUzMjc3Nzgy",
    "organizations_url": "https://api.github.com/users/lugg1/orgs",
    "received_events_url": "https://api.github.com/users/lugg1/received_events",
    "repos_url": "https://api.github.com/users/lugg1/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/lugg1/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/lugg1/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/lugg1"
  }
}