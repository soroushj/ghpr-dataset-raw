{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "<!-- **Are you in the right place?**\r\n1. For issues or feature requests, please create an issue in this repository.\r\n2. For general technical and non-technical questions, we are happy to help you on our [Rook.io Slack](https://slack.rook.io/).\r\n3. Did you already search the existing open issues for anything similar? -->\r\n\r\n**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\n\r\n`rook-ceph` v1.3 couldn't work properly in Minikube because of errors or issues which include:\r\n- port conflict on port `9090`\r\n- no properly configured `cluster.yaml` file\r\n- docs are not reflected on how to run `rook-ceph` in Minikube\r\n\r\n**Expected behavior:**\r\n\r\nBy following the docs, we can easily setup `rook-ceph` in Minikube for evaluation/demo purposes.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n<!-- Please let us know any circumstances for reproduction of your bug. -->\r\n\r\nLet's use the v1.3 release branch:\r\n\r\n```\r\ngit clone --single-branch --branch release-1.3 https://github.com/rook/rook.git\r\n```\r\n\r\nFirstly, there is a known issue (#5299) of port conflict in both rook-cepth v1.2 and v1.3 in `cluster/examples/kubernetes/ceph/operator.yaml`, to avoid that, we need to make a change:\r\n\r\n```yaml\r\nkind: ConfigMap\r\napiVersion: v1\r\nmetadata:\r\n  name: rook-ceph-operator-config\r\n  namespace: rook-ceph\r\ndata:\r\n  ...\r\n  # CSI_RBD_GRPC_METRICS_PORT: \"9090\"\r\n  CSI_RBD_GRPC_METRICS_PORT: \"9092\"\r\n```\r\n\r\nThen go through the basic installation process:\r\n\r\n```sh\r\nkubectl create -f rook/cluster/examples/kubernetes/ceph/common.yaml\r\nkubectl create -f rook/cluster/examples/kubernetes/ceph/operator.yaml   # with updated port\r\nkubectl create -f rook/cluster/examples/kubernetes/ceph/cluster.yaml      # don't know why the cluster-test.yaml in v1.2 is gone\r\n```\r\n\r\nAfter a while, everything _looks_ good except one of the `mon` pod is pending:\r\n\r\n```sh\r\n$ kubectl get pod -n rook-ceph\r\nNAME                                            READY   STATUS    RESTARTS   AGE\r\ncsi-cephfsplugin-5xxf8                          3/3     Running   0          13m\r\ncsi-cephfsplugin-provisioner-5975ffc644-j4bjf   5/5     Running   0          13m\r\ncsi-cephfsplugin-provisioner-5975ffc644-x7gsk   5/5     Running   0          13m\r\ncsi-rbdplugin-8c269                             3/3     Running   0          13m\r\ncsi-rbdplugin-provisioner-dc8b4dd6-92h2x        6/6     Running   0          13m\r\ncsi-rbdplugin-provisioner-dc8b4dd6-cqrsj        6/6     Running   0          13m\r\nrook-ceph-mon-a-canary-6b886f8946-9lvq7         1/1     Running   0          86s\r\nrook-ceph-mon-b-canary-5bdd96f45-9dd9r          0/1     Pending   0          85s\r\nrook-ceph-operator-599765ff49-qvjn5             1/1     Running   0          24m\r\nrook-discover-h554n                             1/1     Running   0          22m\r\n\r\n$ kubectl describe pod rook-ceph-mon-b-canary-5bdd96f45-9dd9r -n rook-ceph\r\n...\r\nEvents:\r\n  Type     Reason            Age   From               Message\r\n  ----     ------            ----  ----               -------\r\n  Warning  FailedScheduling  66s   default-scheduler  0/1 nodes are available: 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.\r\n```\r\n\r\nOf course, because we have only one node here in Minikube so let's change a bit on the `cluster.yaml`:\r\n```yaml\r\napiVersion: ceph.rook.io/v1\r\nkind: CephCluster\r\nmetadata:\r\n  name: rook-ceph\r\n  namespace: rook-ceph\r\nspec:\r\n  ...\r\n  mon:\r\n    count: 1  # reduced to 1 from 3\r\n```\r\n\r\nNow, let's delete the operator and cluster, together with Minikube's data folder and re-install:\r\n\r\n```sh\r\nkubectl delete -f rook/cluster/examples/kubernetes/ceph/cluster.yaml\r\nkubectl delete -f rook/cluster/examples/kubernetes/ceph/operator.yaml\r\nminikube ssh \"sudo rm -rf /var/lib/rook\"\r\n\r\nkubectl create -f rook/cluster/examples/kubernetes/ceph/operator.yaml   # with updated port\r\nkubectl create -f rook/cluster/examples/kubernetes/ceph/cluster.yaml      # with reduced mon\r\n```\r\n\r\nThen everything _looks_ great:\r\n\r\n```sh\r\nNAME                                                 READY   STATUS      RESTARTS   AGE\r\ncsi-cephfsplugin-fr4qw                               3/3     Running     0          112s\r\ncsi-cephfsplugin-provisioner-5975ffc644-hd6n7        5/5     Running     0          112s\r\ncsi-cephfsplugin-provisioner-5975ffc644-njsgc        5/5     Running     0          112s\r\ncsi-rbdplugin-frzqr                                  3/3     Running     0          113s\r\ncsi-rbdplugin-provisioner-dc8b4dd6-bc5ws             6/6     Running     0          113s\r\ncsi-rbdplugin-provisioner-dc8b4dd6-xfs5b             6/6     Running     0          113s\r\nrook-ceph-crashcollector-minikube-7ccc7494cf-2dt2n   1/1     Running     0          85s\r\nrook-ceph-mgr-a-767878578c-hvwt6                     1/1     Running     0          85s\r\nrook-ceph-mon-a-798b9fff44-tsddt                     1/1     Running     0          93s\r\nrook-ceph-operator-599765ff49-4hn7r                  1/1     Running     0          2m14s\r\nrook-ceph-osd-prepare-minikube-dztj7                 0/1     Completed   0          71s\r\nrook-discover-zthzs                                  1/1     Running     0          2m13\r\n```\r\n\r\nAnd then install the `toolbox` and exec into it:\r\n\r\n```sh\r\n$ kubectl apply -f rook/cluster/examples/kubernetes/ceph/toolbox.yaml\r\n\r\n$ kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') bash\r\n[root@rook-ceph-tools-877c4d966-k6bvg /]# ceph status\r\n  cluster:\r\n    id:     5195b163-c59b-494a-9a07-e273a533e349\r\n    health: HEALTH_WARN\r\n            OSD count 0 < osd_pool_default_size 3\r\n\r\n  services:\r\n    mon: 1 daemons, quorum a (age 3m)\r\n    mgr: a(active, since 3m)\r\n    osd: 0 osds: 0 up, 0 in\r\n\r\n  data:\r\n    pools:   0 pools, 0 pgs\r\n    objects: 0 objects, 0 B\r\n    usage:   0 B used, 0 B / 0 B avail\r\n    pgs:\r\n```\r\n\r\nThe status is `HEALTH_WARN` and no OSDs are there.\r\n\r\nLet's check out the logs of the `rook-ceph-osd-prepare-minikube` job:\r\n\r\n```sh\r\n$ kubectl logs -n rook-ceph --all-containers rook-ceph-osd-prepare-minikube-dztj7\r\n...\r\n2020-04-21 08:03:09.836426 I | cephosd: configuring osd devices: {\"Entries\":{}}\r\n2020-04-21 08:03:09.836538 I | cephosd: no new devices to configure. returning devices already configured with ceph-volume.\r\n2020-04-21 08:03:09.836547 D | exec: Running command: ceph-volume lvm list  --format json\r\n2020-04-21 08:03:10.438794 I | cephosd: 0 ceph-volume lvm osd devices configured on this node\r\n2020-04-21 08:03:10.438851 W | cephosd: skipping OSD configuration as no devices matched the storage settings for this node \"minikube\"\r\n```\r\n\r\nNote here: `cephosd: skipping OSD configuration as no devices matched the storage settings for this node \"minikube\"`\r\n\r\nSo to make it work, we may have to follow one of below two things:\r\n1. To use a folder in Minikube to create OSDs -- unfortunately, I didn't know how;\r\n2. (recommended) To simply add an extra disk and attach it to Minikube's VM, which works.\r\n\r\n**File(s) to submit**:\r\n\r\n* Cluster CR (custom resource), typically called `cluster.yaml`, if necessary\r\n* Operator's logs, if necessary\r\n* Crashing pod(s) logs, if necessary\r\n\r\n To get logs, use `kubectl -n <namespace> logs <pod name>`\r\nWhen pasting logs, always surround them with backticks or use the `insert code` button from the Github UI.\r\nRead [Github documentation if you need help](https://help.github.com/en/articles/creating-and-highlighting-code-blocks).\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release): `MacOS`\r\n* Kernel (e.g. `uname -a`): `Darwin Brights-MacBook-Pro.local 19.3.0 Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64 x86_64`\r\n* Rook version (use `rook version` inside of a Rook Pod):\r\n```\r\nrook version\r\nrook: v1.3.1\r\ngo: go1.13.8\r\n```\r\n* Storage backend version (e.g. for ceph do `ceph -v`):\r\n```\r\nceph version 14.2.8 (2d095e947a02261ce61424021bb43bd3022d35cb) nautilus (stable)\r\n```\r\n* Kubernetes version (use `kubectl version`):\r\n```\r\nkubectl version --short\r\nClient Version: v1.16.3\r\nServer Version: v1.16.3\r\n```\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift): Latest Minikube\r\n",
  "closed_at": "2020-04-23T16:16:37Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "comments": 6,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/5301/comments",
  "created_at": "2020-04-21T09:34:57Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/5301/events",
  "html_url": "https://github.com/rook/rook/issues/5301",
  "id": 603853985,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/5301/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU2MDM4NTM5ODU=",
  "number": 5301,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "rook-ceph v1.3 couldn't work in Minikube without some must-have changes",
  "updated_at": "2020-05-11T19:39:36Z",
  "url": "https://api.github.com/repos/rook/rook/issues/5301",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/1422425?v=4",
    "events_url": "https://api.github.com/users/brightzheng100/events{/privacy}",
    "followers_url": "https://api.github.com/users/brightzheng100/followers",
    "following_url": "https://api.github.com/users/brightzheng100/following{/other_user}",
    "gists_url": "https://api.github.com/users/brightzheng100/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/brightzheng100",
    "id": 1422425,
    "login": "brightzheng100",
    "node_id": "MDQ6VXNlcjE0MjI0MjU=",
    "organizations_url": "https://api.github.com/users/brightzheng100/orgs",
    "received_events_url": "https://api.github.com/users/brightzheng100/received_events",
    "repos_url": "https://api.github.com/users/brightzheng100/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/brightzheng100/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/brightzheng100/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/brightzheng100"
  }
}