{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/9363998?v=4",
    "events_url": "https://api.github.com/users/sp98/events{/privacy}",
    "followers_url": "https://api.github.com/users/sp98/followers",
    "following_url": "https://api.github.com/users/sp98/following{/other_user}",
    "gists_url": "https://api.github.com/users/sp98/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/sp98",
    "id": 9363998,
    "login": "sp98",
    "node_id": "MDQ6VXNlcjkzNjM5OTg=",
    "organizations_url": "https://api.github.com/users/sp98/orgs",
    "received_events_url": "https://api.github.com/users/sp98/received_events",
    "repos_url": "https://api.github.com/users/sp98/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/sp98/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/sp98/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/sp98"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars2.githubusercontent.com/u/9363998?v=4",
      "events_url": "https://api.github.com/users/sp98/events{/privacy}",
      "followers_url": "https://api.github.com/users/sp98/followers",
      "following_url": "https://api.github.com/users/sp98/following{/other_user}",
      "gists_url": "https://api.github.com/users/sp98/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/sp98",
      "id": 9363998,
      "login": "sp98",
      "node_id": "MDQ6VXNlcjkzNjM5OTg=",
      "organizations_url": "https://api.github.com/users/sp98/orgs",
      "received_events_url": "https://api.github.com/users/sp98/received_events",
      "repos_url": "https://api.github.com/users/sp98/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/sp98/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sp98/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/sp98"
    }
  ],
  "author_association": "NONE",
  "body": "<!-- **Are you in the right place?**\r\n1. For issues or feature requests, please create an issue in this repository.\r\n2. For general technical and non-technical questions, we are happy to help you on our [Rook.io Slack](https://Rook-io.slack.com).\r\n3. Did you already search the existing open issues for anything similar? -->\r\n\r\n**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\nOne of two osd pods failed randomly. \r\n\r\n**Expected behavior:**\r\nAll osd pods should be in \"Running\" state consistently.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n<!-- Please let us know any circumstances for reproduction of your bug. -->\r\n\r\n1). Failure case:  Two clusters with one cluster with 1 device 'vdd' and the other cluster with 1 device 'vde'. The two clusters are using the same set of nodes. Each node has both vdd and vde as storage node.  \r\n\r\nNote:  Some successful case for reference. \r\nCase 1: one rook-ceph cluster with the same set of storage nodes usinging \"vdd\" and \"vde\" .\r\nCase 2: two rook-ceph clusters with two different set of storage nodes each having one device 'vdd'.\r\nCase 3: two rook-ceph clustesr with the same set of storage nodes each having one device 'vdd' and one directory ( /data/rook)\r\n\r\n2). The 2nd cluster will have one or two osd pods fails to be brought up randomly. \r\n\r\n=======kubectl -n rook-ceph-system get pods -o wide  ====\r\nNAME                                  READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE\r\nrook-ceph-agent-6bh2q                 1/1     Running   0          29m   192.188.88.14   rook-control-01   <none>\r\nrook-ceph-agent-7qbfz                 1/1     Running   0          29m   192.188.88.12   rook-worker-03    <none>\r\nrook-ceph-agent-9dtj9                 1/1     Running   0          29m   192.188.88.8    rook-worker-02    <none>\r\nrook-ceph-agent-q4mp8                 1/1     Running   0          29m   192.188.88.13   rook-control-03   <none>\r\nrook-ceph-agent-rwgw6                 1/1     Running   0          29m   192.188.88.10   rook-control-02   <none>\r\nrook-ceph-agent-t5tv9                 1/1     Running   0          29m   192.188.88.17   rook-worker-01    <none>\r\nrook-ceph-operator-7545bffc9b-z66zd   1/1     Running   0          29m   192.168.1.188   rook-control-01   <none>\r\nrook-discover-7229c                   1/1     Running   0          29m   192.168.1.198   rook-worker-03    <none>\r\nrook-discover-gxvzl                   1/1     Running   0          29m   192.168.1.88    rook-control-03   <none>\r\nrook-discover-l2bm9                   1/1     Running   0          29m   192.168.1.173   rook-worker-01    <none>\r\nrook-discover-p82ft                   1/1     Running   0          29m   192.168.1.16    rook-control-02   <none>\r\nrook-discover-qkfpr                   1/1     Running   0          29m   192.168.1.136   rook-control-01   <none>\r\nrook-discover-vcr2m                   1/1     Running   0          29m   192.168.1.254   rook-worker-02    <none>\r\n=======kubectl -n rook-ceph get pods -o wide  ====\r\nNAME                                             READY   STATUS      RESTARTS   AGE   IP              NODE              NOMINATED NODE\r\nceph-oam-6dfd5ffccc-bl9cs                        2/2     Running     0          28m   192.188.88.8    rook-worker-02    <none>\r\nrook-ceph-mgr-a-f445598f9-hnhw4                  1/1     Running     0          28m   192.168.1.145   rook-control-01   <none>\r\nrook-ceph-mon-a-5cb7f699c7-6lztx                 1/1     Running     0          28m   192.168.1.134   rook-control-01   <none>\r\nrook-ceph-mon-b-56bd6c759c-xxx7q                 1/1     Running     0          28m   192.168.1.34    rook-control-02   <none>\r\nrook-ceph-mon-c-6c6b779f86-bnh4r                 1/1     Running     0          28m   192.168.1.91    rook-control-03   <none>\r\nrook-ceph-osd-0-5b6dd9f967-knvnz                 1/1     Running     0          27m   192.168.1.176   rook-control-01   <none>\r\nrook-ceph-osd-1-7dd9f79bb7-4q87x                 1/1     Running     0          27m   192.168.1.13    rook-control-02   <none>\r\nrook-ceph-osd-2-967dbf495-knllx                  1/1     Running     0          27m   192.168.1.98    rook-control-03   <none>\r\nrook-ceph-osd-prepare-rook-control-01-jv65h      0/2     Completed   1          27m   192.168.1.148   rook-control-01   <none>\r\nrook-ceph-osd-prepare-rook-control-02-fpxkm      0/2     Completed   1          27m   192.168.1.24    rook-control-02   <none>\r\nrook-ceph-osd-prepare-rook-control-03-xwjvr      0/2     Completed   1          27m   192.168.1.95    rook-control-03   <none>\r\nrook-ceph-rgw-rook-ceph-store-5f684dc766-4k7lt   1/1     Running     0          26m   192.168.1.196   rook-worker-02    <none>\r\nrook-ceph-tools-6b8b666d47-q5lj2                 1/1     Running     0          28m   192.188.88.12   rook-worker-03    <none>\r\n=======kubectl -n rook-ceph2 get pods -o wide  ====\r\nNAME                                          READY   STATUS             RESTARTS   AGE   IP              NODE              NOMINATED NODE\r\nceph-oam-6dfd5ffccc-67z74                     2/2     Running            0          28m   192.188.88.17   rook-worker-01    <none>\r\nrook-ceph-mgr-a-6b8b9596db-4ms8x              1/1     Running            0          26m   192.168.1.9     rook-control-02   <none>\r\nrook-ceph-mon-a-6669d55bc-v4h6d               1/1     Running            0          26m   192.168.1.132   rook-control-01   <none>\r\nrook-ceph-mon-b-84fcc48df4-hkfvd              1/1     Running            0          26m   192.168.1.35    rook-control-02   <none>\r\nrook-ceph-mon-c-5fbfd69c76-v68bv              1/1     Running            0          26m   192.168.1.97    rook-control-03   <none>\r\nrook-ceph-osd-0-78f9487846-d7qf8              1/1     Running            0          25m   192.168.1.128   rook-control-01   <none>\r\nrook-ceph-osd-1-5d7976f4d6-lzvw4              0/1     CrashLoopBackOff   9          25m   192.168.1.2     rook-control-02   <none>\r\nrook-ceph-osd-2-5646cd948d-czslc              1/1     Running            0          25m   192.168.1.99    rook-control-03   <none>\r\nrook-ceph-osd-prepare-rook-control-01-9g2d2   0/2     Completed          0          25m   192.168.1.155   rook-control-01   <none>\r\nrook-ceph-osd-prepare-rook-control-02-zgfkh   0/2     Completed          0          25m   192.168.1.14    rook-control-02   <none>\r\nrook-ceph-osd-prepare-rook-control-03-k4j5q   0/2     Completed          0          25m   192.168.1.100   rook-control-03   <none>\r\nrook-ceph-tools-6b8b666d47-qthc2              1/1     Running            0          28m   192.188.88.12   rook-worker-03    <none>\r\n\r\n3). The rook-ceph2 has one pod failure. And rook-ceph1 is healthy with all 3 pods up. \r\n\r\nceph -s\r\n  cluster:\r\n    id:     f6f4f98b-8ca2-4817-99d3-be22b0cded16\r\n    health: HEALTH_OK\r\n \r\n  services:\r\n    mon: 3 daemons, quorum b,c,a\r\n    mgr: a(active)\r\n    osd: 3 osds: 2 up, 2 in\r\n \r\n  data:\r\n    pools:   0 pools, 0 pgs\r\n    objects: 0  objects, 0 B\r\n    usage:   2.0 GiB used, 8.0 GiB / 10 GiB avail\r\n    pgs:     \r\n\r\n\r\nceph osd status\r\n+----+-----------------+-------+-------+--------+---------+--------+---------+------------+\r\n| id |       host      |  used | avail | wr ops | wr data | rd ops | rd data |   state    |\r\n+----+-----------------+-------+-------+--------+---------+--------+---------+------------+\r\n| 0  | rook-control-01 | 1026M | 4090M |    0   |     0   |    0   |     0   | exists,up  |\r\n| 1  |                 |    0  |    0  |    0   |     0   |    0   |     0   | exists,new |\r\n| 2  | rook-control-03 | 1026M | 4090M |    0   |     0   |    0   |     0   | exists,up  |\r\n+----+-----------------+-------+-------+--------+---------+--------+---------+------------+\r\n\r\n4). Failure pod log\r\n\r\nkubectl -n rook-ceph2 log rook-ceph-osd-1-5d7976f4d6-lzvw4\r\n2019-02-21 15:51:16.059213 I | rookcmd: starting Rook v0.9.2 with arguments '/rook/rook ceph osd start -- --foreground --id 1 --osd-uuid 33615b87-041b-46ef-b0ad-2b8965916930 --conf /var/lib/rook/osd1/rook-ceph2.config --cluster ceph'\r\n2019-02-21 15:51:16.059453 I | rookcmd: flag values: --help=false, --log-level=INFO, --osd-id=1, --osd-store-type=bluestore, --osd-uuid=33615b87-041b-46ef-b0ad-2b8965916930\r\n2019-02-21 15:51:16.059464 I | cephmon: parsing mon endpoints: \r\n2019-02-21 15:51:16.059469 W | cephmon: ignoring invalid monitor \r\n2019-02-21 15:51:16.059835 I | exec: Running command: ceph-volume lvm activate --no-systemd --bluestore 1 33615b87-041b-46ef-b0ad-2b8965916930\r\n2019-02-21 15:51:19.683866 I | Running command: /bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-1\r\n2019-02-21 15:51:19.683909 I | Running command: /usr/sbin/restorecon /var/lib/ceph/osd/ceph-1\r\n2019-02-21 15:51:19.683914 I | Running command: /bin/ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-a99d9aa9-e356-4fd3-96c8-aade831755f0/osd-data-b5ae0fcd-e483-4169-aa77-a51df8579ad3 --path /var/lib/ceph/osd/ceph-1 --no-mon-config\r\n2019-02-21 15:51:19.683920 I | Running command: /bin/ln -snf /dev/ceph-a99d9aa9-e356-4fd3-96c8-aade831755f0/osd-data-b5ae0fcd-e483-4169-aa77-a51df8579ad3 /var/lib/ceph/osd/ceph-1/block\r\n2019-02-21 15:51:19.683923 I | Running command: /bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-1/block\r\n2019-02-21 15:51:19.683926 I | Running command: /bin/chown -R ceph:ceph /dev/mapper/ceph--a99d9aa9--e356--4fd3--96c8--aade831755f0-osd--data--b5ae0fcd--e483--4169--aa77--a51df8579ad3\r\n2019-02-21 15:51:19.683929 I | Running command: /bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-1\r\n2019-02-21 15:51:19.683932 I | --> ceph-volume lvm activate successful for osd ID: 1\r\n2019-02-21 15:51:19.694440 I | exec: Running command: ceph-osd --foreground --id 1 --osd-uuid 33615b87-041b-46ef-b0ad-2b8965916930 --conf /var/lib/rook/osd1/rook-ceph2.config --cluster ceph\r\n2019-02-21 15:51:19.805074 I | failed to fetch mon config (--no-mon-config to skip)\r\nfailed to start osd. Failed to complete '': exit status 1. \r\n\r\n5). The logs will be attached. \r\n[rook-logs02-21.gz](https://github.com/rook/rook/files/2890125/rook-logs02-21.gz)\r\n\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release): rhel 7.5\r\n* Kernel (e.g. `uname -a`): Linux rook-control-01 3.10.0-862.14.4.el7.x86_64 #1 SMP Fri Sep 21 09:07:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n* Cloud provider or hardware configuration: openstack\r\n* Rook version (use `rook version` inside of a Rook Pod): v0.9.2\r\n* Kubernetes version (use `kubectl version`): v1.12.3\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift): Tectonic\r\n* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox] (https://rook.io/docs/Rook/master/toolbox.html)):\r\n",
  "closed_at": "2019-06-06T20:03:12Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "comments": 22,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/2696/comments",
  "created_at": "2019-02-21T16:31:46Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/2696/events",
  "html_url": "https://github.com/rook/rook/issues/2696",
  "id": 413014099,
  "labels": [
    {
      "color": "ef5c55",
      "default": false,
      "description": "main ceph tag",
      "id": 479456042,
      "name": "ceph",
      "node_id": "MDU6TGFiZWw0Nzk0NTYwNDI=",
      "url": "https://api.github.com/repos/rook/rook/labels/ceph"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/2696/labels{/name}",
  "locked": false,
  "milestone": {
    "closed_at": "2020-01-28T21:43:45Z",
    "closed_issues": 119,
    "created_at": "2018-11-21T16:40:13Z",
    "creator": {
      "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
      "events_url": "https://api.github.com/users/travisn/events{/privacy}",
      "followers_url": "https://api.github.com/users/travisn/followers",
      "following_url": "https://api.github.com/users/travisn/following{/other_user}",
      "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/travisn",
      "id": 1048514,
      "login": "travisn",
      "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
      "organizations_url": "https://api.github.com/users/travisn/orgs",
      "received_events_url": "https://api.github.com/users/travisn/received_events",
      "repos_url": "https://api.github.com/users/travisn/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/travisn"
    },
    "description": "",
    "due_on": null,
    "html_url": "https://github.com/rook/rook/milestone/11",
    "id": 3838796,
    "labels_url": "https://api.github.com/repos/rook/rook/milestones/11/labels",
    "node_id": "MDk6TWlsZXN0b25lMzgzODc5Ng==",
    "number": 11,
    "open_issues": 0,
    "state": "closed",
    "title": "1.0",
    "updated_at": "2020-01-28T21:43:45Z",
    "url": "https://api.github.com/repos/rook/rook/milestones/11"
  },
  "node_id": "MDU6SXNzdWU0MTMwMTQwOTk=",
  "number": 2696,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "The osd pod failed randomly when creating 2 rook-ceph clusters on the same set of storage nodes",
  "updated_at": "2019-06-06T20:03:13Z",
  "url": "https://api.github.com/repos/rook/rook/issues/2696",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/19477896?v=4",
    "events_url": "https://api.github.com/users/yan636/events{/privacy}",
    "followers_url": "https://api.github.com/users/yan636/followers",
    "following_url": "https://api.github.com/users/yan636/following{/other_user}",
    "gists_url": "https://api.github.com/users/yan636/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/yan636",
    "id": 19477896,
    "login": "yan636",
    "node_id": "MDQ6VXNlcjE5NDc3ODk2",
    "organizations_url": "https://api.github.com/users/yan636/orgs",
    "received_events_url": "https://api.github.com/users/yan636/received_events",
    "repos_url": "https://api.github.com/users/yan636/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/yan636/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/yan636/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/yan636"
  }
}