{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/15130992?v=4",
    "events_url": "https://api.github.com/users/ashishranjan738/events{/privacy}",
    "followers_url": "https://api.github.com/users/ashishranjan738/followers",
    "following_url": "https://api.github.com/users/ashishranjan738/following{/other_user}",
    "gists_url": "https://api.github.com/users/ashishranjan738/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/ashishranjan738",
    "id": 15130992,
    "login": "ashishranjan738",
    "node_id": "MDQ6VXNlcjE1MTMwOTky",
    "organizations_url": "https://api.github.com/users/ashishranjan738/orgs",
    "received_events_url": "https://api.github.com/users/ashishranjan738/received_events",
    "repos_url": "https://api.github.com/users/ashishranjan738/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/ashishranjan738/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ashishranjan738/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/ashishranjan738"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars1.githubusercontent.com/u/15130992?v=4",
      "events_url": "https://api.github.com/users/ashishranjan738/events{/privacy}",
      "followers_url": "https://api.github.com/users/ashishranjan738/followers",
      "following_url": "https://api.github.com/users/ashishranjan738/following{/other_user}",
      "gists_url": "https://api.github.com/users/ashishranjan738/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/ashishranjan738",
      "id": 15130992,
      "login": "ashishranjan738",
      "node_id": "MDQ6VXNlcjE1MTMwOTky",
      "organizations_url": "https://api.github.com/users/ashishranjan738/orgs",
      "received_events_url": "https://api.github.com/users/ashishranjan738/received_events",
      "repos_url": "https://api.github.com/users/ashishranjan738/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/ashishranjan738/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ashishranjan738/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/ashishranjan738"
    }
  ],
  "author_association": "MEMBER",
  "body": "<!-- **Are you in the right place?**\r\n1. For issues or feature requests, please create an issue in this repository.\r\n2. For general technical and non-technical questions, we are happy to help you on our [Rook.io Slack](https://slack.rook.io/).\r\n3. Did you already search the existing open issues for anything similar? -->\r\n\r\n**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\nIf all mons are down (e.g. set the replicas to 0 on all the mon deployments) and the operator is restarted, the operator will wait indefinitely for a single mon to start. \r\n\r\n**Expected behavior:**\r\nThe operator should start all mons before checking for quorum. This will also help with disaster recovery scenarios.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n<!-- Please let us know any circumstances for reproduction of your bug. -->\r\n- Start a cluster with three mons\r\n- Change the mon deployment replicas to 0\r\n- Restart the operator\r\n- See the operator get stuck as seen in the log below\r\n\r\n```\r\n2019-12-18 17:33:49.119871 I | rookcmd: starting Rook v1.2.0-beta.1.16.g32e0289c5 with arguments '/usr/local/bin/rook ceph operator'\r\n2019-12-18 17:33:49.120133 I | rookcmd: flag values: --add_dir_header=false, --alsologtostderr=false, --csi-attacher-image=quay.io/k8scsi/csi-attacher:v1.2.0, --csi-ceph-image=quay.io/cephcsi/cephcsi:v1.2.2, --csi-cephfs-plugin-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin.yaml, --csi-cephfs-provisioner-dep-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-dep.yaml, --csi-cephfs-provisioner-sts-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-sts.yaml, --csi-driver-name-prefix=, --csi-enable-cephfs=true, --csi-enable-grpc-metrics=true, --csi-enable-rbd=true, --csi-kubelet-dir-path=/var/lib/kubelet, --csi-provisioner-image=quay.io/k8scsi/csi-provisioner:v1.4.0, --csi-rbd-plugin-template-path=/etc/ceph-csi/rbd/csi-rbdplugin.yaml, --csi-rbd-provisioner-dep-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-dep.yaml, --csi-rbd-provisioner-sts-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-sts.yaml, --csi-registrar-image=quay.io/k8scsi/csi-node-driver-registrar:v1.1.0, --csi-snapshotter-image=quay.io/k8scsi/csi-snapshotter:v1.2.2, --enable-discovery-daemon=true, --enable-flex-driver=false, --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-flush-frequency=5s, --log-level=INFO, --log_backtrace_at=:0, --log_dir=, --log_file=, --log_file_max_size=1800, --logtostderr=true, --master=, --mon-healthcheck-interval=45s, --mon-out-timeout=10m0s, --operator-image=, --service-account=, --skip_headers=false, --skip_log_headers=false, --stderrthreshold=2, --v=0, --vmodule=\r\n2019-12-18 17:33:49.120261 I | cephcmd: starting operator\r\n2019-12-18 17:33:49.145777 I | op-discover: rook-discover daemonset started\r\n2019-12-18 17:33:49.147270 I | operator: rook-provisioner ceph.rook.io/block started using ceph.rook.io flex vendor dir\r\n2019-12-18 17:33:49.147664 I | operator: rook-provisioner rook.io/block started using rook.io flex vendor dir\r\n2019-12-18 17:33:49.147682 I | operator: Watching all namespaces for cluster CRDs\r\n2019-12-18 17:33:49.147689 I | op-cluster: start watching clusters in all namespaces\r\n2019-12-18 17:33:49.147704 I | op-cluster: Enabling hotplug orchestration: ROOK_DISABLE_DEVICE_HOTPLUG=false\r\nI1218 17:33:49.147790       6 leaderelection.go:217] attempting to acquire leader lease  rook-ceph/ceph.rook.io-block...\r\n2019-12-18 17:33:49.148475 I | operator: setting up the controller-runtime manager\r\nI1218 17:33:49.155076       6 leaderelection.go:217] attempting to acquire leader lease  rook-ceph/rook.io-block...\r\n2019-12-18 17:33:49.193988 I | op-cluster: starting cluster in namespace rook-ceph\r\n2019-12-18 17:33:49.563964 I | operator: starting the controller-runtime manager\r\n2019-12-18 17:33:50.140986 I | ceph-csi: CSIDriver CRD already had been registered for \"rook-ceph.rbd.csi.ceph.com\"\r\n2019-12-18 17:33:50.143980 I | ceph-csi: CSIDriver CRD already had been registered for \"rook-ceph.cephfs.csi.ceph.com\"\r\n2019-12-18 17:33:50.144155 I | operator: successfully started Ceph CSI driver(s)\r\n2019-12-18 17:33:56.145376 I | op-cluster: detecting the ceph image version for image ceph/ceph:v14.2.5...\r\n2019-12-18 17:33:58.500611 I | op-cluster: Detected ceph image version: \"14.2.5-0 nautilus\"\r\n2019-12-18 17:33:58.509617 I | op-mon: parsing mon endpoints: a=10.105.21.14:6789,b=10.99.55.86:6789,c=10.111.202.138:6789\r\n2019-12-18 17:33:58.509932 I | op-mon: loaded: maxMonID=2, mons=map[a:0xc000d4edc0 b:0xc000d4ee00 c:0xc000d4ee40], mapping=&{Node:map[a:0xc000d00a80 b:0xc000d00ab0 c:0xc000d00ae0]}\r\n2019-12-18 17:33:58.510220 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2019-12-18 17:33:58.510277 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2019-12-18 17:33:58.510468 I | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/648665918\r\n^[[CI1218 17:34:04.509390       6 leaderelection.go:227] successfully acquired lease rook-ceph/ceph.rook.io-block\r\nI1218 17:34:04.509563       6 event.go:209] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"rook-ceph\", Name:\"ceph.rook.io-block\", UID:\"309e562f-2a10-449b-ab1b-a7aeec6c7edb\", APIVersion:\"v1\", ResourceVersion:\"4436\", FieldPath:\"\"}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-56654965c7-xxwfn_8d015bae-21bc-11ea-ad13-0242ac110005 became leader\r\nI1218 17:34:04.509826       6 controller.go:769] Starting provisioner controller ceph.rook.io/block_rook-ceph-operator-56654965c7-xxwfn_8d015bae-21bc-11ea-ad13-0242ac110005!\r\nI1218 17:34:04.610361       6 controller.go:818] Started provisioner controller ceph.rook.io/block_rook-ceph-operator-56654965c7-xxwfn_8d015bae-21bc-11ea-ad13-0242ac110005!\r\nI1218 17:34:07.057951       6 leaderelection.go:227] successfully acquired lease rook-ceph/rook.io-block\r\nI1218 17:34:07.058502       6 event.go:209] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"rook-ceph\", Name:\"rook.io-block\", UID:\"d4bd164e-ecdf-4290-9926-eebc0deec5c7\", APIVersion:\"v1\", ResourceVersion:\"4442\", FieldPath:\"\"}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-56654965c7-xxwfn_8d0175fd-21bc-11ea-ad13-0242ac110005 became leader\r\nI1218 17:34:07.058748       6 controller.go:769] Starting provisioner controller rook.io/block_rook-ceph-operator-56654965c7-xxwfn_8d0175fd-21bc-11ea-ad13-0242ac110005!\r\nI1218 17:34:07.160018       6 controller.go:818] Started provisioner controller rook.io/block_rook-ceph-operator-56654965c7-xxwfn_8d0175fd-21bc-11ea-ad13-0242ac110005!\r\n2019-12-18 17:34:13.661117 I | exec: timed out\r\n2019-12-18 17:34:13.664117 E | op-cluster: failed to get ceph daemons versions. failed to run 'ceph versions: exit status 1\r\n2019-12-18 17:34:13.664216 I | op-cluster: CephCluster \"rook-ceph\" status: \"Creating\". \r\n2019-12-18 17:34:13.677472 I | op-mon: start running mons\r\n2019-12-18 17:34:13.683411 I | op-mon: parsing mon endpoints: a=10.105.21.14:6789,b=10.99.55.86:6789,c=10.111.202.138:6789\r\n2019-12-18 17:34:13.683518 I | op-mon: loaded: maxMonID=2, mons=map[a:0xc000611660 b:0xc0006116e0 c:0xc000611740], mapping=&{Node:map[a:0xc000642d80 b:0xc000642db0 c:0xc000642de0]}\r\n2019-12-18 17:34:13.691324 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.105.21.14:6789\",\"10.99.55.86:6789\",\"10.111.202.138:6789\"]}] data:c=10.111.202.138:6789,a=10.105.21.14:6789,b=10.99.55.86:6789 mapping:{\"node\":{\"a\":{\"Name\":\"minikube\",\"Hostname\":\"minikube\",\"Address\":\"10.0.2.15\"},\"b\":{\"Name\":\"minikube\",\"Hostname\":\"minikube\",\"Address\":\"10.0.2.15\"},\"c\":{\"Name\":\"minikube\",\"Hostname\":\"minikube\",\"Address\":\"10.0.2.15\"}}} maxMonId:2]\r\n2019-12-18 17:34:13.697357 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2019-12-18 17:34:13.697439 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2019-12-18 17:34:14.277749 I | op-mon: targeting the mon count 3\r\n2019-12-18 17:34:14.277969 I | exec: Running command: ceph config set global mon_allow_pool_delete true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/232587141\r\n2019-12-18 17:34:29.377155 I | exec: timed out\r\n2019-12-18 17:34:29.377376 W | op-mon: failed to set Rook and/or user-defined Ceph config options before starting mons; will retry after starting mons. failed to apply default Ceph configurations: failed to set one or more Ceph configs: failed to set ceph config in the centralized mon configuration database; you may need to use the rook-config-override ConfigMap. output: timed out\r\n: exit status 1\r\n2019-12-18 17:34:29.377395 I | op-mon: checking for basic quorum with existing mons\r\n2019-12-18 17:34:29.404291 I | op-mon: mon \"a\" endpoint are [v2:10.105.21.14:3300,v1:10.105.21.14:6789]\r\n2019-12-18 17:34:29.426743 I | op-mon: mon \"b\" endpoint are [v2:10.99.55.86:3300,v1:10.99.55.86:6789]\r\n2019-12-18 17:34:29.454535 I | op-mon: mon \"c\" endpoint are [v2:10.111.202.138:3300,v1:10.111.202.138:6789]\r\n2019-12-18 17:34:29.753650 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.105.21.14:6789\",\"10.99.55.86:6789\",\"10.111.202.138:6789\"]}] data:a=10.105.21.14:6789,b=10.99.55.86:6789,c=10.111.202.138:6789 mapping:{\"node\":{\"a\":{\"Name\":\"minikube\",\"Hostname\":\"minikube\",\"Address\":\"10.0.2.15\"},\"b\":{\"Name\":\"minikube\",\"Hostname\":\"minikube\",\"Address\":\"10.0.2.15\"},\"c\":{\"Name\":\"minikube\",\"Hostname\":\"minikube\",\"Address\":\"10.0.2.15\"}}} maxMonId:2]\r\n2019-12-18 17:34:30.153701 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2019-12-18 17:34:30.153913 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2019-12-18 17:34:30.552375 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2019-12-18 17:34:30.553778 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2019-12-18 17:34:30.565922 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed\r\n2019-12-18 17:34:30.570625 I | op-k8sutil: updating deployment rook-ceph-mon-a\r\n2019-12-18 17:34:34.590545 I | op-k8sutil: finished waiting for updated deployment rook-ceph-mon-a\r\n2019-12-18 17:34:34.590578 I | op-mon: waiting for mon quorum with [a b c]\r\n2019-12-18 17:34:34.602244 I | op-mon: mon b is not yet running\r\n2019-12-18 17:34:34.607311 I | op-mon: mon c is not yet running\r\n2019-12-18 17:34:34.607400 I | op-mon: mons running: [a]\r\n2019-12-18 17:34:34.609535 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/607216416\r\n2019-12-18 17:34:49.707835 I | exec: timed out\r\n2019-12-18 17:34:54.717291 I | op-mon: mon b is not yet running\r\n2019-12-18 17:34:54.723031 I | op-mon: mon c is not yet running\r\n2019-12-18 17:34:54.723064 I | op-mon: mons running: [a]\r\n2019-12-18 17:34:54.723483 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/689414143\r\n2019-12-18 17:35:09.820749 I | exec: timed out\r\n2019-12-18 17:35:14.829274 I | op-mon: mon b is not yet running\r\n2019-12-18 17:35:14.833172 I | op-mon: mon c is not yet running\r\n2019-12-18 17:35:14.833988 I | op-mon: mons running: [a]\r\n2019-12-18 17:35:14.834333 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/756801362\r\n2019-12-18 17:35:29.928718 I | exec: timed out\r\n2019-12-18 17:35:34.936048 I | op-mon: mon b is not yet running\r\n2019-12-18 17:35:34.942541 I | op-mon: mon c is not yet running\r\n2019-12-18 17:35:34.942632 I | op-mon: mons running: [a]\r\n2019-12-18 17:35:34.943482 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/267104393\r\n2019-12-18 17:35:50.034954 I | exec: timed out\r\n\r\n```",
  "closed_at": "2020-01-16T19:53:02Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "comments": 4,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/4531/comments",
  "created_at": "2019-12-18T17:40:04Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/4531/events",
  "html_url": "https://github.com/rook/rook/issues/4531",
  "id": 539823869,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    },
    {
      "color": "ef5c55",
      "default": false,
      "description": "main ceph tag",
      "id": 479456042,
      "name": "ceph",
      "node_id": "MDU6TGFiZWw0Nzk0NTYwNDI=",
      "url": "https://api.github.com/repos/rook/rook/labels/ceph"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/4531/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1Mzk4MjM4Njk=",
  "number": 4531,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "Operator should start all mons before checking quorum if all down",
  "updated_at": "2020-01-16T19:53:02Z",
  "url": "https://api.github.com/repos/rook/rook/issues/4531",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  }
}