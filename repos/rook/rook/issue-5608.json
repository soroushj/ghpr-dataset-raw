{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\n\r\nI have following setting on `CephCluster` object which enabled PG autoscler:\r\n\r\n```yaml\r\n  mgr:\r\n    modules:\r\n    - name: pg_autoscaler\r\n      enabled: true\r\n```\r\n\r\n(**NOTE**: These commands are run from a toolbox pod)\r\n\r\nAnd I see correct setting in the cluster:\r\n\r\n```console\r\n# ceph config dump\r\nWHO       MASK LEVEL    OPTION                             VALUE RO \r\nglobal         advanced osd_pool_default_pg_autoscale_mode on       \r\n```\r\n\r\nBut when I check it on the pool this is what I see, this pool is in `warn` and not `on`:\r\n\r\n```console\r\n# ceph osd pool autoscale-status\r\nPOOL          SIZE TARGET SIZE RATE RAW CAPACITY  RATIO TARGET RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE \r\nreplicapool 360.5G              3.0        3241G 0.3336               1.0     32        128 warn\r\n```\r\n\r\n**Possible problem**\r\n\r\nI think that this is a timing issue. I create `CephCluster` and `CephBlockPool` consecutively and immediately. And maybe there is a race condition of some sort. But I could be wrong here and the underlying issue could be something else.\r\n\r\nThe workaround right now is to manually change the setting:\r\n\r\n```console\r\n# ceph osd pool set replicapool pg_autoscale_mode on\r\nset pool 1 pg_autoscale_mode to on\r\n```\r\n\r\n**Expected behavior:**\r\n\r\nI expect the pool `replicapool` to be in `on` state and not `warn`.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n\r\nCreate a `CephCluster` with following config, to enable PG autoscler:\r\n\r\n```yaml\r\napiVersion: ceph.rook.io/v1\r\nkind: CephCluster\r\nmetadata:\r\n  name: rook-ceph\r\n  namespace: rook\r\nspec:\r\n  cephVersion:\r\n    image: ceph/ceph:v14.2.8\r\n    allowUnsupported: false\r\n  dataDirHostPath: /var/lib/rook\r\n  skipUpgradeChecks: false\r\n  continueUpgradeAfterChecksEvenIfNotHealthy: false\r\n  mon:\r\n    count: 3\r\n    allowMultiplePerNode: false\r\n  mgr:\r\n    modules:\r\n    - name: pg_autoscaler\r\n      enabled: true\r\n  dashboard:\r\n    enabled: true\r\n    ssl: true\r\n  monitoring:\r\n    enabled: true\r\n    rulesNamespace: rook\r\n  network:\r\n  rbdMirroring:\r\n    workers: 0\r\n  crashCollector:\r\n    disable: false\r\n  cleanupPolicy:\r\n    deleteDataDirOnHosts: \"\"\r\n  removeOSDsIfOutAndSafeToRemove: false\r\n  storage: # cluster level storage configuration and selection\r\n    useAllNodes: true\r\n    useAllDevices: true\r\n    config:\r\n      storeType: bluestore\r\n      osdsPerDevice: \"1\" # this value can be overridden at the node or device level\r\n  disruptionManagement:\r\n    managePodBudgets: false\r\n    osdMaintenanceTimeout: 30\r\n    manageMachineDisruptionBudgets: false\r\n    machineDisruptionBudgetNamespace: openshift-machine-api\r\n```\r\n\r\nAnd immediately create a `CephBlockPool` as simple as this:\r\n\r\n```yaml\r\napiVersion: ceph.rook.io/v1\r\nkind: CephBlockPool\r\nmetadata:\r\n  name: replicapool\r\n  namespace: rook\r\nspec:\r\n  failureDomain: host\r\n  replicated:\r\n    size: 3\r\n```\r\n\r\nNow deploy rook toolbox and run the following commands: \r\n\r\n```bash\r\nceph config dump | grep autoscale\r\nceph osd pool autoscale-status\r\n```\r\n\r\nAnd verify if there is any discrepancy.\r\n\r\n\r\n**Environment**:\r\n* OS:\r\n\r\n```console\r\n$ cat /etc/os-release\r\nNAME=\"Flatcar Container Linux by Kinvolk\"\r\nID=flatcar\r\nID_LIKE=coreos\r\nVERSION=2247.7.0\r\nVERSION_ID=2247.7.0\r\nBUILD_ID=2019-11-20-1554\r\nPRETTY_NAME=\"Flatcar Container Linux by Kinvolk 2247.7.0 (Rhyolite)\"\r\nANSI_COLOR=\"38;5;75\"\r\nHOME_URL=\"https://flatcar-linux.org/\"\r\nBUG_REPORT_URL=\"https://issues.flatcar-linux.org\"\r\nFLATCAR_BOARD=\"amd64-usr\"\r\n```\r\n\r\n* Kernel:\r\n\r\n```console\r\n$ uname -a\r\nLinux suraj-lk-cluster-monitoring-worker-0 4.19.84-flatcar #1 SMP Wed Nov 20 14:40:21 -00 2019 x86_64 Intel(R) Atom(TM) CPU C2550 @ 2.40GHz GenuineIntel GNU/Linux\r\n```\r\n\r\n* Cloud provider or hardware configuration:\r\n\r\nPacket Cloud and machine https://www.packet.com/cloud/servers/c2-medium-epyc/\r\n\r\n* Rook version:\r\n\r\n`v1.3.2`\r\n\r\n* Storage backend version:\r\n\r\n```console\r\n# ceph -v\r\nceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)\r\n```\r\n\r\n* Kubernetes version:\r\n\r\n```json\r\n$ kubectl version \r\nClient Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.2\", GitCommit:\"52c56ce7a8272c798dbc29846288d7cd9fbae032\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:56:40Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.3\", GitCommit:\"2e7996e3e2712684bc73f0dec0200d64eec7fe40\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T12:43:34Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n* Kubernetes cluster type:\r\n\r\nLokomotive https://github.com/kinvolk/lokomotive\r\n\r\n* Storage backend status:\r\n\r\n```console\r\n# ceph status\r\n  cluster:\r\n    id:     2d726a72-8c3c-497f-b42f-cb9f6648dc45\r\n    health: HEALTH_OK\r\n \r\n  services:\r\n    mon: 3 daemons, quorum a,b,c (age 2h)\r\n    mgr: a(active, since 119m)\r\n    osd: 9 osds: 9 up (since 2h), 9 in (since 2h); 12 remapped pgs\r\n \r\n  data:\r\n    pools:   1 pools, 128 pgs\r\n    objects: 92.59k objects, 361 GiB\r\n    usage:   1.1 TiB used, 2.1 TiB / 3.2 TiB avail\r\n    pgs:     15200/277761 objects misplaced (5.472%)\r\n             116 active+clean\r\n             11  active+remapped+backfill_wait\r\n             1   active+remapped+backfilling\r\n \r\n  io:\r\n    client:   125 KiB/s wr, 0 op/s rd, 0 op/s wr\r\n    recovery: 158 MiB/s, 39 objects/s\r\n```\r\n",
  "closed_at": "2020-06-17T16:19:10Z",
  "closed_by": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "comments": 3,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/5608/comments",
  "created_at": "2020-06-05T13:55:53Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/5608/events",
  "html_url": "https://github.com/rook/rook/issues/5608",
  "id": 631613004,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/5608/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU2MzE2MTMwMDQ=",
  "number": 5608,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "Creating a pool CR at the same time as cluster creation results in a pool without PG autoscaling enabled",
  "updated_at": "2020-06-17T16:19:10Z",
  "url": "https://api.github.com/repos/rook/rook/issues/5608",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/5815795?v=4",
    "events_url": "https://api.github.com/users/surajssd/events{/privacy}",
    "followers_url": "https://api.github.com/users/surajssd/followers",
    "following_url": "https://api.github.com/users/surajssd/following{/other_user}",
    "gists_url": "https://api.github.com/users/surajssd/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/surajssd",
    "id": 5815795,
    "login": "surajssd",
    "node_id": "MDQ6VXNlcjU4MTU3OTU=",
    "organizations_url": "https://api.github.com/users/surajssd/orgs",
    "received_events_url": "https://api.github.com/users/surajssd/received_events",
    "repos_url": "https://api.github.com/users/surajssd/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/surajssd/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/surajssd/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/surajssd"
  }
}