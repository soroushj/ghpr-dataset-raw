{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\n\r\nWhen configuring an OSD on PVC with a PV backed by an LVM Logical Volume, the OSD prepare pod fails to initialize the LV device as an OSD.\r\n\r\nIt fails when I use the master of Rook while it succeeds with Rook v1.2.6.  The problem was introduced around the [refactor of OSD store](https://github.com/rook/rook/commit/227d2d527a09d0e2be580e2f03260a323d208b0f).\r\n\r\nThere are two problems.  The more important issue is that the availability check using `ceph-volume inventory` in `inventoryDevice()` does not succeed for LVs.  I tried `/dev/mapper/vg1-lv1`, `/dev/vg1/lv1`, `/dev/dm-7`, and `vg1/lv1` for `ceph-volume inventory`, and received \"Insufficient space (<5GB)\" in all cases.\r\n\r\n(This is a little off the topic, but the error message of ceph-volume seems odd.  I'll send a bug fix PR to the Ceph community.)\r\n\r\nAs a minor issue, `inventoryDevice()` passes \"/dev/<device.RealName>\" to `ceph-volume inventory` if pvcBacked, but this is not good for LVs.  \"/dev/<device.RealName>\" produces a non-existent device path like `/dev/vg1-lv1` instead of `/dev/mapper/vg1-lv1`.\r\n\r\n**Expected behavior:**\r\n\r\nThe LV device is initialized as an OSD.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n\r\n1. create a k8s cluster\r\n2. deploy the Rook operator by applying \"common.yaml\" and \"operator.yaml\" given at cluster/examples/kubernetes/ceph\r\n3. create an LV on a node, e.g. 10 GiB `vg1/lv1` on 10.69.0.4\r\n4. register the LV as a PV by applying the attached \"pv.yaml\"\r\n5. deploy a Ceph cluster by applying the attached \"cluster.yaml\"\r\n6. watch pods; the OSD prepare pod completes with outputting the attached log and no OSD pod starts\r\n\r\n**Files**:\r\n\r\npv.yaml:\r\n```\r\napiVersion: storage.k8s.io/v1\r\nkind: StorageClass\r\nmetadata:\r\n  name: local-storage\r\nprovisioner: kubernetes.io/no-provisioner\r\nreclaimPolicy: Retain\r\nvolumeBindingMode: WaitForFirstConsumer\r\n---\r\napiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: lv1\r\nspec:\r\n  accessModes:\r\n  - ReadWriteOnce\r\n  capacity:\r\n    storage: 10Gi\r\n  local:\r\n    path: /dev/vg1/lv1\r\n  nodeAffinity:\r\n    required:\r\n      nodeSelectorTerms:\r\n      - matchExpressions:\r\n        - key: kubernetes.io/hostname\r\n          operator: In\r\n          values:\r\n          - 10.69.0.4\r\n  persistentVolumeReclaimPolicy: Retain\r\n  storageClassName: local-storage\r\n  volumeMode: Block\r\n```\r\n\r\ncluster.yaml:\r\n```\r\napiVersion: ceph.rook.io/v1\r\nkind: CephCluster\r\nmetadata:\r\n  name: rook-ceph\r\n  namespace: rook-ceph\r\nspec:\r\n  dataDirHostPath: /var/lib/rook\r\n  mon:\r\n    count: 1\r\n    allowMultiplePerNode: false\r\n  cephVersion:\r\n    image: ceph/ceph:v14.2.8\r\n  dashboard:\r\n    enabled: false\r\n    ssl: true\r\n  network:\r\n    hostNetwork: false\r\n  storage:\r\n    storageClassDeviceSets:\r\n    - name: set1\r\n      count: 1\r\n      volumeClaimTemplates:\r\n      - metadata:\r\n          name: data\r\n        spec:\r\n          resources:\r\n            requests:\r\n              storage: 10Gi\r\n          storageClassName: local-storage\r\n          volumeMode: Block\r\n          accessModes:\r\n            - ReadWriteOnce\r\n```\r\n\r\nOSD prepare pod log:\r\n```\r\n2020-03-23 08:54:15.606396 I | cephcmd: desired devices to configure osds: [{Name:/mnt/set1-data-0-98jxb OSDsPerDevice:1 MetadataDevice: DatabaseSizeMB:0 DeviceClass: IsFilter:false IsDevicePathFilter:false}]\r\n2020-03-23 08:54:15.611320 I | rookcmd: starting Rook v1.1.0-beta.0.1102.gda3bf49 with arguments '/rook/rook ceph osd provision'\r\n2020-03-23 08:54:15.611336 I | rookcmd: flag values: --cluster-id=60d3a214-5a06-423e-8814-7a3c2b2eab46, --data-device-filter=, --data-device-path-filter=, --data-devices=/mnt/set1-data-0-98jxb, --encrypted-device=false, --force-format=false, --help=false, --location=, --log-flush-frequency=5s, --log-level=DEBUG, --metadata-device=, --node-name=set1-data-0-98jxb, --operator-image=, --osd-database-size=0, --osd-store=, --osd-wal-size=576, --osds-per-device=1, --pvc-backed-osd=true, --service-account=\r\n2020-03-23 08:54:15.611345 I | op-mon: parsing mon endpoints: a=10.68.91.3:6789\r\n2020-03-23 08:54:15.661619 I | op-osd: CRUSH location=root=default host=10-69-0-4 zone=rack0\r\n2020-03-23 08:54:15.662016 I | cephcmd: crush location of osd: root=default host=10-69-0-4 zone=rack0\r\n2020-03-23 08:54:15.835101 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2020-03-23 08:54:15.835153 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2020-03-23 08:54:15.835327 D | cephosd: config file @ /etc/ceph/ceph.conf: [global]\r\nfsid                = eeeeb2fe-6dd4-4e3c-8d95-f6031d3287ba\r\nmon initial members = a\r\nmon host            = [v2:10.68.91.3:3300,v1:10.68.91.3:6789]\r\npublic addr         = 10.64.0.12\r\ncluster addr        = 10.64.0.12\r\n\r\n[client.admin]\r\nkeyring = /var/lib/rook/rook-ceph/client.admin.keyring\r\n\r\n2020-03-23 08:54:15.835335 I | cephosd: discovering hardware\r\n2020-03-23 08:54:15.835346 D | exec: Running command: lsblk /mnt/set1-data-0-98jxb --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME,NAME\r\n2020-03-23 08:54:15.854616 D | exec: Running command: sgdisk --print /mnt/set1-data-0-98jxb\r\n2020-03-23 08:54:15.875630 I | cephosd: creating and starting the osds\r\n2020-03-23 08:54:15.876106 D | cephosd: desiredDevices are [{Name:/mnt/set1-data-0-98jxb OSDsPerDevice:1 MetadataDevice: DatabaseSizeMB:0 DeviceClass: IsFilter:false IsDevicePathFilter:false}]\r\n2020-03-23 08:54:15.876118 D | cephosd: context.Devices are [0xc000119440]\r\n2020-03-23 08:54:15.876139 D | exec: Running command: ceph-volume inventory --format json /dev/vg1-lv1\r\n2020-03-23 08:54:19.073529 I | cephosd: skipping device \"/mnt/set1-data-0-98jxb\": [\"Insufficient space (<5GB)\"].\r\n2020-03-23 08:54:19.130929 I | cephosd: configuring osd devices: {\"Entries\":{}}\r\n2020-03-23 08:54:19.131114 I | cephosd: no new devices to configure. returning devices already configured with ceph-volume.\r\n2020-03-23 08:54:19.131169 D | exec: Running command: pvdisplay -C -o lvpath --noheadings /mnt/set1-data-0-98jxb\r\n2020-03-23 08:54:19.241503 W | cephosd: failed to retrieve logical volume path for \"/mnt/set1-data-0-98jxb\". exit status 5\r\n2020-03-23 08:54:19.241524 D | exec: Running command: ceph-volume lvm list  --format json\r\n2020-03-23 08:54:19.863556 I | cephosd: 0 ceph-volume lvm osd devices configured on this node\r\n2020-03-23 08:54:19.863693 D | exec: Running command: ceph-volume raw list /mnt/set1-data-0-98jxb --format json\r\n2020-03-23 08:54:20.373591 I | cephosd: 0 ceph-volume raw osd devices configured on this node\r\n2020-03-23 08:54:20.373681 W | cephosd: skipping OSD configuration as no devices matched the storage settings for this node \"set1-data-0-98jxb\"\r\n```\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release): Container Linux by CoreOS 2345.3.0 (Rhyolite)\r\n* Kernel (e.g. `uname -a`): Linux rack0-cs4 4.19.106-coreos #1 SMP Wed Feb 26 21:43:18 -00 2020 x86_64 Intel(R) Xeon(R) CPU GenuineIntel GNU/Linux\r\n* Cloud provider or hardware configuration: QEMU VMs on GCP\r\n* Rook version (use `rook version` inside of a Rook Pod): v1.1.0-beta.0.1102.gda3bf49 (using rook/ceph:master)\r\n* Storage backend version (e.g. for ceph do `ceph -v`): ceph version 14.2.8 (2d095e947a02261ce61424021bb43bd3022d35cb) nautilus (stable)\r\n* Kubernetes version (use `kubectl version`): v1.16.6-beta.0\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift): [CKE](https://github.com/cybozu-go/cke)\r\n* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox](https://rook.io/docs/rook/master/ceph-toolbox.html)): N/A\r\n",
  "closed_at": "2020-03-27T15:43:49Z",
  "closed_by": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/912735?v=4",
    "events_url": "https://api.github.com/users/leseb/events{/privacy}",
    "followers_url": "https://api.github.com/users/leseb/followers",
    "following_url": "https://api.github.com/users/leseb/following{/other_user}",
    "gists_url": "https://api.github.com/users/leseb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/leseb",
    "id": 912735,
    "login": "leseb",
    "node_id": "MDQ6VXNlcjkxMjczNQ==",
    "organizations_url": "https://api.github.com/users/leseb/orgs",
    "received_events_url": "https://api.github.com/users/leseb/received_events",
    "repos_url": "https://api.github.com/users/leseb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/leseb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/leseb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/leseb"
  },
  "comments": 3,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/5075/comments",
  "created_at": "2020-03-24T08:16:29Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/5075/events",
  "html_url": "https://github.com/rook/rook/issues/5075",
  "id": 586775570,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/5075/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1ODY3NzU1NzA=",
  "number": 5075,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "regression: OSD cannot be deployed on PV backed by LVM Logical Volume",
  "updated_at": "2020-03-27T15:43:49Z",
  "url": "https://api.github.com/repos/rook/rook/issues/5075",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/4961073?v=4",
    "events_url": "https://api.github.com/users/morimoto-cybozu/events{/privacy}",
    "followers_url": "https://api.github.com/users/morimoto-cybozu/followers",
    "following_url": "https://api.github.com/users/morimoto-cybozu/following{/other_user}",
    "gists_url": "https://api.github.com/users/morimoto-cybozu/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/morimoto-cybozu",
    "id": 4961073,
    "login": "morimoto-cybozu",
    "node_id": "MDQ6VXNlcjQ5NjEwNzM=",
    "organizations_url": "https://api.github.com/users/morimoto-cybozu/orgs",
    "received_events_url": "https://api.github.com/users/morimoto-cybozu/received_events",
    "repos_url": "https://api.github.com/users/morimoto-cybozu/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/morimoto-cybozu/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/morimoto-cybozu/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/morimoto-cybozu"
  }
}