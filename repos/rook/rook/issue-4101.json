{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "**Is this a bug report or feature request?**\r\n* Bug Report\r\n\r\n**Deviation from expected behavior:**\r\n\r\nWhen using `topologyAware: true`\r\n\r\nzone and region are not included in the ceph osd tree. The tree structure is\r\n```\r\nroot default\r\n   host node05\r\n      osd.3\r\n```\r\n\r\nzone and region are not included in the ceph osd location.\r\n\r\n**Expected behavior:**\r\n\r\nzone and region are included in the ceph osd tree. The tree structure is\r\n```\r\nroot default\r\n   region se\r\n      zone se-zone01\r\n         host node05\r\n            osd.3\r\n```\r\n\r\nzone and region are included in the ceph osd location.\r\n\r\n**How to reproduce it (minimal and precise):**\r\n\r\n- Install helm chart `rook-ceph` version `v1.1.2` from `https://charts.rook.io/release`\r\n- Configure a `CephCluster` as below:\r\n```\r\n---\r\napiVersion: ceph.rook.io/v1\r\nkind: CephCluster\r\nmetadata:\r\n  name: rook-ceph\r\n  namespace: rook-ceph\r\nspec:\r\n  cephVersion:\r\n    # For the latest ceph images, see https://hub.docker.com/r/ceph/ceph/tags\r\n    image: ceph/ceph:v14.2.4-20190917\r\n    allowUnsupported: false\r\n  dataDirHostPath: /var/lib/rook\r\n  mon:\r\n    count: 3\r\n    allowMultiplePerNode: false\r\n  dashboard:\r\n    enabled: true\r\n  storage:\r\n    useAllNodes: true\r\n    useAllDevices: true\r\n    topologyAware: true\r\n    config:\r\n      metadataDevice:\r\n```\r\n\r\nThe zone and region is with us in the rook-ceph-osd-prepare log:\r\n```\r\n$ kubectl -nrook-ceph logs -f rook-ceph-osd-prepare-node05-ssbqn\r\n2019-10-15 07:53:30.173287 I | rookcmd: starting Rook v1.1.2 with arguments '/rook/rook ceph osd provision'\r\n2019-10-15 07:53:30.173463 I | rookcmd: flag values: --cluster-id=d7e9c424-00e4-4802-bd44-23a3bfdce05b, --data-device-filter=all, --data-devices=, --data-directories=, --encrypted-device=false, --force-format=false, --help=false, --location= region=se zone=se-zone01, --log-flush-frequency=5s, --log-level=INFO, --metadata-device=, --node-name=node05, --operator-image=, --osd-database-size=0, --osd-journal-size=5120, --osd-store=, --osd-wal-size=576, --osds-per-device=1, --pvc-backed-osd=false, --service-account=, --topology-aware=true\r\n2019-10-15 07:53:30.173487 I | op-mon: parsing mon endpoints: c=172.30.77.153:6789,a=172.24.150.51:6789,b=172.28.245.70:6789\r\n2019-10-15 07:53:30.194931 I | cephcmd: crush location of osd:  region=se zone=se-zone01 root=default host=node05 zone=se-zone01 region=se\r\n2019-10-15 07:53:30.206453 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config\r\n2019-10-15 07:53:30.206611 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph\r\n2019-10-15 07:53:30.206776 I | cephosd: discovering hardware\r\n2019-10-15 07:53:30.206799 I | exec: Running command: lsblk --all --noheadings --list --output KNAME\r\n2019-10-15 07:53:30.219772 I | exec: Running command: lsblk /dev/loop1 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.224180 W | inventory: skipping device loop1: Failed to complete 'lsblk /dev/loop1': exit status 1. \r\n2019-10-15 07:53:30.224230 I | exec: Running command: lsblk /dev/nbd3 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.228080 W | inventory: skipping device nbd3: Failed to complete 'lsblk /dev/nbd3': exit status 1. \r\n2019-10-15 07:53:30.228117 I | exec: Running command: lsblk /dev/nbd15 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.232646 W | inventory: skipping device nbd15: Failed to complete 'lsblk /dev/nbd15': exit status 1. \r\n2019-10-15 07:53:30.232710 I | exec: Running command: lsblk /dev/nbd1 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.235988 W | inventory: skipping device nbd1: Failed to complete 'lsblk /dev/nbd1': exit status 1. \r\n2019-10-15 07:53:30.236034 I | exec: Running command: lsblk /dev/nbd13 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.238986 W | inventory: skipping device nbd13: Failed to complete 'lsblk /dev/nbd13': exit status 1. \r\n2019-10-15 07:53:30.239023 I | exec: Running command: lsblk /dev/sdb --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.242194 I | exec: Running command: sgdisk --print /dev/sdb\r\n2019-10-15 07:53:30.269764 I | exec: Running command: udevadm info --query=property /dev/sdb\r\n2019-10-15 07:53:30.274011 I | exec: Running command: lsblk /dev/loop6 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.277867 W | inventory: skipping device loop6: Failed to complete 'lsblk /dev/loop6': exit status 1. \r\n2019-10-15 07:53:30.277902 I | exec: Running command: lsblk /dev/nbd11 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.281641 W | inventory: skipping device nbd11: Failed to complete 'lsblk /dev/nbd11': exit status 1. \r\n2019-10-15 07:53:30.281705 I | exec: Running command: lsblk /dev/nbd8 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.285791 W | inventory: skipping device nbd8: Failed to complete 'lsblk /dev/nbd8': exit status 1. \r\n2019-10-15 07:53:30.285820 I | exec: Running command: lsblk /dev/loop4 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.289428 W | inventory: skipping device loop4: Failed to complete 'lsblk /dev/loop4': exit status 1. \r\n2019-10-15 07:53:30.289487 I | exec: Running command: lsblk /dev/nbd6 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.292748 W | inventory: skipping device nbd6: Failed to complete 'lsblk /dev/nbd6': exit status 1. \r\n2019-10-15 07:53:30.292797 I | exec: Running command: lsblk /dev/sr0 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.297142 W | inventory: skipping device sr0: unsupported diskType rom\r\n2019-10-15 07:53:30.297193 I | exec: Running command: lsblk /dev/loop2 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.300464 W | inventory: skipping device loop2: Failed to complete 'lsblk /dev/loop2': exit status 1. \r\n2019-10-15 07:53:30.300511 I | exec: Running command: lsblk /dev/nbd4 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.304025 W | inventory: skipping device nbd4: Failed to complete 'lsblk /dev/nbd4': exit status 1. \r\n2019-10-15 07:53:30.304074 I | exec: Running command: lsblk /dev/loop0 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.308159 W | inventory: skipping device loop0: Failed to complete 'lsblk /dev/loop0': exit status 1. \r\n2019-10-15 07:53:30.308209 I | exec: Running command: lsblk /dev/nbd2 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.312780 W | inventory: skipping device nbd2: Failed to complete 'lsblk /dev/nbd2': exit status 1. \r\n2019-10-15 07:53:30.312825 I | exec: Running command: lsblk /dev/nbd14 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.317592 W | inventory: skipping device nbd14: Failed to complete 'lsblk /dev/nbd14': exit status 1. \r\n2019-10-15 07:53:30.317639 I | exec: Running command: lsblk /dev/loop7 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.322299 W | inventory: skipping device loop7: Failed to complete 'lsblk /dev/loop7': exit status 1. \r\n2019-10-15 07:53:30.322341 I | exec: Running command: lsblk /dev/nbd0 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.326817 W | inventory: skipping device nbd0: Failed to complete 'lsblk /dev/nbd0': exit status 1. \r\n2019-10-15 07:53:30.326904 I | exec: Running command: lsblk /dev/nbd12 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.331261 W | inventory: skipping device nbd12: Failed to complete 'lsblk /dev/nbd12': exit status 1. \r\n2019-10-15 07:53:30.331306 I | exec: Running command: lsblk /dev/sda --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.336867 I | exec: Running command: sgdisk --print /dev/sda\r\n2019-10-15 07:53:30.342093 I | exec: Running command: udevadm info --query=property /dev/sda\r\n2019-10-15 07:53:30.346541 I | exec: Running command: lsblk /dev/sda2 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.351698 I | exec: Running command: udevadm info --query=property /dev/sda2\r\n2019-10-15 07:53:30.355959 I | exec: Running command: lsblk /dev/dm-0 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.362069 W | inventory: skipping device dm-0: unsupported diskType lvm\r\n2019-10-15 07:53:30.362115 I | exec: Running command: lsblk /dev/sda1 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.367569 I | exec: Running command: udevadm info --query=property /dev/sda1\r\n2019-10-15 07:53:30.371584 I | exec: Running command: lsblk /dev/nbd9 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.376056 W | inventory: skipping device nbd9: Failed to complete 'lsblk /dev/nbd9': exit status 1. \r\n2019-10-15 07:53:30.376109 I | exec: Running command: lsblk /dev/loop5 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.380650 W | inventory: skipping device loop5: Failed to complete 'lsblk /dev/loop5': exit status 1. \r\n2019-10-15 07:53:30.380689 I | exec: Running command: lsblk /dev/nbd10 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.385046 W | inventory: skipping device nbd10: Failed to complete 'lsblk /dev/nbd10': exit status 1. \r\n2019-10-15 07:53:30.385087 I | exec: Running command: lsblk /dev/nbd7 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.388732 W | inventory: skipping device nbd7: Failed to complete 'lsblk /dev/nbd7': exit status 1. \r\n2019-10-15 07:53:30.388769 I | exec: Running command: lsblk /dev/loop3 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.392689 W | inventory: skipping device loop3: Failed to complete 'lsblk /dev/loop3': exit status 1. \r\n2019-10-15 07:53:30.392731 I | exec: Running command: lsblk /dev/nbd5 --bytes --nodeps --pairs --output SIZE,ROTA,RO,TYPE,PKNAME\r\n2019-10-15 07:53:30.396367 W | inventory: skipping device nbd5: Failed to complete 'lsblk /dev/nbd5': exit status 1. \r\n2019-10-15 07:53:30.396404 I | cephosd: creating and starting the osds\r\n2019-10-15 07:53:30.396428 I | exec: Running command: lsblk /dev/sdb --bytes --pairs --output NAME,SIZE,TYPE,PKNAME\r\n2019-10-15 07:53:30.400114 I | sys: Output: NAME=\"sdb\" SIZE=\"498999492608\" TYPE=\"disk\" PKNAME=\"\"\r\n2019-10-15 07:53:30.400164 I | sys: Device found - sdb\r\n2019-10-15 07:53:30.400188 I | exec: Running command: udevadm info --query=property /dev/sdb\r\n2019-10-15 07:53:30.403530 I | exec: Running command: lsblk /dev/sda --bytes --pairs --output NAME,SIZE,TYPE,PKNAME\r\n2019-10-15 07:53:30.407185 I | sys: Output: NAME=\"sda\" SIZE=\"498999492608\" TYPE=\"disk\" PKNAME=\"\"\r\nNAME=\"sda2\" SIZE=\"498455281664\" TYPE=\"part\" PKNAME=\"sda\"\r\nNAME=\"vgroot-lvroot\" SIZE=\"498451087360\" TYPE=\"lvm\" PKNAME=\"sda2\"\r\nNAME=\"sda1\" SIZE=\"536870912\" TYPE=\"part\" PKNAME=\"sda\"\r\n2019-10-15 07:53:30.407236 I | sys: Device found - sda\r\n2019-10-15 07:53:30.407262 I | exec: Running command: udevadm info --query=property /dev/sda2\r\n2019-10-15 07:53:30.411481 I | exec: Running command: udevadm info --query=property /dev/sda1\r\n2019-10-15 07:53:30.415945 I | sys: non-rook partition: \r\n2019-10-15 07:53:30.415980 I | sys: non-rook partition: \r\n2019-10-15 07:53:30.415999 I | exec: Running command: udevadm info --query=property /dev/sda\r\n2019-10-15 07:53:30.419728 I | cephosd: skipping device sda that is in use (not by rook). fs: , ownPartitions: false\r\n2019-10-15 07:53:30.437568 I | cephosd: configuring osd devices: {\"Entries\":{\"sdb\":{\"Data\":-1,\"Metadata\":null,\"Config\":{\"Name\":\"\",\"OSDsPerDevice\":0,\"MetadataDevice\":\"\",\"DatabaseSizeMB\":0,\"DeviceClass\":\"\",\"IsFilter\":false},\"LegacyPartitionsFound\":false}}}\r\n2019-10-15 07:53:30.437606 I | exec: Running command: ceph-volume lvm batch --prepare\r\n2019-10-15 07:53:31.826033 I | cephosd: device sdb to be configured by ceph-volume\r\n2019-10-15 07:53:31.826076 I | cephosd: 0/0 pre-ceph-volume osd devices succeeded on this node\r\n2019-10-15 07:53:31.826444 I | exec: Running command: ceph auth get-or-create-key client.bootstrap-osd mon allow profile bootstrap-osd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/081856435\r\n2019-10-15 07:53:32.737811 I | cephosd: Successfully updated lvm config file\r\n2019-10-15 07:53:32.737864 I | cephosd: configuring new device sdb\r\n2019-10-15 07:53:32.737885 I | cephosd: Base command - stdbuf\r\n2019-10-15 07:53:32.737897 I | cephosd: immediateReportArgs - stdbuf\r\n2019-10-15 07:53:32.737927 I | cephosd: immediateExecuteArgs - [-oL ceph-volume lvm batch --prepare --bluestore --yes --osds-per-device 1 /dev/sdb]\r\n2019-10-15 07:53:32.737945 I | exec: Running command: stdbuf -oL ceph-volume lvm batch --prepare --bluestore --yes --osds-per-device 1 /dev/sdb --report\r\n2019-10-15 07:53:37.648929 I | \r\n2019-10-15 07:53:37.648989 I | Total OSDs: 1\r\n2019-10-15 07:53:37.649005 I | \r\n2019-10-15 07:53:37.649013 I |   Type            Path                                                    LV Size         % of device\r\n2019-10-15 07:53:37.649020 I | ----------------------------------------------------------------------------------------------------\r\n2019-10-15 07:53:37.649037 I |   [data]          /dev/sdb                                                463.00 GB       100%\r\n2019-10-15 07:53:37.662153 I | exec: Running command: stdbuf -oL ceph-volume lvm batch --prepare --bluestore --yes --osds-per-device 1 /dev/sdb\r\n2019-10-15 07:53:42.901619 I | Running command: /usr/sbin/vgcreate -s 1G --force --yes ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933 /dev/sdb\r\n2019-10-15 07:53:43.456013 I |  stdout: Physical volume \"/dev/sdb\" successfully created.\r\n2019-10-15 07:53:43.545890 I |  stdout: Volume group \"ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933\" successfully created\r\n2019-10-15 07:53:44.167541 I | Running command: /usr/sbin/lvcreate --yes -l 464 -n osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933\r\n2019-10-15 07:53:45.607728 I |  stdout: Logical volume \"osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b\" created.\r\n2019-10-15 07:53:50.154926 I | Running command: /bin/ceph-authtool --gen-print-key\r\n2019-10-15 07:53:50.671956 I | Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new e2d290b4-1daa-4775-bc66-c3da5a4909c7\r\n2019-10-15 07:54:03.882752 I | Running command: /bin/ceph-authtool --gen-print-key\r\n2019-10-15 07:54:04.396410 I | Running command: /bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-3\r\n2019-10-15 07:54:04.904624 I | Running command: /usr/sbin/restorecon /var/lib/ceph/osd/ceph-3\r\n2019-10-15 07:54:05.340432 I | Running command: /bin/chown -h ceph:ceph /dev/ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933/osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b\r\n2019-10-15 07:54:05.792656 I | Running command: /bin/chown -R ceph:ceph /dev/mapper/ceph--61fa3f6f--f20f--4e65--a704--28ee170b2933-osd--data--d6e421bb--f257--4c69--887d--88c78ab4f66b\r\n2019-10-15 07:54:06.228906 I | Running command: /bin/ln -s /dev/ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933/osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b /var/lib/ceph/osd/ceph-3/block\r\n2019-10-15 07:54:06.661906 I | Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-3/activate.monmap\r\n2019-10-15 07:54:07.965602 I |  stderr: got monmap epoch 3\r\n2019-10-15 07:54:07.989299 I | Running command: /bin/ceph-authtool /var/lib/ceph/osd/ceph-3/keyring --create-keyring --name osd.3 --add-key AQAOe6VdpfGpJxAADU5Zq6sVgjDpUyjNKe9YvQ==\r\n2019-10-15 07:54:08.500985 I |  stdout: creating /var/lib/ceph/osd/ceph-3/keyring\r\n2019-10-15 07:54:08.501050 I | added entity osd.3 auth(key=AQAOe6VdpfGpJxAADU5Zq6sVgjDpUyjNKe9YvQ==)\r\n2019-10-15 07:54:08.506097 I | Running command: /bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-3/keyring\r\n2019-10-15 07:54:08.940875 I | Running command: /bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-3/\r\n2019-10-15 07:54:09.376845 I | Running command: /bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 3 --monmap /var/lib/ceph/osd/ceph-3/activate.monmap --keyfile - --osd-data /var/lib/ceph/osd/ceph-3/ --osd-uuid e2d290b4-1daa-4775-bc66-c3da5a4909c7 --setuser ceph --setgroup ceph\r\n2019-10-15 07:54:12.712757 I | --> ceph-volume lvm prepare successful for: ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933/osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b\r\n2019-10-15 07:54:12.728290 I | exec: Running command: ceph-volume lvm list  --format json\r\n2019-10-15 07:54:15.895708 I | cephosd: osdInfo has 1 elements. [{Name:osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b Path:/dev/ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933/osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b Tags:{OSDFSID:e2d290b4-1daa-4775-bc66-c3da5a4909c7 Encrypted:0 ClusterFSID:e1506592-7240-409e-9cd1-cb3cb2016770} Type:block}]\r\n2019-10-15 07:54:15.895774 I | cephosd: 1 ceph-volume osd devices configured on this node\r\n2019-10-15 07:54:15.895841 I | cephosd: devices = [{ID:3 DataPath:/var/lib/rook/osd3 Config:/var/lib/rook/osd3/rook-ceph.config Cluster:ceph KeyringPath:/var/lib/rook/osd3/keyring UUID:e2d290b4-1daa-4775-bc66-c3da5a4909c7 Journal: IsFileStore:false IsDirectory:false DevicePartUUID: CephVolumeInitiated:true LVPath:}]\r\n2019-10-15 07:54:15.901575 I | cephosd: configuring osd dirs: map[]\r\n2019-10-15 07:54:15.901885 I | cephosd: removing osd devices: {\"metadata\":null,\"entries\":[]}\r\n2019-10-15 07:54:15.901907 I | cephosd: removing osd dirs: map[]\r\n2019-10-15 07:54:15.901918 I | cephosd: saving osd dir map\r\n2019-10-15 07:54:15.901954 I | cephosd: device osds:[{ID:3 DataPath:/var/lib/rook/osd3 Config:/var/lib/rook/osd3/rook-ceph.config Cluster:ceph KeyringPath:/var/lib/rook/osd3/keyring UUID:e2d290b4-1daa-4775-bc66-c3da5a4909c7 Journal: IsFileStore:false IsDirectory:false DevicePartUUID: CephVolumeInitiated:true LVPath:}]\r\ndir osds: []\r\n```\r\n\r\nBut zone and region is not included in the `--crush-location` when starting the OSD as seen in the OSD log:\r\n```\r\n$ kubectl -nrook-ceph logs -f rook-ceph-osd-3-578bd4986f-th2nx\r\n2019-10-15 07:54:31.604370 I | rookcmd: starting Rook v1.1.2 with arguments '/rook/rook ceph osd start -- --foreground --id 3 --fsid e1506592-7240-409e-9cd1-cb3cb2016770 --cluster ceph --setuser ceph --setgroup ceph --setuser-match-path /var/lib/rook/osd3 --default-log-to-file false --ms-learn-addr-from-peer=false'\r\n2019-10-15 07:54:31.604482 I | rookcmd: flag values: --help=false, --log-flush-frequency=5s, --log-level=INFO, --lv-path=, --operator-image=, --osd-id=3, --osd-store-type=bluestore, --osd-uuid=e2d290b4-1daa-4775-bc66-c3da5a4909c7, --pvc-backed-osd=false, --service-account=\r\n2019-10-15 07:54:31.604499 I | op-mon: parsing mon endpoints: \r\n2019-10-15 07:54:31.604507 W | op-mon: ignoring invalid monitor \r\n2019-10-15 07:54:31.610144 I | cephosd: Successfully updated lvm config file\r\n2019-10-15 07:54:31.610175 I | exec: Running command: stdbuf -oL ceph-volume lvm activate --no-systemd --bluestore 3 e2d290b4-1daa-4775-bc66-c3da5a4909c7\r\n2019-10-15 07:54:32.359224 I | Running command: /bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-3\r\n2019-10-15 07:54:32.866283 I | Running command: /usr/sbin/restorecon /var/lib/ceph/osd/ceph-3\r\n2019-10-15 07:54:33.336763 I | Running command: /bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-3\r\n2019-10-15 07:54:33.829520 I | Running command: /bin/ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933/osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b --path /var/lib/ceph/osd/ceph-3 --no-mon-config\r\n2019-10-15 07:54:34.381942 I | Running command: /bin/ln -snf /dev/ceph-61fa3f6f-f20f-4e65-a704-28ee170b2933/osd-data-d6e421bb-f257-4c69-887d-88c78ab4f66b /var/lib/ceph/osd/ceph-3/block\r\n2019-10-15 07:54:34.859428 I | Running command: /bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-3/block\r\n2019-10-15 07:54:35.331304 I | Running command: /bin/chown -R ceph:ceph /dev/mapper/ceph--61fa3f6f--f20f--4e65--a704--28ee170b2933-osd--data--d6e421bb--f257--4c69--887d--88c78ab4f66b\r\n2019-10-15 07:54:35.829047 I | Running command: /bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-3\r\n2019-10-15 07:54:36.293733 I | --> ceph-volume lvm activate successful for osd ID: 3\r\n2019-10-15 07:54:36.312568 I | exec: Running command: ceph-osd --foreground --id 3 --fsid e1506592-7240-409e-9cd1-cb3cb2016770 --cluster ceph --setuser ceph --setgroup ceph --setuser-match-path /var/lib/rook/osd3 --default-log-to-file false --ms-learn-addr-from-peer=false --crush-location=root=default host=node05\r\n2019-10-15 07:54:37.030085 I | 2019-10-15 07:54:37.024 7fe950623dc0 -1 Falling back to public interface\r\n2019-10-15 07:54:37.794396 I | 2019-10-15 07:54:37.788 7fe950623dc0 -1 osd.3 0 log_to_monitors {default=true}\r\n2019-10-15 07:54:39.700859 I | 2019-10-15 07:54:39.696 7fe9363ff700 -1 osd.3 0 waiting for initial osdmap\r\n2019-10-15 07:54:39.743496 I | 2019-10-15 07:54:39.740 7fe942c18700 -1 osd.3 10 set_numa_affinity unable to identify public interface 'eth0' numa node: (2) No such file or directory\r\n```\r\n\r\nThe zone and region IS in the OSD config on disk, but I don't know if that config is used (or overridden by command line flags):\r\n```\r\n# cat /var/lib/rook/osd3/rook-ceph.config \r\n[global]\r\nfsid                      = e1506592-7240-409e-9cd1-cb3cb2016770\r\nmon initial members       = c a b\r\nmon host                  = v1:172.30.77.153:6789,v1:172.24.150.51:6789,v1:172.28.245.70:6789\r\npublic addr               = 172.16.5.115\r\ncluster addr              = 172.16.5.115\r\nmon keyvaluedb            = rocksdb\r\nmon_allow_pool_delete     = true\r\nmon_max_pg_per_osd        = 1000\r\ndebug default             = 0\r\ndebug rados               = 0\r\ndebug mon                 = 0\r\ndebug osd                 = 0\r\ndebug bluestore           = 0\r\ndebug filestore           = 0\r\ndebug journal             = 0\r\ndebug leveldb             = 0\r\nfilestore_omap_backend    = rocksdb\r\nosd pg bits               = 11\r\nosd pgp bits              = 11\r\nosd pool default size     = 1\r\nosd pool default min size = 1\r\nosd pool default pg num   = 100\r\nosd pool default pgp num  = 100\r\nosd objectstore           = filestore\r\ncrush location            =  region=se zone=se-zone01 root=default host=node05 zone=se-zone01 region=se\r\nrbd_default_features      = 3\r\nfatal signal handlers     = false\r\n\r\n[osd.3]\r\nkeyring              = /var/lib/ceph/osd/ceph-3/keyring\r\nbluestore block path = /var/lib/ceph/osd/ceph-3/block\r\n```\r\n\r\n**Environment**:\r\n* OS (e.g. from /etc/os-release): Ubuntu 18.04.3 LTS\r\n* Kernel (e.g. `uname -a`): 4.15.0-65-generic #74-Ubuntu SMP Tue Sep 17 17:06:04 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n* Cloud provider or hardware configuration: on-prem bare metal\r\n* Rook version (use `rook version` inside of a Rook Pod): v1.1.2\r\n* Storage backend version (e.g. for ceph do `ceph -v`): ceph version 14.2.4 (75f4de193b3ea58512f204623e6c5a16e6c1e1ba) nautilus (stable)\r\n* Kubernetes version (use `kubectl version`): Server Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.1\", GitCommit:\"4485c6f18cee9a5d3c3b4e523bd27972b1b53892\", GitTreeState:\"clean\", BuildDate:\"2019-07-18T09:09:21Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift): kubeadm on-prem\r\n* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox](https://rook.io/docs/rook/master/ceph-toolbox.html)): HEALTH_OK\r\n",
  "closed_at": "2019-10-16T00:40:28Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1048514?v=4",
    "events_url": "https://api.github.com/users/travisn/events{/privacy}",
    "followers_url": "https://api.github.com/users/travisn/followers",
    "following_url": "https://api.github.com/users/travisn/following{/other_user}",
    "gists_url": "https://api.github.com/users/travisn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/travisn",
    "id": 1048514,
    "login": "travisn",
    "node_id": "MDQ6VXNlcjEwNDg1MTQ=",
    "organizations_url": "https://api.github.com/users/travisn/orgs",
    "received_events_url": "https://api.github.com/users/travisn/received_events",
    "repos_url": "https://api.github.com/users/travisn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/travisn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/travisn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/travisn"
  },
  "comments": 0,
  "comments_url": "https://api.github.com/repos/rook/rook/issues/4101/comments",
  "created_at": "2019-10-15T09:10:11Z",
  "events_url": "https://api.github.com/repos/rook/rook/issues/4101/events",
  "html_url": "https://github.com/rook/rook/issues/4101",
  "id": 507110528,
  "labels": [
    {
      "color": "ee0000",
      "default": true,
      "description": "",
      "id": 405241115,
      "name": "bug",
      "node_id": "MDU6TGFiZWw0MDUyNDExMTU=",
      "url": "https://api.github.com/repos/rook/rook/labels/bug"
    }
  ],
  "labels_url": "https://api.github.com/repos/rook/rook/issues/4101/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1MDcxMTA1Mjg=",
  "number": 4101,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/rook/rook",
  "state": "closed",
  "title": "Ceph OSD gets neither zone nor region when using topologyAware",
  "updated_at": "2019-10-16T00:40:28Z",
  "url": "https://api.github.com/repos/rook/rook/issues/4101",
  "user": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/3112766?v=4",
    "events_url": "https://api.github.com/users/splushii/events{/privacy}",
    "followers_url": "https://api.github.com/users/splushii/followers",
    "following_url": "https://api.github.com/users/splushii/following{/other_user}",
    "gists_url": "https://api.github.com/users/splushii/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/splushii",
    "id": 3112766,
    "login": "splushii",
    "node_id": "MDQ6VXNlcjMxMTI3NjY=",
    "organizations_url": "https://api.github.com/users/splushii/orgs",
    "received_events_url": "https://api.github.com/users/splushii/received_events",
    "repos_url": "https://api.github.com/users/splushii/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/splushii/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/splushii/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/splushii"
  }
}