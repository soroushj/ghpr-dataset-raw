{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/29974120?v=4",
    "events_url": "https://api.github.com/users/krzysztof-jastrzebski/events{/privacy}",
    "followers_url": "https://api.github.com/users/krzysztof-jastrzebski/followers",
    "following_url": "https://api.github.com/users/krzysztof-jastrzebski/following{/other_user}",
    "gists_url": "https://api.github.com/users/krzysztof-jastrzebski/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/krzysztof-jastrzebski",
    "id": 29974120,
    "login": "krzysztof-jastrzebski",
    "node_id": "MDQ6VXNlcjI5OTc0MTIw",
    "organizations_url": "https://api.github.com/users/krzysztof-jastrzebski/orgs",
    "received_events_url": "https://api.github.com/users/krzysztof-jastrzebski/received_events",
    "repos_url": "https://api.github.com/users/krzysztof-jastrzebski/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/krzysztof-jastrzebski/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/krzysztof-jastrzebski/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/krzysztof-jastrzebski"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars0.githubusercontent.com/u/29974120?v=4",
      "events_url": "https://api.github.com/users/krzysztof-jastrzebski/events{/privacy}",
      "followers_url": "https://api.github.com/users/krzysztof-jastrzebski/followers",
      "following_url": "https://api.github.com/users/krzysztof-jastrzebski/following{/other_user}",
      "gists_url": "https://api.github.com/users/krzysztof-jastrzebski/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/krzysztof-jastrzebski",
      "id": 29974120,
      "login": "krzysztof-jastrzebski",
      "node_id": "MDQ6VXNlcjI5OTc0MTIw",
      "organizations_url": "https://api.github.com/users/krzysztof-jastrzebski/orgs",
      "received_events_url": "https://api.github.com/users/krzysztof-jastrzebski/received_events",
      "repos_url": "https://api.github.com/users/krzysztof-jastrzebski/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/krzysztof-jastrzebski/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/krzysztof-jastrzebski/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/krzysztof-jastrzebski"
    }
  ],
  "author_association": "MEMBER",
  "body": "**What happened**:\r\nOccasionally we get a Daemonset stuck with less Current replicas than Desired, causing a Daemonset sync tends to fix it, e.g (add a new node). This happens to all our Daemonsets. I'm using fluentd as an example.\r\n```\r\nNAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\r\nfluentd   59        58        58      58           58          <none>          717d\r\n```\r\nThere are no Daemonsets pods in pending nor are there any errors when describing the Daemonset\r\n```\r\nName:           fluentd\r\nSelector:       app=fluentd\r\nNode-Selector:  <none>\r\nLabels:         app=fluentd\r\nAnnotations:    deprecated.daemonset.template.generation: 20\r\n                kubectl.kubernetes.io/last-applied-configuration:\r\n                  {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"DaemonSet\",\"metadata\":{\"annotations\":{},\"name\":\"fluentd\",\"namespace\":\"kube-system\"},\"spec\":{\"te...\r\nDesired Number of Nodes Scheduled: 59\r\nCurrent Number of Nodes Scheduled: 58\r\nNumber of Nodes Scheduled with Up-to-date Pods: 58\r\nNumber of Nodes Scheduled with Available Pods: 58\r\nNumber of Nodes Misscheduled: 0\r\nPods Status:  58 Running / 0 Waiting / 0 Succeeded / 0 Failed\r\nPod Template:\r\n  Labels:       app=fluentd\r\n  Annotations:  iam.amazonaws.com/role: cloud_fluentd_logging\r\n                prometheus.io/port: 24231\r\n                prometheus.io/scrape: true\r\n  Containers:\r\n   fluentd:\r\n    Image:      quay.io/uswitch/fluentd:2.1.0-fluentd-1.2-alpine\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Limits:\r\n      memory:  4Gi\r\n    Requests:\r\n      cpu:      50m\r\n      memory:   512Mi\r\n    Readiness:  http-get http://:24231/metrics delay=0s timeout=1s period=10s #success=1 #failure=3\r\n    Environment:\r\n      FLUENT_UID:   0\r\n      FLUENTD_OPT:  --no-supervisor\r\n    Mounts:\r\n      /fluentd/etc from fluentd-config (ro)\r\n      /var/lib/docker/containers from docker-containers (ro)\r\n      /var/log from logs (rw)\r\n      /var/run/docker.sock from docker (rw)\r\n  Volumes:\r\n   logs:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/log\r\n    HostPathType:\r\n   docker-containers:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/lib/docker/containers\r\n    HostPathType:\r\n   docker:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/run/docker.sock\r\n    HostPathType:\r\n   fluentd-config:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      fluentd-config\r\n    Optional:  false\r\nEvents:\r\n  Type    Reason            Age   From                  Message\r\n  ----    ------            ----  ----                  -------\r\n  Normal  SuccessfulDelete  59m   daemonset-controller  Deleted pod: fluentd-gt9ml\r\n  Normal  SuccessfulCreate  57m   daemonset-controller  Created pod: fluentd-gdr95\r\n  Normal  SuccessfulDelete  54m   daemonset-controller  Deleted pod: fluentd-4xrth\r\n  Normal  SuccessfulCreate  53m   daemonset-controller  Created pod: fluentd-znktv\r\n  Normal  SuccessfulDelete  47m   daemonset-controller  Deleted pod: fluentd-8t6kz\r\n  Normal  SuccessfulCreate  34m   daemonset-controller  Created pod: fluentd-xcvj7\r\n  Normal  SuccessfulDelete  32m   daemonset-controller  Deleted pod: fluentd-gv467\r\n```\r\n**What you expected to happen**:\r\nThe Daemonset Pod to be scheduled\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nReproducing this state is very tricky, I've had the most success by terminating nodes in our ASG causing a new one to come up.\r\n\r\nHowever I think I've identified why this is happening.\r\nThe issue appears to be related to daemonset Pod deletion, a Pod gets deleted causing `deletePod` to trigger, this lowers the expectations by `del: -1` as expected. \r\n```\r\nI0328 15:44:49.470363       1 daemon_controller.go:686] Pod fluentd-gv467 deleted.\r\nI0328 15:44:49.470387       1 controller_utils.go:235] Lowered expectations &controller.ControlleeExpectations{add:0, del:-1, key:\"kube-system/fluentd\", timestamp:time.Time{wall:0xbf1f58d9e8ff2cb3, ext:238457973525, loc:(*time.Location)(0x9578760)}}\r\n```\r\nThe delete pod event then triggers a sync of the daemonset, in which it finds `podsToDelete` which involves fetching pods from the informer cache\r\n```go\r\npods, err := dsc.podLister.Pods(ds.Namespace).List(labels.Everything())\r\n```\r\nThis is somehow returning the deleted pod. You can see in the logs here `syncNodes` thinks that it needs to delete the pod that was deleted. It is also setting the expectation back to `del: 1` after the delete pod event previously lowered it by 1.\r\n```\r\nI0328 15:44:49.473164       1 controller_utils.go:218] Setting expectations &controller.ControlleeExpectations{add:0, del:1, key:\"kube-system/fluentd\", timestamp:time.Time{wall:0xbf1f58dc5c33c66b, ext:248243317929, loc:(*time.Location)(0x9578760)}}\r\nI0328 15:44:49.473197       1 daemon_controller.go:1016] Nodes needing daemon pods for daemon set fluentd: [], creating 0\r\nI0328 15:44:49.473259       1 daemon_controller.go:1102] Pods to delete for daemon set fluentd: [fluentd-gv467], deleting 1\r\n```\r\nAt this point you're stuck as the `syncDaemonSet` function can't get past this line\r\n```go\r\nif !dsc.expectations.SatisfiedExpectations(dsKey) {\r\n\t\t// Only update status.\r\n\t\treturn dsc.updateDaemonSetStatus(ds, hash)\r\n\t}\r\n```\r\nAs the expectation can not be satisfied as syncNodes set it back to `del: 1` and there's nothing it to get it back to `del: 0`. \r\n\r\nSo I believe this is what's causing us to get less replicas than expected, the `syncDaemonSet` function is stuck due to the expectation so it can't perform a full sync to schedule the new pods. There's also not much to trigger a new sync once the expectation has expired until a new node comes up or one goes down. \r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.1\", GitCommit:\"eec55b9ba98609a46fee712359c7b5b365bdd920\", GitTreeState:\"clean\", BuildDate:\"2018-12-13T10:39:04Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.7\", GitCommit:\"6f482974b76db3f1e0f5d24605a9d1d38fad9a2b\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T02:41:57Z\", GoVersion:\"go1.10.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n- Cloud provider or hardware configuration:\r\nAWS\r\n- OS (e.g: `cat /etc/os-release`): \r\n```\r\nNAME=\"Container Linux by CoreOS\"\r\nID=coreos\r\nVERSION=2023.5.0\r\nVERSION_ID=2023.5.0\r\nBUILD_ID=2019-03-09-0138\r\nPRETTY_NAME=\"Container Linux by CoreOS 2023.5.0 (Rhyolite)\"\r\nANSI_COLOR=\"38;5;75\"\r\nHOME_URL=\"https://coreos.com/\"\r\nBUG_REPORT_URL=\"https://issues.coreos.com\"\r\nCOREOS_BOARD=\"amd64-usr\"\r\n```\r\n- Kernel (e.g. `uname -a`):\r\n`4.19.25-coreos #1 SMP Sat Mar 9 01:05:06 -00 2019 x86_64 Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz GenuineIntel GNU/Linux`",
  "closed_at": "2019-04-08T15:58:28Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 7,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/75836/comments",
  "created_at": "2019-03-28T16:38:54Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/75836/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/75836",
  "id": 426595845,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Apps.",
      "id": 404091735,
      "name": "sig/apps",
      "node_id": "MDU6TGFiZWw0MDQwOTE3MzU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/apps"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Scheduling.",
      "id": 125550211,
      "name": "sig/scheduling",
      "node_id": "MDU6TGFiZWwxMjU1NTAyMTE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/scheduling"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/75836/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU0MjY1OTU4NDU=",
  "number": 75836,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Daemonset controller occasionally fails to schedule desired number of pods ",
  "updated_at": "2019-04-08T15:58:28Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/75836",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/15195889?v=4",
    "events_url": "https://api.github.com/users/Joseph-Irving/events{/privacy}",
    "followers_url": "https://api.github.com/users/Joseph-Irving/followers",
    "following_url": "https://api.github.com/users/Joseph-Irving/following{/other_user}",
    "gists_url": "https://api.github.com/users/Joseph-Irving/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/Joseph-Irving",
    "id": 15195889,
    "login": "Joseph-Irving",
    "node_id": "MDQ6VXNlcjE1MTk1ODg5",
    "organizations_url": "https://api.github.com/users/Joseph-Irving/orgs",
    "received_events_url": "https://api.github.com/users/Joseph-Irving/received_events",
    "repos_url": "https://api.github.com/users/Joseph-Irving/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/Joseph-Irving/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/Joseph-Irving/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/Joseph-Irving"
  }
}