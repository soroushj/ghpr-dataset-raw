{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/647318?v=4",
    "events_url": "https://api.github.com/users/lavalamp/events{/privacy}",
    "followers_url": "https://api.github.com/users/lavalamp/followers",
    "following_url": "https://api.github.com/users/lavalamp/following{/other_user}",
    "gists_url": "https://api.github.com/users/lavalamp/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/lavalamp",
    "id": 647318,
    "login": "lavalamp",
    "node_id": "MDQ6VXNlcjY0NzMxOA==",
    "organizations_url": "https://api.github.com/users/lavalamp/orgs",
    "received_events_url": "https://api.github.com/users/lavalamp/received_events",
    "repos_url": "https://api.github.com/users/lavalamp/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/lavalamp/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/lavalamp/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/lavalamp"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars0.githubusercontent.com/u/647318?v=4",
      "events_url": "https://api.github.com/users/lavalamp/events{/privacy}",
      "followers_url": "https://api.github.com/users/lavalamp/followers",
      "following_url": "https://api.github.com/users/lavalamp/following{/other_user}",
      "gists_url": "https://api.github.com/users/lavalamp/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/lavalamp",
      "id": 647318,
      "login": "lavalamp",
      "node_id": "MDQ6VXNlcjY0NzMxOA==",
      "organizations_url": "https://api.github.com/users/lavalamp/orgs",
      "received_events_url": "https://api.github.com/users/lavalamp/received_events",
      "repos_url": "https://api.github.com/users/lavalamp/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/lavalamp/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/lavalamp/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/lavalamp"
    }
  ],
  "author_association": "MEMBER",
  "body": "Forked off from https://github.com/kubernetes/kubernetes/issues/17523.\n\nFailed test: http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-e2e-gce/10260/consoleFull\n\n```\n06:56:32   should support exec through an HTTP proxy\n06:56:32   /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:402\n06:56:32 [BeforeEach] Kubectl client\n06:56:32   /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:68\n06:56:32 STEP: Creating a kubernetes client\n06:56:32 Jan  7 06:56:32.941: INFO: >>> testContext.KubeConfig: /var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config\n06:56:32 \n06:56:32 STEP: Building a namespace api object\n06:56:32 Jan  7 06:56:32.947: INFO: Waiting up to 2m0s for service account default to be provisioned in ns e2e-tests-kubectl-qfhbh\n06:56:32 Jan  7 06:56:32.949: INFO: Service account default in ns e2e-tests-kubectl-qfhbh had 0 secrets, ignoring for 2s: <nil>\n06:56:34 Jan  7 06:56:34.953: INFO: Service account default in ns e2e-tests-kubectl-qfhbh with secrets found. (2.005869969s)\n06:56:34 STEP: Waiting for a default service account to be provisioned in namespace\n06:56:34 Jan  7 06:56:34.953: INFO: Waiting up to 2m0s for service account default to be provisioned in ns e2e-tests-kubectl-qfhbh\n06:56:34 Jan  7 06:56:34.955: INFO: Service account default in ns e2e-tests-kubectl-qfhbh with secrets found. (1.977688ms)\n06:56:34 [BeforeEach] Kubectl client\n06:56:34   /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:85\n06:56:34 [BeforeEach] Simple pod\n06:56:34   /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:153\n06:56:34 STEP: creating the pod\n06:56:34 Jan  7 06:56:34.955: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config create -f /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/docs/user-guide/pod.yaml --namespace=e2e-tests-kubectl-qfhbh'\n06:56:35 Jan  7 06:56:35.384: INFO: stdout: \"pod \\\"nginx\\\" created\\n\"\n06:56:35 Jan  7 06:56:35.384: INFO: stderr: \"\"\n06:56:35 Jan  7 06:56:35.384: INFO: Waiting up to 5m0s for the following 1 pods to be running and ready: [nginx]\n06:56:35 Jan  7 06:56:35.396: INFO: Waiting up to 5m0s for pod nginx status to be running and ready\n06:56:35 Jan  7 06:56:35.399: INFO: Waiting for pod nginx in namespace 'e2e-tests-kubectl-qfhbh' status to be 'running and ready'(found phase: \"Pending\", readiness: false) (2.572412ms elapsed)\n06:56:37 Jan  7 06:56:37.403: INFO: Waiting for pod nginx in namespace 'e2e-tests-kubectl-qfhbh' status to be 'running and ready'(found phase: \"Pending\", readiness: false) (2.006631445s elapsed)\n06:56:39 Jan  7 06:56:39.407: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [nginx]\n06:56:39 [It] should support exec through an HTTP proxy\n06:56:39   /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:402\n06:56:39 STEP: Finding a static kubectl for upload\n06:56:39 STEP: Using the kubectl in /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/386/kubectl\n06:56:39 Jan  7 06:56:39.484: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config create -f /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/test/images/netexec/pod.yaml --namespace=e2e-tests-kubectl-qfhbh'\n06:56:39 Jan  7 06:56:39.929: INFO: stdout: \"pod \\\"netexec\\\" created\\n\"\n06:56:39 Jan  7 06:56:39.929: INFO: stderr: \"\"\n06:56:39 Jan  7 06:56:39.929: INFO: Waiting up to 5m0s for the following 1 pods to be running and ready: [netexec]\n06:56:39 Jan  7 06:56:39.929: INFO: Waiting up to 5m0s for pod netexec status to be running and ready\n06:56:39 Jan  7 06:56:39.934: INFO: Waiting for pod netexec in namespace 'e2e-tests-kubectl-qfhbh' status to be 'running and ready'(found phase: \"Pending\", readiness: false) (5.224876ms elapsed)\n06:56:41 Jan  7 06:56:41.961: INFO: Waiting for pod netexec in namespace 'e2e-tests-kubectl-qfhbh' status to be 'running and ready'(found phase: \"Pending\", readiness: false) (2.032643484s elapsed)\n06:56:43 Jan  7 06:56:43.966: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [netexec]\n06:56:43 STEP: uploading kubeconfig to netexec\n06:56:44 STEP: uploading kubectl to netexec\n06:56:46 STEP: Running kubectl in netexec via an HTTP proxy using https_proxy\n06:56:46 Jan  7 06:56:46.173: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config create -f /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/test/images/goproxy/pod.yaml --namespace=e2e-tests-kubectl-qfhbh'\n06:56:46 Jan  7 06:56:46.515: INFO: stdout: \"pod \\\"goproxy\\\" created\\n\"\n06:56:46 Jan  7 06:56:46.515: INFO: stderr: \"\"\n06:56:46 Jan  7 06:56:46.515: INFO: Waiting up to 5m0s for the following 1 pods to be running and ready: [goproxy]\n06:56:46 Jan  7 06:56:46.515: INFO: Waiting up to 5m0s for pod goproxy status to be running and ready\n06:56:46 Jan  7 06:56:46.520: INFO: Waiting for pod goproxy in namespace 'e2e-tests-kubectl-qfhbh' status to be 'running and ready'(found phase: \"Pending\", readiness: false) (4.760552ms elapsed)\n06:56:48 Jan  7 06:56:48.525: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [goproxy]\n06:56:49 Jan  7 06:56:49.673: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config log goproxy --namespace=e2e-tests-kubectl-qfhbh'\n06:56:49 Jan  7 06:56:49.913: INFO: stdout: \"2016/01/07 14:56:48 [001] INFO: Running 0 CONNECT handlers\\n2016/01/07 14:56:48 [001] INFO: Accepting CONNECT to 104.197.25.58:443\\n2016/01/07 14:56:49 [002] INFO: Running 0 CONNECT handlers\\n2016/01/07 14:56:49 [002] INFO: Accepting CONNECT to 104.197.25.58:443\\n2016/01/07 14:56:49 [002] WARN: Error copying to client: read tcp 10.245.2.70:38724->104.197.25.58:443: read tcp 10.245.2.70:8080->10.245.2.69:51416: read: connection reset by peer\\n\"\n06:56:49 Jan  7 06:56:49.913: INFO: stderr: \"\"\n06:56:49 STEP: using delete to clean up resources\n06:56:49 Jan  7 06:56:49.913: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config delete --grace-period=0 -f /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/test/images/goproxy/pod.yaml --namespace=e2e-tests-kubectl-qfhbh'\n06:56:50 Jan  7 06:56:50.142: INFO: stdout: \"pod \\\"goproxy\\\" deleted\\n\"\n06:56:50 Jan  7 06:56:50.142: INFO: stderr: \"\"\n06:56:50 Jan  7 06:56:50.142: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config get rc,svc -l name=goproxy --no-headers --namespace=e2e-tests-kubectl-qfhbh'\n06:56:50 Jan  7 06:56:50.360: INFO: stdout: \"\"\n06:56:50 Jan  7 06:56:50.360: INFO: stderr: \"\"\n06:56:50 Jan  7 06:56:50.360: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config get pods -l name=goproxy --namespace=e2e-tests-kubectl-qfhbh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ \"\\n\" }}{{ end }}{{ end }}'\n06:56:50 Jan  7 06:56:50.599: INFO: stdout: \"\"\n06:56:50 Jan  7 06:56:50.599: INFO: stderr: \"\"\n06:56:50 STEP: Running kubectl in netexec via an HTTP proxy using HTTPS_PROXY\n06:56:50 Jan  7 06:56:50.599: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config create -f /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/test/images/goproxy/pod.yaml --namespace=e2e-tests-kubectl-qfhbh'\n06:56:50 Jan  7 06:56:50.886: INFO: stdout: \"pod \\\"goproxy\\\" created\\n\"\n06:56:50 Jan  7 06:56:50.886: INFO: stderr: \"\"\n06:56:50 Jan  7 06:56:50.886: INFO: Waiting up to 5m0s for the following 1 pods to be running and ready: [goproxy]\n06:56:50 Jan  7 06:56:50.886: INFO: Waiting up to 5m0s for pod goproxy status to be running and ready\n06:56:50 Jan  7 06:56:50.893: INFO: Waiting for pod goproxy in namespace 'e2e-tests-kubectl-qfhbh' status to be 'running and ready'(found phase: \"Pending\", readiness: false) (6.783458ms elapsed)\n06:56:52 Jan  7 06:56:52.897: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [goproxy]\n06:57:03 STEP: using delete to clean up resources\n06:57:03 Jan  7 06:57:03.379: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config delete --grace-period=0 -f /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/test/images/netexec/pod.yaml --namespace=e2e-tests-kubectl-qfhbh'\n06:57:03 Jan  7 06:57:03.604: INFO: stdout: \"pod \\\"netexec\\\" deleted\\n\"\n06:57:03 Jan  7 06:57:03.604: INFO: stderr: \"\"\n06:57:03 Jan  7 06:57:03.604: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config get rc,svc -l name=netexec --no-headers --namespace=e2e-tests-kubectl-qfhbh'\n06:57:03 Jan  7 06:57:03.815: INFO: stdout: \"\"\n06:57:03 Jan  7 06:57:03.815: INFO: stderr: \"\"\n06:57:03 Jan  7 06:57:03.815: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config get pods -l name=netexec --namespace=e2e-tests-kubectl-qfhbh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ \"\\n\" }}{{ end }}{{ end }}'\n06:57:04 Jan  7 06:57:04.022: INFO: stdout: \"\"\n06:57:04 Jan  7 06:57:04.023: INFO: stderr: \"\"\n06:57:04 [AfterEach] Simple pod\n06:57:04   /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:156\n06:57:04 STEP: using delete to clean up resources\n06:57:04 Jan  7 06:57:04.023: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config delete --grace-period=0 -f /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/docs/user-guide/pod.yaml --namespace=e2e-tests-kubectl-qfhbh'\n06:57:04 Jan  7 06:57:04.255: INFO: stdout: \"pod \\\"nginx\\\" deleted\\n\"\n06:57:04 Jan  7 06:57:04.255: INFO: stderr: \"\"\n06:57:04 Jan  7 06:57:04.255: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-qfhbh'\n06:57:04 Jan  7 06:57:04.490: INFO: stdout: \"\"\n06:57:04 Jan  7 06:57:04.490: INFO: stderr: \"\"\n06:57:04 Jan  7 06:57:04.490: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config get pods -l name=nginx --namespace=e2e-tests-kubectl-qfhbh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ \"\\n\" }}{{ end }}{{ end }}'\n06:57:04 Jan  7 06:57:04.748: INFO: stdout: \"\"\n06:57:04 Jan  7 06:57:04.749: INFO: stderr: \"\"\n06:57:04 [AfterEach] Kubectl client\n06:57:04   /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:69\n06:57:04 STEP: Collecting events from namespace \"e2e-tests-kubectl-qfhbh\".\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {default-scheduler } Scheduled: Successfully assigned goproxy to e2e-gce-minion-tjyd\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {kubelet e2e-gce-minion-tjyd} Pulling: pulling image \"gcr.io/google_containers/goproxy:0.1\"\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {kubelet e2e-gce-minion-tjyd} Pulled: Successfully pulled image \"gcr.io/google_containers/goproxy:0.1\"\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {kubelet e2e-gce-minion-tjyd} Created: Created container with docker id 6fe027a31090\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {kubelet e2e-gce-minion-tjyd} Started: Started container with docker id 6fe027a31090\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {kubelet e2e-gce-minion-tjyd} Killing: Killing container with docker id 6fe027a31090: Need to kill pod.\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {default-scheduler } Scheduled: Successfully assigned goproxy to e2e-gce-minion-tjyd\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {kubelet e2e-gce-minion-tjyd} Pulled: Container image \"gcr.io/google_containers/goproxy:0.1\" already present on machine\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {kubelet e2e-gce-minion-tjyd} Created: Created container with docker id 8b773f9d5e22\n06:57:04 Jan  7 06:57:04.755: INFO: event for goproxy: {kubelet e2e-gce-minion-tjyd} Started: Started container with docker id 8b773f9d5e22\n06:57:04 Jan  7 06:57:04.755: INFO: event for netexec: {default-scheduler } Scheduled: Successfully assigned netexec to e2e-gce-minion-tjyd\n06:57:04 Jan  7 06:57:04.755: INFO: event for netexec: {kubelet e2e-gce-minion-tjyd} Pulling: pulling image \"gcr.io/google_containers/netexec:1.3.1\"\n06:57:04 Jan  7 06:57:04.755: INFO: event for netexec: {kubelet e2e-gce-minion-tjyd} Pulled: Successfully pulled image \"gcr.io/google_containers/netexec:1.3.1\"\n06:57:04 Jan  7 06:57:04.755: INFO: event for netexec: {kubelet e2e-gce-minion-tjyd} Created: Created container with docker id 2feab83424c9\n06:57:04 Jan  7 06:57:04.755: INFO: event for netexec: {kubelet e2e-gce-minion-tjyd} Started: Started container with docker id 2feab83424c9\n06:57:04 Jan  7 06:57:04.755: INFO: event for netexec: {kubelet e2e-gce-minion-tjyd} Killing: Killing container with docker id 2feab83424c9: Need to kill pod.\n06:57:04 Jan  7 06:57:04.755: INFO: event for nginx: {default-scheduler } Scheduled: Successfully assigned nginx to e2e-gce-minion-tjyd\n06:57:04 Jan  7 06:57:04.755: INFO: event for nginx: {kubelet e2e-gce-minion-tjyd} Pulling: pulling image \"nginx\"\n06:57:04 Jan  7 06:57:04.755: INFO: event for nginx: {kubelet e2e-gce-minion-tjyd} Pulled: Successfully pulled image \"nginx\"\n06:57:04 Jan  7 06:57:04.755: INFO: event for nginx: {kubelet e2e-gce-minion-tjyd} Created: Created container with docker id bb794dc54772\n06:57:04 Jan  7 06:57:04.755: INFO: event for nginx: {kubelet e2e-gce-minion-tjyd} Started: Started container with docker id bb794dc54772\n06:57:04 Jan  7 06:57:04.755: INFO: event for nginx: {kubelet e2e-gce-minion-tjyd} Killing: Killing container with docker id bb794dc54772: Need to kill pod.\n06:57:04 Jan  7 06:57:04.776: INFO: POD                                        NODE                 PHASE    GRACE  CONDITIONS\n06:57:04 Jan  7 06:57:04.776: INFO: goproxy                                    e2e-gce-minion-tjyd  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:56:51 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: elasticsearch-logging-v1-cvrpi             e2e-gce-minion-1y2u  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 05:40:43 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: elasticsearch-logging-v1-wvnj9             e2e-gce-minion-s2jz  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 05:40:28 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: fluentd-elasticsearch-e2e-gce-minion-1y2u  e2e-gce-minion-1y2u  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:18:01 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: fluentd-elasticsearch-e2e-gce-minion-s2jz  e2e-gce-minion-s2jz  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:18:00 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: fluentd-elasticsearch-e2e-gce-minion-tjyd  e2e-gce-minion-tjyd  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:18:26 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: heapster-v10-a1qhe                         e2e-gce-minion-1y2u  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:30:06 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: kibana-logging-v1-9c3b9                    e2e-gce-minion-s2jz  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:30:28 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: kube-dns-v10-5odlk                         e2e-gce-minion-s2jz  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:23:14 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: kube-proxy-e2e-gce-minion-1y2u             e2e-gce-minion-1y2u  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:17:57 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: kube-proxy-e2e-gce-minion-s2jz             e2e-gce-minion-s2jz  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:17:57 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: kube-proxy-e2e-gce-minion-tjyd             e2e-gce-minion-tjyd  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:18:25 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.776: INFO: kube-ui-v4-uzhvz                           e2e-gce-minion-s2jz  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 06:30:03 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.777: INFO: l7-lb-controller-lve54                     e2e-gce-minion-1y2u  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 05:40:41 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.777: INFO: monitoring-influxdb-grafana-v2-y1ccn       e2e-gce-minion-1y2u  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-01-07 05:41:03 -0800 PST  }]\n06:57:04 Jan  7 06:57:04.777: INFO: \n06:57:04 Jan  7 06:57:04.780: INFO: \n06:57:04 Logging node info for node e2e-gce-minion-1y2u\n06:57:04 Jan  7 06:57:04.787: INFO: Node Info: &{{ } {e2e-gce-minion-1y2u   /api/v1/nodes/e2e-gce-minion-1y2u fd18ca6f-b543-11e5-8316-42010af00002 10910 0 2016-01-07 05:38:51 -0800 PST <nil> <nil> map[kubernetes.io/hostname:e2e-gce-minion-1y2u failure-domain.alpha.kubernetes.io/region:us-central1 failure-domain.alpha.kubernetes.io/zone:us-central1-f] map[]} {10.245.0.0/24 16796349005645129301 gce://k8s-jkns-e2e-gce/us-central1-f/e2e-gce-minion-1y2u false} {map[pods:{110.000 DecimalSI} cpu:{2.000 DecimalSI} memory:{7863918592.000 BinarySI}] map[cpu:{2.000 DecimalSI} memory:{7863918592.000 BinarySI} pods:{110.000 DecimalSI}]  [{OutOfDisk False 2016-01-07 06:57:01 -0800 PST 2016-01-07 06:24:39 -0800 PST KubeletHasSufficientDisk kubelet has sufficient disk space available} {Ready True 2016-01-07 06:57:01 -0800 PST 2016-01-07 06:24:39 -0800 PST KubeletReady kubelet is posting ready status}] [{InternalIP 10.240.0.4} {ExternalIP 104.197.17.178}] {{10250}} { 66110D32-14F0-98DF-A305-F900E4AA0792 5316447d-bedc-474f-acb3-d2affe1bd86d 3.16.0-0.bpo.4-amd64 Debian GNU/Linux 7 (wheezy) docker://1.8.3 v1.2.0-alpha.5.690+ab6edd8170522f v1.2.0-alpha.5.690+ab6edd8170522f}}}\n06:57:04 Jan  7 06:57:04.838: INFO: \n06:57:04 Logging kubelet events for node e2e-gce-minion-1y2u\n06:57:04 Jan  7 06:57:04.845: INFO: \n06:57:04 Logging pods the kubelet thinks is on node e2e-gce-minion-1y2u\n06:57:04 Jan  7 06:57:04.903: INFO: monitoring-influxdb-grafana-v2-y1ccn started at <nil> (0 container statuses recorded)\n06:57:04 Jan  7 06:57:04.903: INFO: fluentd-elasticsearch-e2e-gce-minion-1y2u started at <nil> (0 container statuses recorded)\n06:57:04 Jan  7 06:57:04.904: INFO: elasticsearch-logging-v1-cvrpi started at <nil> (0 container statuses recorded)\n06:57:04 Jan  7 06:57:04.904: INFO: heapster-v10-a1qhe started at <nil> (0 container statuses recorded)\n06:57:04 Jan  7 06:57:04.904: INFO: l7-lb-controller-lve54 started at <nil> (0 container statuses recorded)\n06:57:04 Jan  7 06:57:04.904: INFO: kube-proxy-e2e-gce-minion-1y2u started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.075: INFO: ERROR kubelet_docker_errors{operation_type=\"info\"} => 1 @[0]\n06:57:05 Jan  7 06:57:05.077: INFO: ERROR kubelet_docker_errors{operation_type=\"inspect_image\"} => 3 @[0]\n06:57:05 Jan  7 06:57:05.077: INFO: ERROR kubelet_docker_errors{operation_type=\"list_containers\"} => 12 @[0]\n06:57:05 Jan  7 06:57:05.077: INFO: ERROR kubelet_docker_errors{operation_type=\"list_images\"} => 1 @[0]\n06:57:05 Jan  7 06:57:05.077: INFO: ERROR kubelet_docker_errors{operation_type=\"stop_container\"} => 12 @[0]\n06:57:05 Jan  7 06:57:05.077: INFO: ERROR kubelet_docker_errors{operation_type=\"version\"} => 18 @[0]\n06:57:05 Jan  7 06:57:05.077: INFO: \n06:57:05 Latency metrics for node e2e-gce-minion-1y2u\n06:57:05 Jan  7 06:57:05.077: INFO: \n06:57:05 Logging node info for node e2e-gce-minion-s2jz\n06:57:05 Jan  7 06:57:05.082: INFO: Node Info: &{{ } {e2e-gce-minion-s2jz   /api/v1/nodes/e2e-gce-minion-s2jz fbc7b94f-b543-11e5-8316-42010af00002 10911 0 2016-01-07 05:38:49 -0800 PST <nil> <nil> map[failure-domain.alpha.kubernetes.io/zone:us-central1-f kubernetes.io/hostname:e2e-gce-minion-s2jz failure-domain.alpha.kubernetes.io/region:us-central1] map[]} {10.245.1.0/24 1883977523354088664 gce://k8s-jkns-e2e-gce/us-central1-f/e2e-gce-minion-s2jz false} {map[cpu:{2.000 DecimalSI} memory:{7863918592.000 BinarySI} pods:{110.000 DecimalSI}] map[memory:{7863918592.000 BinarySI} pods:{110.000 DecimalSI} cpu:{2.000 DecimalSI}]  [{OutOfDisk False 2016-01-07 06:57:02 -0800 PST 2016-01-07 06:24:39 -0800 PST KubeletHasSufficientDisk kubelet has sufficient disk space available} {Ready True 2016-01-07 06:57:02 -0800 PST 2016-01-07 06:24:39 -0800 PST KubeletReady kubelet is posting ready status}] [{InternalIP 10.240.0.5} {ExternalIP 104.197.212.161}] {{10250}} { 310C3B7F-C042-1BE5-B085-90DBF7AFA777 d8770d6c-e799-4bec-a6f6-2dba02fbe0da 3.16.0-0.bpo.4-amd64 Debian GNU/Linux 7 (wheezy) docker://1.8.3 v1.2.0-alpha.5.690+ab6edd8170522f v1.2.0-alpha.5.690+ab6edd8170522f}}}\n06:57:05 Jan  7 06:57:05.082: INFO: \n06:57:05 Logging kubelet events for node e2e-gce-minion-s2jz\n06:57:05 Jan  7 06:57:05.088: INFO: \n06:57:05 Logging pods the kubelet thinks is on node e2e-gce-minion-s2jz\n06:57:05 Jan  7 06:57:05.112: INFO: kibana-logging-v1-9c3b9 started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.113: INFO: kube-ui-v4-uzhvz started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.113: INFO: kube-dns-v10-5odlk started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.113: INFO: elasticsearch-logging-v1-wvnj9 started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.113: INFO: fluentd-elasticsearch-e2e-gce-minion-s2jz started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.113: INFO: kube-proxy-e2e-gce-minion-s2jz started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.178: INFO: ERROR kubelet_docker_errors{operation_type=\"info\"} => 1 @[0]\n06:57:05 Jan  7 06:57:05.178: INFO: ERROR kubelet_docker_errors{operation_type=\"inspect_image\"} => 2 @[0]\n06:57:05 Jan  7 06:57:05.178: INFO: ERROR kubelet_docker_errors{operation_type=\"list_containers\"} => 12 @[0]\n06:57:05 Jan  7 06:57:05.178: INFO: ERROR kubelet_docker_errors{operation_type=\"list_images\"} => 1 @[0]\n06:57:05 Jan  7 06:57:05.178: INFO: ERROR kubelet_docker_errors{operation_type=\"stop_container\"} => 4 @[0]\n06:57:05 Jan  7 06:57:05.178: INFO: ERROR kubelet_docker_errors{operation_type=\"version\"} => 18 @[0]\n06:57:05 Jan  7 06:57:05.179: INFO: \n06:57:05 Latency metrics for node e2e-gce-minion-s2jz\n06:57:05 Jan  7 06:57:05.179: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:16.018836s}\n06:57:05 Jan  7 06:57:05.179: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:16.018836s}\n06:57:05 Jan  7 06:57:05.179: INFO: \n06:57:05 Logging node info for node e2e-gce-minion-tjyd\n06:57:05 Jan  7 06:57:05.182: INFO: Node Info: &{{ } {e2e-gce-minion-tjyd   /api/v1/nodes/e2e-gce-minion-tjyd 0848b3de-b544-11e5-8316-42010af00002 10909 0 2016-01-07 05:39:10 -0800 PST <nil> <nil> map[failure-domain.alpha.kubernetes.io/zone:us-central1-f kubernetes.io/e2e-296d7dba-b54e-11e5-a891-42010af01555:42 kubernetes.io/hostname:e2e-gce-minion-tjyd failure-domain.alpha.kubernetes.io/region:us-central1] map[]} {10.245.2.0/24 17555172111140660062 gce://k8s-jkns-e2e-gce/us-central1-f/e2e-gce-minion-tjyd false} {map[cpu:{2.000 DecimalSI} memory:{7863918592.000 BinarySI} pods:{110.000 DecimalSI}] map[pods:{110.000 DecimalSI} cpu:{2.000 DecimalSI} memory:{7863918592.000 BinarySI}]  [{OutOfDisk False 2016-01-07 06:56:58 -0800 PST 2016-01-07 06:30:36 -0800 PST KubeletHasSufficientDisk kubelet has sufficient disk space available} {Ready True 2016-01-07 06:56:58 -0800 PST 2016-01-07 06:30:36 -0800 PST KubeletReady kubelet is posting ready status}] [{InternalIP 10.240.0.3} {ExternalIP 104.197.246.2}] {{10250}} { 18C6261C-F851-231F-6171-1CE800DC5C72 d9ec9cd7-2da1-4c3b-9843-f2f77b503331 3.16.0-0.bpo.4-amd64 Debian GNU/Linux 7 (wheezy) docker://1.8.3 v1.2.0-alpha.5.690+ab6edd8170522f v1.2.0-alpha.5.690+ab6edd8170522f}}}\n06:57:05 Jan  7 06:57:05.182: INFO: \n06:57:05 Logging kubelet events for node e2e-gce-minion-tjyd\n06:57:05 Jan  7 06:57:05.351: INFO: \n06:57:05 Logging pods the kubelet thinks is on node e2e-gce-minion-tjyd\n06:57:05 Jan  7 06:57:05.575: INFO: kube-proxy-e2e-gce-minion-tjyd started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.575: INFO: goproxy started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.575: INFO: fluentd-elasticsearch-e2e-gce-minion-tjyd started at <nil> (0 container statuses recorded)\n06:57:05 Jan  7 06:57:05.785: INFO: ERROR kubelet_docker_errors{operation_type=\"create_container\"} => 1 @[0]\n06:57:05 Jan  7 06:57:05.785: INFO: ERROR kubelet_docker_errors{operation_type=\"info\"} => 1 @[0]\n06:57:05 Jan  7 06:57:05.785: INFO: ERROR kubelet_docker_errors{operation_type=\"inspect_image\"} => 11 @[0]\n06:57:05 Jan  7 06:57:05.785: INFO: ERROR kubelet_docker_errors{operation_type=\"list_containers\"} => 12 @[0]\n06:57:05 Jan  7 06:57:05.785: INFO: ERROR kubelet_docker_errors{operation_type=\"list_images\"} => 1 @[0]\n06:57:05 Jan  7 06:57:05.785: INFO: ERROR kubelet_docker_errors{operation_type=\"stop_container\"} => 61 @[0]\n06:57:05 Jan  7 06:57:05.785: INFO: ERROR kubelet_docker_errors{operation_type=\"version\"} => 19 @[0]\n06:57:05 Jan  7 06:57:05.785: INFO: \n06:57:05 Latency metrics for node e2e-gce-minion-tjyd\n06:57:05 Jan  7 06:57:05.785: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:25.030417s}\n06:57:05 Jan  7 06:57:05.785: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:19.608699s}\n06:57:05 Jan  7 06:57:05.785: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:11.800639s}\n06:57:05 Jan  7 06:57:05.785: INFO: Waiting up to 1m0s for all nodes to be ready\n06:57:05 STEP: Destroying namespace \"e2e-tests-kubectl-qfhbh\" for this suite.\n06:57:06 \n06:57:06 \u2022 Failure [33.605 seconds]\n06:57:06 Kubectl client\n06:57:06 /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:977\n06:57:06   Simple pod\n06:57:06   /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:470\n06:57:06     should support exec through an HTTP proxy [It]\n06:57:06     /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:402\n06:57:06 \n06:57:06     Jan  7 06:57:03.358: Unexpected kubectl exec output. Wanted \"running in container\\n\", got  \"Unable to connect to the server: net/http: TLS handshake timeout\\n\"\n06:57:06 \n06:57:06     /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:389\n```\n\nFrom above output, one can see that the related pods are running and in Ready state:\n\n```\n06:56:50 STEP: Running kubectl in netexec via an HTTP proxy using HTTPS_PROXY\n06:56:50 Jan  7 06:56:50.599: INFO: Running '/jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://104.197.25.58 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gce/workspace/.kube/config create -f /jenkins-master-data/jobs/kubernetes-e2e-gce/workspace/kubernetes/test/images/goproxy/pod.yaml --namespace=e2e-tests-kubectl-qfhbh'\n06:56:50 Jan  7 06:56:50.886: INFO: stdout: \"pod \\\"goproxy\\\" created\\n\"\n...\n06:56:52 Jan  7 06:56:52.897: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [goproxy]\n```\n\nand the test is trying to using HTTPS_PROXY, and failed. APIServer is temporarily unvailable here? \n",
  "closed_at": "2016-03-01T02:15:48Z",
  "closed_by": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/10927820?v=4",
    "events_url": "https://api.github.com/users/bprashanth/events{/privacy}",
    "followers_url": "https://api.github.com/users/bprashanth/followers",
    "following_url": "https://api.github.com/users/bprashanth/following{/other_user}",
    "gists_url": "https://api.github.com/users/bprashanth/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/bprashanth",
    "id": 10927820,
    "login": "bprashanth",
    "node_id": "MDQ6VXNlcjEwOTI3ODIw",
    "organizations_url": "https://api.github.com/users/bprashanth/orgs",
    "received_events_url": "https://api.github.com/users/bprashanth/received_events",
    "repos_url": "https://api.github.com/users/bprashanth/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/bprashanth/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/bprashanth/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/bprashanth"
  },
  "comments": 2,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/19500/comments",
  "created_at": "2016-01-11T21:08:29Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/19500/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/19500",
  "id": 126036938,
  "labels": [
    {
      "color": "f7c6c7",
      "default": false,
      "description": "Categorizes issue or PR as related to a flaky test.",
      "id": 264749912,
      "name": "kind/flake",
      "node_id": "MDU6TGFiZWwyNjQ3NDk5MTI=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/flake"
    },
    {
      "color": "eb6420",
      "default": false,
      "description": "Must be staffed and worked on either currently, or very soon, ideally in time for the next release.",
      "id": 114528223,
      "name": "priority/important-soon",
      "node_id": "MDU6TGFiZWwxMTQ1MjgyMjM=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/priority/important-soon"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG API Machinery.",
      "id": 173493835,
      "name": "sig/api-machinery",
      "node_id": "MDU6TGFiZWwxNzM0OTM4MzU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/api-machinery"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/19500/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUxMjYwMzY5Mzg=",
  "number": 19500,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Flake: Kubectl client Simple pod should support exec through an HTTP proxy",
  "updated_at": "2016-03-01T02:15:48Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/19500",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/7740897?v=4",
    "events_url": "https://api.github.com/users/dchen1107/events{/privacy}",
    "followers_url": "https://api.github.com/users/dchen1107/followers",
    "following_url": "https://api.github.com/users/dchen1107/following{/other_user}",
    "gists_url": "https://api.github.com/users/dchen1107/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/dchen1107",
    "id": 7740897,
    "login": "dchen1107",
    "node_id": "MDQ6VXNlcjc3NDA4OTc=",
    "organizations_url": "https://api.github.com/users/dchen1107/orgs",
    "received_events_url": "https://api.github.com/users/dchen1107/received_events",
    "repos_url": "https://api.github.com/users/dchen1107/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/dchen1107/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/dchen1107/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/dchen1107"
  }
}