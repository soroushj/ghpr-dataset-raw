{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/22770898?v=4",
    "events_url": "https://api.github.com/users/tosi3k/events{/privacy}",
    "followers_url": "https://api.github.com/users/tosi3k/followers",
    "following_url": "https://api.github.com/users/tosi3k/following{/other_user}",
    "gists_url": "https://api.github.com/users/tosi3k/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/tosi3k",
    "id": 22770898,
    "login": "tosi3k",
    "node_id": "MDQ6VXNlcjIyNzcwODk4",
    "organizations_url": "https://api.github.com/users/tosi3k/orgs",
    "received_events_url": "https://api.github.com/users/tosi3k/received_events",
    "repos_url": "https://api.github.com/users/tosi3k/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/tosi3k/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/tosi3k/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/tosi3k"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars0.githubusercontent.com/u/22770898?v=4",
      "events_url": "https://api.github.com/users/tosi3k/events{/privacy}",
      "followers_url": "https://api.github.com/users/tosi3k/followers",
      "following_url": "https://api.github.com/users/tosi3k/following{/other_user}",
      "gists_url": "https://api.github.com/users/tosi3k/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/tosi3k",
      "id": 22770898,
      "login": "tosi3k",
      "node_id": "MDQ6VXNlcjIyNzcwODk4",
      "organizations_url": "https://api.github.com/users/tosi3k/orgs",
      "received_events_url": "https://api.github.com/users/tosi3k/received_events",
      "repos_url": "https://api.github.com/users/tosi3k/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/tosi3k/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/tosi3k/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/tosi3k"
    }
  ],
  "author_association": "CONTRIBUTOR",
  "body": "**Which jobs are flaking**:\r\n`kubemark-5000`\r\n`kubemark-5000-scheduler`\r\n\r\n**Which test(s) are flaking**:\r\nhttps://k8s-testgrid.appspot.com/sig-scalability-kubemark#kubemark-5000\r\nhttps://k8s-testgrid.appspot.com/sig-scalability-kubemark#kubemark-5000-scheduler\r\n\r\n**Testgrid link**:\r\nhttps://k8s-testgrid.appspot.com/sig-scalability-kubemark#kubemark-5000\r\nhttps://k8s-testgrid.appspot.com/sig-scalability-kubemark#kubemark-5000-scheduler\r\n\r\n**Reason for failure**:\r\n```\r\nI1007 16:18:41.982] Created and uploaded the kubemark hollow-node image to docker registry.\r\n...\r\nI1007 16:18:45.193] Created secrets, configMaps, replication-controllers required for hollow-nodes.\r\nI1007 16:48:48.114] Waiting for all hollow-nodes to become Running....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\r\nI1007 16:48:48.116]  Timeout waiting for all hollow-nodes to become Running. \r\nI1007 16:48:50.231] Found only 2155 ready hollow-nodes while waiting for 5000.\r\nI1007 16:48:58.056] 2414 hollow-nodes are reported as 'Running'\r\nI1007 16:48:58.073] 2586 hollow-nodes are reported as NOT 'Running'\r\nI1007 16:48:58.084] NAME                READY   STATUS                  RESTARTS   AGE\r\nI1007 16:48:58.084] hollow-node-227k7   0/3     Init:ImagePullBackOff   0          27m\r\nI1007 16:48:58.085] hollow-node-22jbc   0/3     Init:ImagePullBackOff   0          26m\r\nI1007 16:48:58.085] hollow-node-22jk2   0/3     Init:ImagePullBackOff   0          27m\r\nI1007 16:48:58.085] hollow-node-24b82   0/3     Init:ImagePullBackOff   0          26m\r\nI1007 16:48:58.085] hollow-node-24t27   0/3     Init:ImagePullBackOff   0          27m\r\n...\r\n```\r\n\r\n**Anything else we need to know**:\r\nLogs suggest that pulling the image of first init container (`busybox`) of the hollow node pod fails:\r\n\r\n```\r\n./kubelet.log:2760:E1005 16:27:02.791161    1520 pod_workers.go:191] Error syncing pod 4eede5fe-5e7d-4807-8b86-b51f9cc1f93e (\"hollow-node-2282b_kubemark(4eede5fe-5e7d-4807-8b86-b51f9cc1f93e)\"), skipping: failed to \"StartContainer\" for \"init-inotify-limit\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox\\\"\"\r\n./kubelet.log:2837:E1005 16:27:13.791524    1520 pod_workers.go:191] Error syncing pod 4eede5fe-5e7d-4807-8b86-b51f9cc1f93e (\"hollow-node-2282b_kubemark(4eede5fe-5e7d-4807-8b86-b51f9cc1f93e)\"), skipping: failed to \"StartContainer\" for \"init-inotify-limit\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox\\\"\"\r\n./kubelet.log:2902:E1005 16:27:26.792468    1520 pod_workers.go:191] Error syncing pod 4eede5fe-5e7d-4807-8b86-b51f9cc1f93e (\"hollow-node-2282b_kubemark(4eede5fe-5e7d-4807-8b86-b51f9cc1f93e)\"), skipping: failed to \"StartContainer\" for \"init-inotify-limit\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox\\\"\"\r\n```\r\n\r\nFurther perusal shows it's related to rate limiting by `docker.io` from where the image is pulled - we receive lots of 429 errors when attempting to pull the `busybox` image:\r\n```\r\n./kubelet.log:1447:E1005 16:24:42.005736    1520 kuberuntime_manager.go:815] init container &Container{Name:init-inotify-limit,Image:busybox,Command:[sysctl -w fs.inotify.max_user_instances=1000],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dsplv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod hollow-node-2282b_kubemark(4eede5fe-5e7d-4807-8b86-b51f9cc1f93e): ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/library/busybox:latest\": failed to resolve reference \"docker.io/library/busybox:latest\": unexpected status code [manifests latest]: 429 Too Many Requests\r\n...\r\n./kubelet.log:1676:E1005 16:25:06.179796    1520 kuberuntime_manager.go:815] init container &Container{Name:init-inotify-limit,Image:busybox,Command:[sysctl -w fs.inotify.max_user_instances=1000],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dsplv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod hollow-node-2282b_kubemark(4eede5fe-5e7d-4807-8b86-b51f9cc1f93e): ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/library/busybox:latest\": failed to resolve reference \"docker.io/library/busybox:latest\": unexpected status code [manifests latest]: 429 Too Many Requests\r\n...\r\n./kubelet.log:2030:E1005 16:25:44.155094    1520 kuberuntime_manager.go:815] init container &Container{Name:init-inotify-limit,Image:busybox,Command:[sysctl -w fs.inotify.max_user_instances=1000],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dsplv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod hollow-node-2282b_kubemark(4eede5fe-5e7d-4807-8b86-b51f9cc1f93e): ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/library/busybox:latest\": failed to resolve reference \"docker.io/library/busybox:latest\": unexpected status code [manifests latest]: 429 Too Many Requests\r\n...\r\n```\r\n\r\nHollow node template: https://github.com/kubernetes/kubernetes/blob/master/test/kubemark/resources/hollow-node_template.yaml.\r\n\r\n/assign\r\n/sig scalability",
  "closed_at": "2020-10-12T18:33:00Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 8,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/95402/comments",
  "created_at": "2020-10-08T14:52:09Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/95402/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/95402",
  "id": 717418162,
  "labels": [
    {
      "color": "f7c6c7",
      "default": false,
      "description": "Categorizes issue or PR as related to a flaky test.",
      "id": 264749912,
      "name": "kind/flake",
      "node_id": "MDU6TGFiZWwyNjQ3NDk5MTI=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/flake"
    },
    {
      "color": "ededed",
      "default": false,
      "description": "Indicates an issue or PR lacks a `triage/foo` label and requires one.",
      "id": 2389815605,
      "name": "needs-triage",
      "node_id": "MDU6TGFiZWwyMzg5ODE1NjA1",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/needs-triage"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Scalability.",
      "id": 125010198,
      "name": "sig/scalability",
      "node_id": "MDU6TGFiZWwxMjUwMTAxOTg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/scalability"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/95402/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU3MTc0MTgxNjI=",
  "number": 95402,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Very flaky kubemark-5000 and kubemark-5000-scheduler jobs",
  "updated_at": "2020-10-12T18:33:00Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/95402",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/22770898?v=4",
    "events_url": "https://api.github.com/users/tosi3k/events{/privacy}",
    "followers_url": "https://api.github.com/users/tosi3k/followers",
    "following_url": "https://api.github.com/users/tosi3k/following{/other_user}",
    "gists_url": "https://api.github.com/users/tosi3k/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/tosi3k",
    "id": 22770898,
    "login": "tosi3k",
    "node_id": "MDQ6VXNlcjIyNzcwODk4",
    "organizations_url": "https://api.github.com/users/tosi3k/orgs",
    "received_events_url": "https://api.github.com/users/tosi3k/received_events",
    "repos_url": "https://api.github.com/users/tosi3k/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/tosi3k/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/tosi3k/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/tosi3k"
  }
}