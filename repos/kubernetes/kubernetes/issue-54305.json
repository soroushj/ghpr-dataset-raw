{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a BUG REPORT or FEATURE REQUEST?**:\r\n\r\n/kind bug\r\n\r\n**What happened**:\r\n\r\nRan `kube-proxy --cleanup` and it crashed after cleaning up `iptables`:\r\n\r\n```\r\n# ./kube-proxy --v=10 --cleanup\r\nW1020 16:27:12.840883    2064 server.go:191] WARNING: all flags other than --config, --write-config-to, and --cleanup are deprecated. Please begin using a config file ASAP.\r\nI1020 16:27:12.861382    2064 iptables.go:564] couldn't get iptables-restore version; assuming it doesn't support --wait\r\nI1020 16:27:12.863689    2064 iptables.go:396] running iptables -C [OUTPUT -t nat -m comment --comment handle ClusterIPs; NOTE: this must be before the NodePort rules -j KUBE-PORTALS-HOST]\r\nI1020 16:27:12.864612    2064 iptables.go:396] running iptables -C [PREROUTING -t nat -m comment --comment handle ClusterIPs; NOTE: this must be before the NodePort rules -j KUBE-PORTALS-CONTAINER]\r\nI1020 16:27:12.865435    2064 iptables.go:396] running iptables -C [OUTPUT -t nat -m addrtype --dst-type LOCAL -m comment --comment handle service NodePorts; NOTE: this must be the last rule in the chain -j KUBE-NODEPORT-HOST]\r\nI1020 16:27:12.866281    2064 iptables.go:396] running iptables -C [PREROUTING -t nat -m addrtype --dst-type LOCAL -m comment --comment handle service NodePorts; NOTE: this must be the last rule in the chain -j KUBE-NODEPORT-CONTAINER]\r\nI1020 16:27:12.867102    2064 iptables.go:396] running iptables -C [INPUT -t filter -m comment --comment Ensure that non-local NodePort traffic can flow -j KUBE-NODEPORT-NON-LOCAL]\r\nI1020 16:27:12.867862    2064 iptables.go:396] running iptables -F [KUBE-PORTALS-CONTAINER -t nat]\r\nI1020 16:27:12.868564    2064 iptables.go:396] running iptables -F [KUBE-PORTALS-HOST -t nat]\r\nI1020 16:27:12.869247    2064 iptables.go:396] running iptables -F [KUBE-NODEPORT-HOST -t nat]\r\nI1020 16:27:12.869921    2064 iptables.go:396] running iptables -F [KUBE-NODEPORT-CONTAINER -t nat]\r\nI1020 16:27:12.870622    2064 iptables.go:396] running iptables -F [KUBE-NODEPORT-NON-LOCAL -t filter]\r\nI1020 16:27:12.871304    2064 iptables.go:396] running iptables -C [INPUT -t filter -m comment --comment kubernetes service portals -j KUBE-SERVICES]\r\nI1020 16:27:12.872129    2064 iptables.go:396] running iptables -C [OUTPUT -t filter -m comment --comment kubernetes service portals -j KUBE-SERVICES]\r\nI1020 16:27:12.872884    2064 iptables.go:396] running iptables -C [OUTPUT -t nat -m comment --comment kubernetes service portals -j KUBE-SERVICES]\r\nI1020 16:27:12.873639    2064 iptables.go:396] running iptables -C [PREROUTING -t nat -m comment --comment kubernetes service portals -j KUBE-SERVICES]\r\nI1020 16:27:12.874398    2064 iptables.go:396] running iptables -C [POSTROUTING -t nat -m comment --comment kubernetes postrouting rules -j KUBE-POSTROUTING]\r\nI1020 16:27:12.875133    2064 iptables.go:314] running iptables-save [-t nat]\r\nI1020 16:27:12.875860    2064 iptables.go:373] running iptables-restore [-T nat --noflush --counters]\r\nI1020 16:27:12.876637    2064 iptables.go:373] running iptables-restore [-T filter --noflush --counters]\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x20 pc=0x1287ad8]\r\n\r\ngoroutine 1 [running]:\r\nk8s.io/kubernetes/pkg/proxy/ipvs.deleteDummyDevice(0x0, 0x0, 0x16076ac, 0xa, 0x15ff1dc, 0x3)\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/proxy/ipvs/proxier.go:1496 +0xa8\r\nk8s.io/kubernetes/pkg/proxy/ipvs.CleanupLeftovers(0x0, 0x0, 0x1f763a0, 0xc420342a40, 0x1f76aa0, 0xc4203e4ab0, 0x0)\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/proxy/ipvs/proxier.go:720 +0x75\r\nk8s.io/kubernetes/cmd/kube-proxy/app.(*ProxyServer).Run(0xc4200b63c0, 0xc42006d601, 0xc4204c2b80)\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kube-proxy/app/server.go:428 +0x10aa\r\nk8s.io/kubernetes/cmd/kube-proxy/app.(*Options).Run(0xc42006d6b0, 0x0, 0x0)\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kube-proxy/app/server.go:229 +0xa0\r\nk8s.io/kubernetes/cmd/kube-proxy/app.NewProxyCommand.func1(0xc4204be6c0, 0xc4201cce60, 0x0, 0x2)\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kube-proxy/app/server.go:344 +0x80\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute(0xc4204be6c0, 0xc42000e130, 0x2, 0x2, 0xc4204be6c0, 0xc42000e130)\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:603 +0x22b\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0xc4204be6c0, 0xc420645f60, 0xc420645f58, 0xc42010b380)\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:689 +0x339\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute(0xc4204be6c0, 0x168ecb0, 0xc4200001a0)\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:648 +0x2b\r\nmain.main()\r\n\t/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kube-proxy/proxy.go:44 +0x96\r\n```\r\n\r\n**What you expected to happen**:\r\n\r\nI expected `kube-proxy --cleanup` not to crash.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nThis can be reproduced this by running the abovementioned command or the shorter\r\n\r\n```\r\n# kube-proxy --cleanup\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\n\u2014\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): v1.8.1 (all components from the `v1.8.1` tag)\r\n- Cloud provider or hardware configuration: GCE\r\n- OS (e.g. from /etc/os-release): Ubuntu 16.04 LTS\r\n- Kernel (e.g. `uname -a`): `Linux worker-0 4.10.0-37-generic #41~16.04.1-Ubuntu SMP Fri Oct 6 22:42:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux`\r\n- Install tools: \u2014\r\n- Others: \u2014",
  "closed_at": "2017-10-26T04:20:33Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/13653959?v=4",
    "events_url": "https://api.github.com/users/k8s-github-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-github-robot/followers",
    "following_url": "https://api.github.com/users/k8s-github-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-github-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-github-robot",
    "id": 13653959,
    "login": "k8s-github-robot",
    "node_id": "MDQ6VXNlcjEzNjUzOTU5",
    "organizations_url": "https://api.github.com/users/k8s-github-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-github-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-github-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-github-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-github-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-github-robot"
  },
  "comments": 1,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/54305/comments",
  "created_at": "2017-10-20T16:28:08Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/54305/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/54305",
  "id": 267231646,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Network.",
      "id": 116712108,
      "name": "sig/network",
      "node_id": "MDU6TGFiZWwxMTY3MTIxMDg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/network"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/54305/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUyNjcyMzE2NDY=",
  "number": 54305,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "v1.8.x: kube-proxy panics on cleanup because of nil pointer",
  "updated_at": "2017-10-26T04:20:33Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/54305",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/2365176?v=4",
    "events_url": "https://api.github.com/users/bmcustodio/events{/privacy}",
    "followers_url": "https://api.github.com/users/bmcustodio/followers",
    "following_url": "https://api.github.com/users/bmcustodio/following{/other_user}",
    "gists_url": "https://api.github.com/users/bmcustodio/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/bmcustodio",
    "id": 2365176,
    "login": "bmcustodio",
    "node_id": "MDQ6VXNlcjIzNjUxNzY=",
    "organizations_url": "https://api.github.com/users/bmcustodio/orgs",
    "received_events_url": "https://api.github.com/users/bmcustodio/received_events",
    "repos_url": "https://api.github.com/users/bmcustodio/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/bmcustodio/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/bmcustodio/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/bmcustodio"
  }
}