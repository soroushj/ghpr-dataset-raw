{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**What happened**:\r\n\r\nAfter upgrade to `v1.18.0`, kubelet can't  start on most of my nodes:\r\n\r\n```\r\nkubelet[29955]: I0326 11:26:59.463314   29955 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.5, apiVersion: 1.40.0\r\nkubelet[29955]: I0326 11:26:59.464273   29955 server.go:1125] Started kubelet\r\nkubelet[29955]: I0326 11:26:59.464344   29955 server.go:145] Starting to listen on 0.0.0.0:10250\r\nkubelet[29955]: E0326 11:26:59.464467   29955 kubelet.go:1305] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data in memory cache\r\nkubelet[29955]: I0326 11:26:59.465908   29955 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer\r\nkubelet[29955]: W0326 11:26:59.466346   29955 oomparser.go:150] error reading /dev/kmsg: read /dev/kmsg: broken pipe\r\nkubelet[29955]: E0326 11:26:59.466402   29955 oomparser.go:126] exiting analyzeLines. OOM events will not be reported.\r\nkubelet[29955]: I0326 11:26:59.466464   29955 volume_manager.go:265] Starting Kubelet Volume Manager\r\nkubelet[29955]: I0326 11:26:59.466707   29955 server.go:393] Adding debug handlers to kubelet server.\r\nkubelet[29955]: I0326 11:26:59.467200   29955 desired_state_of_world_populator.go:139] Desired state populator starts to run\r\nkubelet[29955]: I0326 11:26:59.481018   29955 clientconn.go:106] parsed scheme: \"unix\"\r\nkubelet[29955]: I0326 11:26:59.481035   29955 clientconn.go:106] scheme \"unix\" not registered, fallback to default scheme\r\nkubelet[29955]: I0326 11:26:59.481099   29955 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}\r\nkubelet[29955]: I0326 11:26:59.481110   29955 clientconn.go:933] ClientConn switching balancer to \"pick_first\"\r\nkubelet[29955]: I0326 11:26:59.482101   29955 status_manager.go:158] Starting to sync pod status with apiserver\r\nkubelet[29955]: I0326 11:26:59.482125   29955 kubelet.go:1821] Starting kubelet main sync loop.\r\nkubelet[29955]: E0326 11:26:59.482160   29955 kubelet.go:1845] skipping pod synchronization - [container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]\r\nkubelet[29955]: W0326 11:26:59.513913   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"opennebula-sunstone-64485765f9-zw72q_opennebula\": CNI \r\nkubelet[29955]: W0326 11:26:59.545664   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"victoria-metrics-vminsert-76d9c5fb96-7zrcf_monitoring\"\r\nkubelet[29955]: W0326 11:26:59.555650   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"opennebula-oned-0_opennebula\": CNI failed to retrieve \r\nkubelet[29955]: I0326 11:26:59.566741   29955 kuberuntime_manager.go:978] updating runtime config through cri with podcidr 10.112.3.0/24\r\nkubelet[29955]: I0326 11:26:59.567009   29955 docker_service.go:353] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.112.3.0/24,},}\r\nkubelet[29955]: I0326 11:26:59.567147   29955 kubelet_network.go:77] Setting Pod CIDR:  -> 10.112.3.0/24\r\nkubelet[29955]: I0326 11:26:59.567201   29955 kubelet_node_status.go:294] Setting node annotation to enable volume controller attach/detach\r\nkubelet[29955]: W0326 11:26:59.575639   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"victoria-metrics-vmselect-5f85d8fd6b-d8c4k_monitoring\"\r\nkubelet[29955]: E0326 11:26:59.582255   29955 kubelet.go:1845] skipping pod synchronization - container runtime status check may not have completed yet\r\nkubelet[29955]: W0326 11:26:59.591105   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"victoria-metrics-vminsert-76d9c5fb96-mhg9s_monitoring\"\r\nkubelet[29955]: I0326 11:26:59.594514   29955 kubelet_node_status.go:70] Attempting to register node m1c4\r\nkubelet[29955]: W0326 11:26:59.596651   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"keycloak-stolon-keeper-1_nextcloud\": CNI failed to ret\r\nkubelet[29955]: I0326 11:26:59.600085   29955 kubelet_node_status.go:112] Node m1c4 was previously registered\r\nkubelet[29955]: I0326 11:26:59.600146   29955 kubelet_node_status.go:73] Successfully registered node m1c4\r\nkubelet[29955]: W0326 11:26:59.602258   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"onlyoffice-postgresql-0_nextcloud\": CNI failed to retr\r\nkubelet[29955]: W0326 11:26:59.606953   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"keycloak-stolon-keeper-1_keycloak\": CNI failed to retr\r\nkubelet[29955]: W0326 11:26:59.611679   29955 docker_sandbox.go:400] failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"linstor-csi-node-9vxlj_kube-system\": CNI failed to ret\r\nkubelet[29955]: W0326 11:26:59.615368   29955 docker_container.go:353] ignore error image \"sha256:24017ab20afe2a1df2acc27a6cd429690d040e76ccd823b74409a0cbc7fa9ba3\" not found while inspecting docker container \"2f230771c\r\nkubelet[29955]: panic: runtime error: invalid memory address or nil pointer dereference\r\nkubelet[29955]: [signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x32e92aa]\r\nkubelet[29955]: goroutine 621 [running]:\r\nkubelet[29955]: k8s.io/kubernetes/pkg/kubelet/dockershim.(*dockerService).ContainerStatus(0xc0003cae70, 0x4b24720, 0xc00185c570, 0xc000657f40, 0xc0003cae70, 0xc00185c570, 0xc000f7ab30)\r\nkubelet[29955]:         /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/dockershim/docker_container.go:418 +0x6fa\r\nkubelet[29955]: k8s.io/kubernetes/vendor/k8s.io/cri-api/pkg/apis/runtime/v1alpha2._RuntimeService_ContainerStatus_Handler(0x42e6260, 0xc0003cae70, 0x4b24720, 0xc00185c570, 0xc0013fd200, 0x0, 0x4b24720, 0xc00185c570, 0x\r\nkubelet[29955]:         /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/cri-api/pkg/apis/runtime/v1alpha2/api.pb.go:7882 +0x217\r\nkubelet[29955]: k8s.io/kubernetes/vendor/google.golang.org/grpc.(*Server).processUnaryRPC(0xc0009f6f00, 0x4b84860, 0xc000da2000, 0xc0010db600, 0xc000987dd0, 0x6f215c8, 0x0, 0x0, 0x0)\r\nkubelet[29955]:         /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/google.golang.org/grpc/server.go:1024 +0x4f4\r\nkubelet[29955]: k8s.io/kubernetes/vendor/google.golang.org/grpc.(*Server).handleStream(0xc0009f6f00, 0x4b84860, 0xc000da2000, 0xc0010db600, 0x0)\r\nkubelet[29955]:         /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/google.golang.org/grpc/server.go:1313 +0xd97\r\nkubelet[29955]: k8s.io/kubernetes/vendor/google.golang.org/grpc.(*Server).serveStreams.func1.1(0xc0007d0a00, 0xc0009f6f00, 0x4b84860, 0xc000da2000, 0xc0010db600)\r\nkubelet[29955]:         /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/google.golang.org/grpc/server.go:722 +0xbb\r\nkubelet[29955]: created by k8s.io/kubernetes/vendor/google.golang.org/grpc.(*Server).serveStreams.func1\r\nkubelet[29955]:         /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/google.golang.org/grpc/server.go:720 +0xa1\r\nsystemd[1]: kubelet.service: Main process exited, code=exited, status=2/INVALIDARGUMENT\r\nsystemd[1]: kubelet.service: Failed with result 'exit-code'.\r\n```\r\n\r\nI've tried to restart `docker.service`, and cleanup `/var/lib/kubelet/` directory, also reset and rejoin to the cluster, all without success, kubelet is keeps dying.\r\n\r\n**What you expected to happen**:\r\n\r\nKubelet is up and working\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version: `v1.18.0`\r\n- Cloud provider or hardware configuration: Bare Metal\r\n- OS: `Ubuntu 18.04.3 LTS`\r\n- Kernel: `4.15.18-24-pve`\r\n- Install tools: `kubeadm`\r\n- Network plugin and version: `kube-router v0.4.0`\r\n- Others:\r\n  ```\r\n  Docker version 19.03.5, build 633a0ea838\r\n  ```\r\n",
  "closed_at": "2020-03-27T00:02:57Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/980082?v=4",
    "events_url": "https://api.github.com/users/liggitt/events{/privacy}",
    "followers_url": "https://api.github.com/users/liggitt/followers",
    "following_url": "https://api.github.com/users/liggitt/following{/other_user}",
    "gists_url": "https://api.github.com/users/liggitt/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/liggitt",
    "id": 980082,
    "login": "liggitt",
    "node_id": "MDQ6VXNlcjk4MDA4Mg==",
    "organizations_url": "https://api.github.com/users/liggitt/orgs",
    "received_events_url": "https://api.github.com/users/liggitt/received_events",
    "repos_url": "https://api.github.com/users/liggitt/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/liggitt/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/liggitt/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/liggitt"
  },
  "comments": 5,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/89529/comments",
  "created_at": "2020-03-26T16:00:18Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/89529/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/89529",
  "id": 588532538,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Node.",
      "id": 173493665,
      "name": "sig/node",
      "node_id": "MDU6TGFiZWwxNzM0OTM2NjU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/node"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/89529/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1ODg1MzI1Mzg=",
  "number": 89529,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Invalid memory address or nil pointer dereference error",
  "updated_at": "2020-03-27T00:03:07Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/89529",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/7556217?v=4",
    "events_url": "https://api.github.com/users/kvaps/events{/privacy}",
    "followers_url": "https://api.github.com/users/kvaps/followers",
    "following_url": "https://api.github.com/users/kvaps/following{/other_user}",
    "gists_url": "https://api.github.com/users/kvaps/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/kvaps",
    "id": 7556217,
    "login": "kvaps",
    "node_id": "MDQ6VXNlcjc1NTYyMTc=",
    "organizations_url": "https://api.github.com/users/kvaps/orgs",
    "received_events_url": "https://api.github.com/users/kvaps/received_events",
    "repos_url": "https://api.github.com/users/kvaps/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/kvaps/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/kvaps/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/kvaps"
  }
}