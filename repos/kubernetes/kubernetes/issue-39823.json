{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):  BUG REPORT\r\n\r\n<!--\r\nIf this is a BUG REPORT, please:\r\n  - Fill in as much of the template below as you can.  If you leave out\r\n    information, we can't help you as well.\r\n\r\nIf this is a FEATURE REQUEST, please:\r\n  - Describe *in detail* the feature/behavior/change you'd like to see.\r\n\r\nIn both cases, be ready for followup questions, and please respond in a timely\r\nmanner.  If we can't reproduce a bug or think a feature already exists, we\r\nmight close your issue.  If we're wrong, PLEASE feel free to reopen it and\r\nexplain why.\r\n-->\r\n\r\n**Kubernetes version** (use `kubectl version`):  1.5.1\r\n\r\n\r\n**Environment**:\r\n- **Cloud provider or hardware configuration**:  3x Ubuntu 16.04 nodes running in VMware vSphere\r\n- **OS** (e.g. from /etc/os-release):  Ubuntu 16.04\r\n- **Kernel** (e.g. `uname -a`):  4.4.0-57-generic\r\n- **Install tools**:  kubeadm\r\n- **Others**:  CNI: Canal (flannel + calico)\r\n\r\n\r\n**What happened**:\r\n\r\nI originally opened #39658 to make Canal CNI work when the node's FORWARD chain default policy is set to DROP.  However, I realized that the DROP policy was the root cause for why my other CNI tests also were failing.  When either weave and flannel CNI is used and kube-proxy has --masquerade-all enabled, pod to pod and pod to service communication fails.  Specifically, kubernetes-dashboard and tiller-deploy fail to reach the kube-apiserver and enter a crash loop.  Also, the kube-dns pod also crashes.\r\n\r\n<details>\r\n<summary>kubectl get pods --all-namespaces -o wide</summary>\r\n\r\n```\r\ndeploy@ravi-kube196:~$ kubectl get pods --all-namespaces -o wide\r\nNAMESPACE     NAME                                    READY     STATUS             RESTARTS   AGE       IP               NODE\r\nkube-system   dummy-2088944543-flctq                  1/1       Running            0          23m       10.163.148.197   ravi-kube197\r\nkube-system   etcd-ravi-kube196                       1/1       Running            1          23m       10.163.148.196   ravi-kube196\r\nkube-system   kube-apiserver-ravi-kube196             1/1       Running            1          23m       10.163.148.196   ravi-kube196\r\nkube-system   kube-controller-manager-ravi-kube196    1/1       Running            1          23m       10.163.148.196   ravi-kube196\r\nkube-system   kube-discovery-1769846148-qdzsv         1/1       Running            0          23m       10.163.148.196   ravi-kube196\r\nkube-system   kube-dns-2924299975-gz34l               3/4       Running            18         23m       10.96.2.7        ravi-kube197\r\nkube-system   kube-flannel-ds-2f9tc                   2/2       Running            0          23m       10.163.148.196   ravi-kube196\r\nkube-system   kube-flannel-ds-mgkk9                   2/2       Running            0          23m       10.163.148.198   ravi-kube198\r\nkube-system   kube-flannel-ds-s8ktw                   2/2       Running            0          23m       10.163.148.197   ravi-kube197\r\nkube-system   kube-proxy-3gxs7                        1/1       Running            0          23m       10.163.148.196   ravi-kube196\r\nkube-system   kube-proxy-ftdp9                        1/1       Running            0          23m       10.163.148.198   ravi-kube198\r\nkube-system   kube-proxy-rc0wv                        1/1       Running            0          23m       10.163.148.197   ravi-kube197\r\nkube-system   kube-scheduler-ravi-kube196             1/1       Running            1          23m       10.163.148.196   ravi-kube196\r\nkube-system   kubernetes-dashboard-3203831700-5vzdq   0/1       CrashLoopBackOff   8          23m       10.96.2.8        ravi-kube197\r\nkube-system   tiller-deploy-2885612843-jm143          0/1       CrashLoopBackOff   13         23m       10.96.1.12       ravi-kube198\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>kubectl logs --namespace=kube-system kubernetes-dashboard-3203831700-5vzdq</summary>\r\n\r\n```\r\ndeploy@ravi-kube196:~$ kubectl logs --namespace=kube-system kubernetes-dashboard-3203831700-5vzdq\r\nUsing HTTP port: 9090\r\nCreating API server client for https://10.96.0.1:443\r\nError while initializing connection to Kubernetes apiserver. This most likely means that the cluster is misconfigured (e.g., it has invalid apiserver certificates or service accounts configuration) or the --apiserver-host param points to a server that does not exist. Reason: Get https://10.96.0.1:443/version: dial tcp 10.96.0.1:443: i/o timeout\r\nRefer to the troubleshooting guide for more information: https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>kubectl describe --namespace=kube-system pod kubernetes-dashboard-3203831700-5vzdq</summary>\r\n\r\n```\r\ndeploy@ravi-kube196:~$ kubectl describe --namespace=kube-system pod kubernetes-dashboard-3203831700-5vzdq\r\nName:           kubernetes-dashboard-3203831700-5vzdq\r\nNamespace:      kube-system\r\nNode:           ravi-kube197/10.163.148.197\r\nStart Time:     Thu, 12 Jan 2017 20:18:59 +0000\r\nLabels:         app=kubernetes-dashboard\r\n                pod-template-hash=3203831700\r\nStatus:         Running\r\nIP:             10.96.2.8\r\nControllers:    ReplicaSet/kubernetes-dashboard-3203831700\r\nContainers:\r\n  kubernetes-dashboard:\r\n    Container ID:       docker://5718b319fe4af33b97115a14cc17083bab078d868556cb4e2c6f93363da7a462\r\n    Image:              gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.1\r\n    Image ID:           docker-pullable://gcr.io/google_containers/kubernetes-dashboard-amd64@sha256:46a09eb9c611e625e7de3fcf325cf78e629d002e57dc80348e9b0638338206b5\r\n    Port:               9090/TCP\r\n    State:              Running\r\n      Started:          Thu, 12 Jan 2017 20:44:34 +0000\r\n    Last State:         Terminated\r\n      Reason:           Error\r\n      Exit Code:        1\r\n      Started:          Thu, 12 Jan 2017 20:38:52 +0000\r\n      Finished:         Thu, 12 Jan 2017 20:39:22 +0000\r\n    Ready:              True\r\n    Restart Count:      9\r\n    Liveness:           http-get http://:9090/ delay=30s timeout=30s period=10s #success=1 #failure=3\r\n    Volume Mounts:\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zfzvp (ro)\r\n    Environment Variables:      <none>\r\nConditions:\r\n  Type          Status\r\n  Initialized   True\r\n  Ready         True\r\n  PodScheduled  True\r\nVolumes:\r\n  default-token-zfzvp:\r\n    Type:       Secret (a volume populated by a Secret)\r\n    SecretName: default-token-zfzvp\r\nQoS Class:      BestEffort\r\nTolerations:    dedicated=master:Equal:NoSchedule\r\nEvents:\r\n  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message\r\n  ---------     --------        -----   ----                    -------------                           --------        ------          -------\r\n  25m           25m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-dashboard-3203831700-5vzdq to ravi-kube197\r\n  25m           25m             1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal          Created         Created container with docker id 8cee6d8c1fcd; Security:[seccomp=unconfined]\r\n  25m           25m             1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal          Started         Started container with docker id 8cee6d8c1fcd\r\n  25m           25m             1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal          Started         Started container with docker id 1a9b25a74c94\r\n  25m           25m             1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal          Created         Created container with docker id 1a9b25a74c94; Security:[seccomp=unconfined]\r\n  24m           24m             2       {kubelet ravi-kube197}                                          Warning         FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"Back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-3203831700-5vzdq_kube-system(5a050ca2-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  24m   24m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Started         Started container with docker id 2da001604ef9\r\n  24m   24m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Created         Created container with docker id 2da001604ef9; Security:[seccomp=unconfined]\r\n  23m   23m     2       {kubelet ravi-kube197}                                          Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"Back-off 20s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-3203831700-5vzdq_kube-system(5a050ca2-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  23m   23m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Started         Started container with docker id 257a758141b0\r\n  23m   23m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Created         Created container with docker id 257a758141b0; Security:[seccomp=unconfined]\r\n  23m   22m     4       {kubelet ravi-kube197}                                          Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"Back-off 40s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-3203831700-5vzdq_kube-system(5a050ca2-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  22m   22m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Started         Started container with docker id 3eb69d8169d6\r\n  22m   22m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Created         Created container with docker id 3eb69d8169d6; Security:[seccomp=unconfined]\r\n  21m   20m     7       {kubelet ravi-kube197}                                          Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"Back-off 1m20s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-3203831700-5vzdq_kube-system(5a050ca2-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  20m   20m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Created         Created container with docker id 6244b99877be; Security:[seccomp=unconfined]\r\n  20m   20m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Started         Started container with docker id 6244b99877be\r\n  20m   20m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Warning Unhealthy       Liveness probe failed: Get http://10.96.2.8:9090/: dial tcp 10.96.2.8:9090: getsockopt: connection refused\r\n  19m   17m     13      {kubelet ravi-kube197}                                          Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-3203831700-5vzdq_kube-system(5a050ca2-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  17m   17m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Created         Created container with docker id 39a9bf9798a8; Security:[seccomp=unconfined]\r\n  17m   17m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Started         Started container with docker id 39a9bf9798a8\r\n  11m   11m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Created         Created container with docker id 24a59311513f; Security:[seccomp=unconfined]\r\n  11m   11m     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Started         Started container with docker id 24a59311513f\r\n  5m    5m      1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Started         Started container with docker id b2970a8b9df9\r\n  5m    5m      1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Created         Created container with docker id b2970a8b9df9; Security:[seccomp=unconfined]\r\n  24m   29s     98      {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Warning BackOff         Back-off restarting failed docker container\r\n  16m   29s     70      {kubelet ravi-kube197}                                          Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-3203831700-5vzdq_kube-system(5a050ca2-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  25m   15s     10      {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Pulling pulling image \"gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.1\"\r\n  25m   15s     10      {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Pulled  Successfully pulled image \"gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.1\"\r\n  15s   15s     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Created (events with common reason combined)\r\n  15s   15s     1       {kubelet ravi-kube197}  spec.containers{kubernetes-dashboard}   Normal  Started (events with common reason combined)\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>kubectl describe --namespace=kube-system pod kube-dns-2924299975-gz34l</summary>\r\n\r\n```\r\ndeploy@ravi-kube196:~$ kubectl describe --namespace=kube-system pod kube-dns-2924299975-gz34l\r\nName:           kube-dns-2924299975-gz34l\r\nNamespace:      kube-system\r\nNode:           ravi-kube197/10.163.148.197\r\nStart Time:     Thu, 12 Jan 2017 20:18:56 +0000\r\nLabels:         component=kube-dns\r\n                k8s-app=kube-dns\r\n                kubernetes.io/cluster-service=true\r\n                name=kube-dns\r\n                pod-template-hash=2924299975\r\n                tier=node\r\nStatus:         Running\r\nIP:             10.96.2.7\r\nControllers:    ReplicaSet/kube-dns-2924299975\r\nContainers:\r\n  kube-dns:\r\n    Container ID:       docker://1827435a310c7b343c28be9f46e69155bc1da45bfa97376da1b39436b9580a58\r\n    Image:              gcr.io/google_containers/kubedns-amd64:1.9\r\n    Image ID:           docker-pullable://gcr.io/google_containers/kubedns-amd64@sha256:3d3d67f519300af646e00adcf860b2f380d35ed4364e550d74002dadace20ead\r\n    Ports:              10053/UDP, 10053/TCP, 10055/TCP\r\n    Args:\r\n      --domain=cluster.local\r\n      --dns-port=10053\r\n      --config-map=kube-dns\r\n      --v=2\r\n    Limits:\r\n      memory:   170Mi\r\n    Requests:\r\n      cpu:              100m\r\n      memory:           70Mi\r\n    State:              Waiting\r\n      Reason:           CrashLoopBackOff\r\n    Last State:         Terminated\r\n      Reason:           Error\r\n      Exit Code:        137\r\n      Started:          Thu, 12 Jan 2017 20:41:46 +0000\r\n      Finished:         Thu, 12 Jan 2017 20:43:16 +0000\r\n    Ready:              False\r\n    Restart Count:      9\r\n    Liveness:           http-get http://:8080/healthz-kubedns delay=60s timeout=5s period=10s #success=1 #failure=5\r\n    Readiness:          http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3\r\n    Volume Mounts:\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zfzvp (ro)\r\n    Environment Variables:\r\n      PROMETHEUS_PORT:  10055\r\n  dnsmasq:\r\n    Container ID:       docker://c4cbdc7d245b412046dd7f71df3f9b78402d1d1d8bb686461e1d79a2fa9d622c\r\n    Image:              gcr.io/google_containers/kube-dnsmasq-amd64:1.4\r\n    Image ID:           docker-pullable://gcr.io/google_containers/kube-dnsmasq-amd64@sha256:a722df15c0cf87779aad8ba2468cf072dd208cb5d7cfcaedd90e66b3da9ea9d2\r\n    Ports:              53/UDP, 53/TCP\r\n    Args:\r\n      --cache-size=1000\r\n      --no-resolv\r\n      --server=127.0.0.1#10053\r\n      --log-facility=-\r\n    Requests:\r\n      cpu:              150m\r\n      memory:           10Mi\r\n    State:              Waiting\r\n      Reason:           CrashLoopBackOff\r\n    Last State:         Terminated\r\n      Reason:           Error\r\n      Exit Code:        137\r\n      Started:          Thu, 12 Jan 2017 20:42:26 +0000\r\n      Finished:         Thu, 12 Jan 2017 20:43:56 +0000\r\n    Ready:              False\r\n    Restart Count:      9\r\n    Liveness:           http-get http://:8080/healthz-dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5\r\n    Volume Mounts:\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zfzvp (ro)\r\n    Environment Variables:      <none>\r\n  dnsmasq-metrics:\r\n    Container ID:       docker://6d40d421a883ed08b08c90db87a7886edd5f1ecbd8fad555cee698392aeca74e\r\n    Image:              gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\r\n    Image ID:           docker-pullable://gcr.io/google_containers/dnsmasq-metrics-amd64@sha256:4063e37fd9b2fd91b7cc5392ed32b30b9c8162c4c7ad2787624306fc133e80a9\r\n    Port:               10054/TCP\r\n    Args:\r\n      --v=2\r\n      --logtostderr\r\n    Requests:\r\n      memory:           10Mi\r\n    State:              Running\r\n      Started:          Thu, 12 Jan 2017 20:18:58 +0000\r\n    Ready:              True\r\n    Restart Count:      0\r\n    Liveness:           http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5\r\n    Volume Mounts:\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zfzvp (ro)\r\n    Environment Variables:      <none>\r\n  healthz:\r\n    Container ID:       docker://6be2f1379179a74fdaa823bfef08c46d57b32a04fa4e12afba9b8b53715a4008\r\n    Image:              gcr.io/google_containers/exechealthz-amd64:1.2\r\n    Image ID:           docker-pullable://gcr.io/google_containers/exechealthz-amd64@sha256:503e158c3f65ed7399f54010571c7c977ade7fe59010695f48d9650d83488c0a\r\n    Port:               8080/TCP\r\n    Args:\r\n      --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null\r\n      --url=/healthz-dnsmasq\r\n      --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null\r\n      --url=/healthz-kubedns\r\n      --port=8080\r\n      --quiet\r\n    Limits:\r\n      memory:   50Mi\r\n    Requests:\r\n      cpu:              10m\r\n      memory:           50Mi\r\n    State:              Running\r\n      Started:          Thu, 12 Jan 2017 20:18:58 +0000\r\n    Ready:              True\r\n    Restart Count:      0\r\n    Volume Mounts:\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zfzvp (ro)\r\n    Environment Variables:      <none>\r\nConditions:\r\n  Type          Status\r\n  Initialized   True\r\n  Ready         False\r\n  PodScheduled  True\r\nVolumes:\r\n  default-token-zfzvp:\r\n    Type:       Secret (a volume populated by a Secret)\r\n    SecretName: default-token-zfzvp\r\nQoS Class:      Burstable\r\nTolerations:    dedicated=master:NoSchedule\r\nEvents:\r\n  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message\r\n  ---------     --------        -----   ----                    -------------                           --------        ------          -------\r\n  27m           27m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kube-dns-2924299975-gz34l to ravi-kube197\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Started         Started container with docker id af0b015ac580\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Created         Created container with docker id af0b015ac580; Security:[seccomp=unconfined]\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Created         Created container with docker id 8ef0eccb7a5f; Security:[seccomp=unconfined]\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Started         Started container with docker id 8ef0eccb7a5f\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq-metrics}        Normal          Pulled          Container image \"gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\" already present on machine\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq-metrics}        Normal          Created         Created container with docker id 6d40d421a883; Security:[seccomp=unconfined]\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{healthz}                Normal          Started         Started container with docker id 6be2f1379179\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{healthz}                Normal          Created         Created container with docker id 6be2f1379179; Security:[seccomp=unconfined]\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq-metrics}        Normal          Started         Started container with docker id 6d40d421a883\r\n  27m           27m             1       {kubelet ravi-kube197}  spec.containers{healthz}                Normal          Pulled          Container image \"gcr.io/google_containers/exechealthz-amd64:1.2\" already present on machine\r\n  25m           25m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Killing         Killing container with docker id af0b015ac580: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"dnsmasq\" is unhealthy, it will be killed and re-created.\r\n  24m           24m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Created         Created container with docker id 4fa958a4757b; Security:[seccomp=unconfined]\r\n  24m           24m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Killing         Killing container with docker id 8ef0eccb7a5f: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"kube-dns\" is unhealthy, it will be killed and re-created.\r\n  24m           24m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Created         Created container with docker id bd28687cc3a3; Security:[seccomp=unconfined]\r\n  24m           24m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Started         Started container with docker id bd28687cc3a3\r\n  24m           24m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Started         Started container with docker id 4fa958a4757b\r\n  23m           23m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Killing         Killing container with docker id 4fa958a4757b: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"kube-dns\" is unhealthy, it will be killed and re-created.\r\n  23m           23m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Created         Created container with docker id b6692369e2e9; Security:[seccomp=unconfined]\r\n  23m           23m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Started         Started container with docker id b6692369e2e9\r\n  22m           22m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Started         Started container with docker id dbc73eb48e9a\r\n  22m           22m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Killing         Killing container with docker id bd28687cc3a3: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"dnsmasq\" is unhealthy, it will be killed and re-created.\r\n  22m           22m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Created         Created container with docker id dbc73eb48e9a; Security:[seccomp=unconfined]\r\n  21m           21m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Killing         Killing container with docker id b6692369e2e9: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"kube-dns\" is unhealthy, it will be killed and re-created.\r\n  21m           21m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Created         Created container with docker id d961e82157ba; Security:[seccomp=unconfined]\r\n  21m           21m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Started         Started container with docker id d961e82157ba\r\n  21m           21m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Killing         Killing container with docker id dbc73eb48e9a: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"dnsmasq\" is unhealthy, it will be killed and re-created.\r\n  20m           20m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Killing         Killing container with docker id d961e82157ba: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"kube-dns\" is unhealthy, it will be killed and re-created.\r\n  19m           19m             1       {kubelet ravi-kube197}  spec.containers{dnsmasq}                Normal          Killing         Killing container with docker id 8ee9a9b16e2e: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"dnsmasq\" is unhealthy, it will be killed and re-created.\r\n  18m           18m             1       {kubelet ravi-kube197}  spec.containers{kube-dns}               Normal          Killing         Killing container with docker id d035ec298418: pod \"kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\" container \"kube-dns\" is unhealthy, it will be killed and re-created.\r\n  15m           15m             2       {kubelet ravi-kube197}                                          Warning         FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"kube-dns\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=kube-dns pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  15m   15m     2       {kubelet ravi-kube197}          Warning FailedSync      Error syncing pod, skipping: [failed to \"StartContainer\" for \"dnsmasq\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n, failed to \"StartContainer\" for \"kube-dns\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=kube-dns pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n]\r\n  15m   13m     9       {kubelet ravi-kube197}          Warning FailedSync      Error syncing pod, skipping: [failed to \"StartContainer\" for \"kube-dns\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=kube-dns pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n, failed to \"StartContainer\" for \"dnsmasq\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n]\r\n  13m   12m     4       {kubelet ravi-kube197}          Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"dnsmasq\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  6m    5m      4       {kubelet ravi-kube197}          Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"dnsmasq\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  27m   4m      10      {kubelet ravi-kube197}  spec.containers{kube-dns}       Normal  Pulled          Container image \"gcr.io/google_containers/kubedns-amd64:1.9\" already present on machine\r\n  27m   4m      10      {kubelet ravi-kube197}  spec.containers{dnsmasq}        Normal  Pulled          Container image \"gcr.io/google_containers/kube-dnsmasq-amd64:1.4\" already present on machine\r\n  21m   4m      13      {kubelet ravi-kube197}  spec.containers{dnsmasq}        Normal  Created         (events with common reason combined)\r\n  21m   4m      13      {kubelet ravi-kube197}  spec.containers{dnsmasq}        Normal  Started         (events with common reason combined)\r\n  27m   3m      100     {kubelet ravi-kube197}  spec.containers{kube-dns}       Warning Unhealthy       Readiness probe failed: Get http://10.96.2.7:8081/readiness: dial tcp 10.96.2.7:8081: getsockopt: connection refused\r\n  11m   3m      4       {kubelet ravi-kube197}                                  Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"kube-dns\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=kube-dns pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n\r\n  26m   3m      28      {kubelet ravi-kube197}  spec.containers{dnsmasq}        Warning Unhealthy       Liveness probe failed: HTTP probe failed with statuscode: 503\r\n  18m   2m      11      {kubelet ravi-kube197}  spec.containers{dnsmasq}        Normal  Killing         (events with common reason combined)\r\n  10m   54s     11      {kubelet ravi-kube197}                                  Warning FailedSync      Error syncing pod, skipping: [failed to \"StartContainer\" for \"dnsmasq\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n, failed to \"StartContainer\" for \"kube-dns\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=kube-dns pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n]\r\n  10m   11s     24      {kubelet ravi-kube197}          Warning FailedSync      Error syncing pod, skipping: [failed to \"StartContainer\" for \"kube-dns\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=kube-dns pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n, failed to \"StartContainer\" for \"dnsmasq\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-2924299975-gz34l_kube-system(582a3424-d904-11e6-bec0-0050568a2a2f)\"\r\n]\r\n  15m   11s     106     {kubelet ravi-kube197}  spec.containers{kube-dns}       Warning BackOff Back-off restarting failed docker container\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>sudo iptables -t filter -v -L FORWARD --line-numbers (197 currently has the dashboard pod scheduled.  Note the 2 dropped packets.)</summary>\r\n\r\n```\r\ndeploy@ravi-kube197:~$ sudo iptables -t filter -v -L FORWARD --line-numbers\r\nChain FORWARD (policy DROP 2 packets, 164 bytes)\r\nnum   pkts bytes target     prot opt in     out     source               destination\r\n1     1007 67570 DOCKER-ISOLATION  all  --  any    any     anywhere             anywhere\r\n2        0     0 DOCKER     all  --  any    docker0  anywhere             anywhere\r\n3        0     0 ACCEPT     all  --  any    docker0  anywhere             anywhere             ctstate RELATED,ESTABLISHED\r\n4        0     0 ACCEPT     all  --  docker0 !docker0  anywhere             anywhere\r\n5        0     0 ACCEPT     all  --  docker0 docker0  anywhere             anywhere\r\n6     1007 67570 ufw-before-logging-forward  all  --  any    any     anywhere             anywhere\r\n7     1007 67570 ufw-before-forward  all  --  any    any     anywhere             anywhere\r\n8     1007 67570 ufw-after-forward  all  --  any    any     anywhere             anywhere\r\n9     1007 67570 ufw-after-logging-forward  all  --  any    any     anywhere             anywhere\r\n10    1007 67570 ufw-reject-forward  all  --  any    any     anywhere             anywhere\r\n11    1007 67570 ufw-track-forward  all  --  any    any     anywhere             anywhere\r\n```\r\n</details>\r\n\r\n</br>\r\n\r\n**What you expected to happen**:\r\n\r\nEither it should be better documented that the default FORWARD chain policy should be ACCEPT or rules should be added to prevent packets from hitting the default policy.  As a note, Ubuntu 16.04 defaults the FORWARD chain to DROP.  I assume the `--masquerade-all` is why the packets are going through the FORWARD chain in the first place but without `--masquerade-all`, the dashboard still can not access the kube-apiserver.\r\n\r\nOne option that I suggested in #39658 is to have kube-proxy add an iptables FORWARD rule to ACCEPT packets that have been marked by KUBE-MARK-MASQ.\r\n\r\n**How to reproduce it** (as minimally and precisely as possible):\r\n\r\nCreate a cluster with:\r\n\r\n- Weave or Flannel CNI enabled\r\n- `iptables -P FORWARD DROP` on all nodes\r\n- kube-proxy set to `--masquerade-all`\r\n- kubernetes-dashboard enabled.  kubernetes-dashboard will crash often.\r\n\r\n**Anything else do we need to know**:\r\n\r\n#39658 is closed because I thought the issue was specific to Canal and instead opened https://github.com/projectcalico/canal/issues/31.  However, this looks to be a larger issue that kubernetes itself should address.",
  "closed_at": "2017-11-14T08:09:57Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/13653959?v=4",
    "events_url": "https://api.github.com/users/k8s-github-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-github-robot/followers",
    "following_url": "https://api.github.com/users/k8s-github-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-github-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-github-robot",
    "id": 13653959,
    "login": "k8s-github-robot",
    "node_id": "MDQ6VXNlcjEzNjUzOTU5",
    "organizations_url": "https://api.github.com/users/k8s-github-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-github-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-github-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-github-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-github-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-github-robot"
  },
  "comments": 15,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/39823/comments",
  "created_at": "2017-01-12T20:52:37Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/39823/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/39823",
  "id": 200476418,
  "labels": [
    {
      "color": "0052cc",
      "default": false,
      "description": null,
      "id": 128716589,
      "name": "area/kube-proxy",
      "node_id": "MDU6TGFiZWwxMjg3MTY1ODk=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/area/kube-proxy"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Network.",
      "id": 116712108,
      "name": "sig/network",
      "node_id": "MDU6TGFiZWwxMTY3MTIxMDg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/network"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/39823/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUyMDA0NzY0MTg=",
  "number": 39823,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Handle nodes with iptables FORWARD DROP better",
  "updated_at": "2019-01-20T15:34:57Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/39823",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/4229409?v=4",
    "events_url": "https://api.github.com/users/ravishivt/events{/privacy}",
    "followers_url": "https://api.github.com/users/ravishivt/followers",
    "following_url": "https://api.github.com/users/ravishivt/following{/other_user}",
    "gists_url": "https://api.github.com/users/ravishivt/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/ravishivt",
    "id": 4229409,
    "login": "ravishivt",
    "node_id": "MDQ6VXNlcjQyMjk0MDk=",
    "organizations_url": "https://api.github.com/users/ravishivt/orgs",
    "received_events_url": "https://api.github.com/users/ravishivt/received_events",
    "repos_url": "https://api.github.com/users/ravishivt/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/ravishivt/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ravishivt/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/ravishivt"
  }
}