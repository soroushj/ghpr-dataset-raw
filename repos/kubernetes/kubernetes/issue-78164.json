{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/100893?v=4",
    "events_url": "https://api.github.com/users/justinsb/events{/privacy}",
    "followers_url": "https://api.github.com/users/justinsb/followers",
    "following_url": "https://api.github.com/users/justinsb/following{/other_user}",
    "gists_url": "https://api.github.com/users/justinsb/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/justinsb",
    "id": 100893,
    "login": "justinsb",
    "node_id": "MDQ6VXNlcjEwMDg5Mw==",
    "organizations_url": "https://api.github.com/users/justinsb/orgs",
    "received_events_url": "https://api.github.com/users/justinsb/received_events",
    "repos_url": "https://api.github.com/users/justinsb/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/justinsb/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/justinsb/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/justinsb"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars2.githubusercontent.com/u/100893?v=4",
      "events_url": "https://api.github.com/users/justinsb/events{/privacy}",
      "followers_url": "https://api.github.com/users/justinsb/followers",
      "following_url": "https://api.github.com/users/justinsb/following{/other_user}",
      "gists_url": "https://api.github.com/users/justinsb/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/justinsb",
      "id": 100893,
      "login": "justinsb",
      "node_id": "MDQ6VXNlcjEwMDg5Mw==",
      "organizations_url": "https://api.github.com/users/justinsb/orgs",
      "received_events_url": "https://api.github.com/users/justinsb/received_events",
      "repos_url": "https://api.github.com/users/justinsb/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/justinsb/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/justinsb/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/justinsb"
    },
    {
      "avatar_url": "https://avatars0.githubusercontent.com/u/13111288?v=4",
      "events_url": "https://api.github.com/users/wongma7/events{/privacy}",
      "followers_url": "https://api.github.com/users/wongma7/followers",
      "following_url": "https://api.github.com/users/wongma7/following{/other_user}",
      "gists_url": "https://api.github.com/users/wongma7/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/wongma7",
      "id": 13111288,
      "login": "wongma7",
      "node_id": "MDQ6VXNlcjEzMTExMjg4",
      "organizations_url": "https://api.github.com/users/wongma7/orgs",
      "received_events_url": "https://api.github.com/users/wongma7/received_events",
      "repos_url": "https://api.github.com/users/wongma7/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/wongma7/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wongma7/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/wongma7"
    }
  ],
  "author_association": "NONE",
  "body": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\nTrying to get logs from pod using *client-go*.\r\n\r\n**What happened**:\r\nError:\r\n```\r\ntime=\"2019-05-20T12:40:54Z\" level=info msg=\"Error on get logs: an error on the server (\\\"unknown\\\") has prevented the request from succeeding (get pods nsmgr-ip-192-168-218-117.us-east-2.compute.internal) retrying\"\r\n```\r\n\r\n**What you expected to happen**:\r\nSuccesful response with logs\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nRarely happens on CI\r\n\r\n**Anything else we need to know?**:\r\n*Exec*, *Deploy* commands are working fine even if error with getting logs happen \r\nThere are panic with SIGSEGV in kubelet logs:\r\n```\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.657001    4502 kubelet_node_status.go:73] Successfully registered node ip-192-168-218-117.us-east-2.compute.internal\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878777    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-net-dir\" (UniqueName: \"kubernetes.io/host-path/32a6b763-7afb-11e9-93a9-0236cdd357f4-cni-net-dir\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878836    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"log-dir\" (UniqueName: \"kubernetes.io/host-path/32a6b763-7afb-11e9-93a9-0236cdd357f4-log-dir\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878860    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/32a64068-7afb-11e9-93a9-0236cdd357f4-lib-modules\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878887    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/configmap/32a64068-7afb-11e9-93a9-0236cdd357f4-kubeconfig\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878908    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"config\" (UniqueName: \"kubernetes.io/configmap/32a64068-7afb-11e9-93a9-0236cdd357f4-config\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878930    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy-token-kqppp\" (UniqueName: \"kubernetes.io/secret/32a64068-7afb-11e9-93a9-0236cdd357f4-kube-proxy-token-kqppp\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878952    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-bin-dir\" (UniqueName: \"kubernetes.io/host-path/32a6b763-7afb-11e9-93a9-0236cdd357f4-cni-bin-dir\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878976    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"aws-node-token-b4qfw\" (UniqueName: \"kubernetes.io/secret/32a6b763-7afb-11e9-93a9-0236cdd357f4-aws-node-token-b4qfw\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.878998    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"varlog\" (UniqueName: \"kubernetes.io/host-path/32a64068-7afb-11e9-93a9-0236cdd357f4-varlog\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.879018    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/32a64068-7afb-11e9-93a9-0236cdd357f4-xtables-lock\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: I0520 12:31:31.879039    4502 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"dockersock\" (UniqueName: \"kubernetes.io/host-path/32a6b763-7afb-11e9-93a9-0236cdd357f4-dockersock\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:31 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: W0520 12:31:31.995724    4502 container.go:393] Failed to create summary reader for \"/system.slice/run-5238.scope\": none of the resources are being tracked.\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: panic: runtime error: invalid memory address or nil pointer dereference\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: [signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x1bdaed0]\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: goroutine 1023 [running]:\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/pkg/credentialprovider/aws.(*ecrProvider).Provide(0xc4212ea450, 0x4098c1e)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/credentialprovider/aws/aws_credentials.go:191 +0x60\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/pkg/credentialprovider.(*CachingDockerConfigProvider).Provide(0xc421b168c0, 0x0)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/credentialprovider/provider.go:120 +0x1c6\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/pkg/credentialprovider/aws.(*lazyEcrProvider).LazyProvide(0xc420ab8cc0, 0xc420cff560)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/credentialprovider/aws/aws_credentials.go:134 +0x63\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/pkg/credentialprovider.LazyProvide(0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/credentialprovider/provider.go:45 +0x77\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/pkg/kubelet/dockershim.ensureSandboxImageExists(0x44572c0, 0xc420fe49b0, 0x7ffdc6274eda, 0x40, 0x0, 0x3cbf960)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/dockershim/helpers.go:347 +0x343\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/pkg/kubelet/dockershim.(*dockerService).RunPodSandbox(0xc420f54140, 0x4419400, 0xc420d01950, 0xc420ae0ae0, 0x0, 0x0, 0x0)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/dockershim/docker_sandbox.go:94 +0xaa\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/pkg/kubelet/apis/cri/runtime/v1alpha2._RuntimeService_RunPodSandbox_Handler(0x3fd65c0, 0xc420f54140, 0x4419400, 0xc420d01950, 0xc421d9bd60, 0x0, 0x0, 0x0, 0xc420ca5b48, 0x57018e)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/apis/cri/runtime/v1alpha2/api.pb.go:4165 +0x244\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/vendor/google.golang.org/grpc.(*Server).processUnaryRPC(0xc4203dfe00, 0x442cce0, 0xc420b02000, 0xc420cfab00, 0xc420a6b680, 0x63bf218, 0x0, 0x0, 0x0)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/google.golang.org/grpc/server.go:843 +0x9c8\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/vendor/google.golang.org/grpc.(*Server).handleStream(0xc4203dfe00, 0x442cce0, 0xc420b02000, 0xc420cfab00, 0x0)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/google.golang.org/grpc/server.go:1040 +0x1318\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: k8s.io/kubernetes/vendor/google.golang.org/grpc.(*Server).serveStreams.func1.1(0xc420fe14d0, 0xc4203dfe00, 0x442cce0, 0xc420b02000, 0xc420cfab00)\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/google.golang.org/grpc/server.go:589 +0x9f\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: created by k8s.io/kubernetes/vendor/google.golang.org/grpc.(*Server).serveStreams.func1\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal kubelet[4502]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/google.golang.org/grpc/server.go:587 +0xa1\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal systemd[1]: kubelet.service: main process exited, code=exited, status=2/INVALIDARGUMENT\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal systemd[1]: Unit kubelet.service entered failed state.\r\nMay 20 12:31:32 ip-192-168-218-117.us-east-2.compute.internal systemd[1]: kubelet.service failed.\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal systemd[1]: kubelet.service holdoff time over, scheduling restart.\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal systemd[1]: Starting Kubernetes Kubelet...\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal systemd[1]: Started Kubernetes Kubelet.\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: Flag --allow-privileged has been deprecated, will be removed in a future version\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: Flag --allow-privileged has been deprecated, will be removed in a future version\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.420154    5313 server.go:408] Version: v1.12.7\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.420392    5313 aws.go:1042] Building AWS cloudprovider\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.420430    5313 aws.go:1004] Zone not specified in configuration file; querying AWS metadata service\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.589946    5313 tags.go:76] AWS cloud filtering on ClusterID: nsm-2-5dfee610-c9bc-4d74-8a87-5b7d9aee96e8\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.614723    5313 container_manager_linux.go:247] container manager verified user specified cgroup-root exists: []\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.614752    5313 container_manager_linux.go:252] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.15} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms}\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.614907    5313 container_manager_linux.go:271] Creating device plugin manager: true\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.614942    5313 state_mem.go:36] [cpumanager] initializing new in-memory state store\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.615196    5313 state_mem.go:84] [cpumanager] updated default cpuset: \"\"\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.615210    5313 state_mem.go:92] [cpumanager] updated cpuset assignments: \"map[]\"\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.615310    5313 kubelet.go:304] Watching apiserver\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.625382    5313 client.go:75] Connecting to docker on unix:///var/run/docker.sock\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.625642    5313 client.go:104] Start docker client with request timeout=2m0s\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.626964    5313 docker_service.go:236] Hairpin mode set to \"hairpin-veth\"\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: W0520 12:31:37.627048    5313 cni.go:188] Unable to update cni config: No networks found in /etc/cni/net.d\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: W0520 12:31:37.633306    5313 cni.go:188] Unable to update cni config: No networks found in /etc/cni/net.d\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.633350    5313 docker_service.go:251] Docker cri networking managed by cni\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.642545    5313 docker_service.go:256] Docker Info: &{ID:W4N2:NU5N:KLEG:7ETR:A46B:GNY7:5GRW:QI62:L3GV:2VNH:7QC5:DDLF Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem xfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:19 OomKillDisable:true NGoroutines:44 SystemTime:2019-05-20T12:31:37.634601955Z LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:4.14.106-97.85.amzn2.x86_64 OperatingSystem:Amazon Linux 2 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc4208778f0 NCPU:2 MemTotal:4134801408 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ip-192-168-218-117.us-east-2.compute.internal Labels:[] ExperimentalBuild:false ServerVersion:18.06.1-ce ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[]}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil>} LiveRestoreEnabled:true Isolation: InitBinary:docker-init ContainerdCommit:{ID:468a545b9edcd5932818eb9de8e72413e616e86e Expected:468a545b9edcd5932818eb9de8e72413e616e86e} RuncCommit:{ID:69663f0bd4b60df09991c08812a60108003fa340 Expected:69663f0bd4b60df09991c08812a60108003fa340} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default]}\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.644317    5313 docker_service.go:269] Setting cgroupDriver to cgroupfs\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.671959    5313 kuberuntime_manager.go:197] Container runtime docker initialized, version: 18.06.1-ce, apiVersion: 1.38.0\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.687810    5313 server.go:1013] Started kubelet\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:31:37.688436    5313 kubelet.go:1287] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data in memory cache\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.689005    5313 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.689188    5313 status_manager.go:152] Starting to sync pod status with apiserver\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.689365    5313 kubelet.go:1804] Starting kubelet main sync loop.\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.689539    5313 kubelet.go:1821] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.689794    5313 server.go:133] Starting to listen on 0.0.0.0:10250\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.690477    5313 server.go:318] Adding debug handlers to kubelet server.\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.691643    5313 volume_manager.go:248] Starting Kubelet Volume Manager\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.693024    5313 desired_state_of_world_populator.go:130] Desired state populator starts to run\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: W0520 12:31:37.695579    5313 cni.go:188] Unable to update cni config: No networks found in /etc/cni/net.d\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:31:37.695938    5313 kubelet.go:2167] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.789844    5313 kubelet.go:1821] skipping pod synchronization - [container runtime is down]\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.792038    5313 kubelet_node_status.go:277] Setting node annotation to enable volume controller attach/detach\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.792223    5313 kubelet_node_status.go:325] Adding node label from cloud provider: beta.kubernetes.io/instance-type=t2.medium\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.792385    5313 kubelet_node_status.go:336] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/zone=us-east-2c\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.792539    5313 kubelet_node_status.go:340] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/region=us-east-2\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.794599    5313 kubelet_node_status.go:70] Attempting to register node ip-192-168-218-117.us-east-2.compute.internal\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.814206    5313 kubelet_node_status.go:112] Node ip-192-168-218-117.us-east-2.compute.internal was previously registered\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.814511    5313 kubelet_node_status.go:73] Successfully registered node ip-192-168-218-117.us-east-2.compute.internal\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.818344    5313 cpu_manager.go:155] [cpumanager] starting with none policy\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.818365    5313 cpu_manager.go:156] [cpumanager] reconciling every 10s\r\nMay 20 12:31:37 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:37.818374    5313 policy_none.go:42] [cpumanager] none policy: Start\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094390    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/32a64068-7afb-11e9-93a9-0236cdd357f4-xtables-lock\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094439    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/configmap/32a64068-7afb-11e9-93a9-0236cdd357f4-kubeconfig\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094464    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-bin-dir\" (UniqueName: \"kubernetes.io/host-path/32a6b763-7afb-11e9-93a9-0236cdd357f4-cni-bin-dir\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094485    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"aws-node-token-b4qfw\" (UniqueName: \"kubernetes.io/secret/32a6b763-7afb-11e9-93a9-0236cdd357f4-aws-node-token-b4qfw\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094507    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"log-dir\" (UniqueName: \"kubernetes.io/host-path/32a6b763-7afb-11e9-93a9-0236cdd357f4-log-dir\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094527    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"dockersock\" (UniqueName: \"kubernetes.io/host-path/32a6b763-7afb-11e9-93a9-0236cdd357f4-dockersock\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094548    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"varlog\" (UniqueName: \"kubernetes.io/host-path/32a64068-7afb-11e9-93a9-0236cdd357f4-varlog\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094575    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/32a64068-7afb-11e9-93a9-0236cdd357f4-lib-modules\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094614    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"config\" (UniqueName: \"kubernetes.io/configmap/32a64068-7afb-11e9-93a9-0236cdd357f4-config\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094637    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy-token-kqppp\" (UniqueName: \"kubernetes.io/secret/32a64068-7afb-11e9-93a9-0236cdd357f4-kube-proxy-token-kqppp\") pod \"kube-proxy-8bpkd\" (UID: \"32a64068-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094659    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-net-dir\" (UniqueName: \"kubernetes.io/host-path/32a6b763-7afb-11e9-93a9-0236cdd357f4-cni-net-dir\") pod \"aws-node-6s2vz\" (UID: \"32a6b763-7afb-11e9-93a9-0236cdd357f4\")\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:31:38.094667    5313 reconciler.go:154] Reconciler: start to sync state\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: W0520 12:31:38.843819    5313 pod_container_deletor.go:75] Container \"34307d7425424de847ac4530c62f43d6a1a521d9bd12324856c81623ecc1e199\" not found in pod's containers\r\nMay 20 12:31:38 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: W0520 12:31:38.851829    5313 pod_container_deletor.go:75] Container \"965576ff642a7d10aa8ec29fff4055dfea8716a5c2e348c80020bd849dbc4104\" not found in pod's containers\r\nMay 20 12:31:42 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: W0520 12:31:42.820333    5313 cni.go:188] Unable to update cni config: No networks found in /etc/cni/net.d\r\nMay 20 12:31:42 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:31:42.820441    5313 kubelet.go:2167] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\r\nMay 20 12:31:47 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: W0520 12:31:47.821337    5313 cni.go:188] Unable to update cni config: No networks found in /etc/cni/net.d\r\nMay 20 12:31:47 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:31:47.821463    5313 kubelet.go:2167] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\r\nMay 20 12:32:59 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:32:59.080148    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"nsm-socket\" (UniqueName: \"kubernetes.io/host-path/66b36f5b-7afb-11e9-86df-06fb8ce3114c-nsm-socket\") pod \"nsmgr-ip-192-168-218-117.us-east-2.compute.internal\" (UID: \"66b36f5b-7afb-11e9-86df-06fb8ce3114c\")\r\nMay 20 12:32:59 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:32:59.080202    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"kubelet-socket\" (UniqueName: \"kubernetes.io/host-path/66b36f5b-7afb-11e9-86df-06fb8ce3114c-kubelet-socket\") pod \"nsmgr-ip-192-168-218-117.us-east-2.compute.internal\" (UID: \"66b36f5b-7afb-11e9-86df-06fb8ce3114c\")\r\nMay 20 12:32:59 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:32:59.080229    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"default-token-j9bnj\" (UniqueName: \"kubernetes.io/secret/66b36f5b-7afb-11e9-86df-06fb8ce3114c-default-token-j9bnj\") pod \"nsmgr-ip-192-168-218-117.us-east-2.compute.internal\" (UID: \"66b36f5b-7afb-11e9-86df-06fb8ce3114c\")\r\nMay 20 12:32:59 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:32:59.381920    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"default-token-j9bnj\" (UniqueName: \"kubernetes.io/secret/66d0a62b-7afb-11e9-86df-06fb8ce3114c-default-token-j9bnj\") pod \"nsmd-dataplane-ip-192-168-218-117.us-east-2.compute.internal\" (UID: \"66d0a62b-7afb-11e9-86df-06fb8ce3114c\")\r\nMay 20 12:32:59 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:32:59.381981    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"workspace\" (UniqueName: \"kubernetes.io/host-path/66d0a62b-7afb-11e9-86df-06fb8ce3114c-workspace\") pod \"nsmd-dataplane-ip-192-168-218-117.us-east-2.compute.internal\" (UID: \"66d0a62b-7afb-11e9-86df-06fb8ce3114c\")\r\nMay 20 12:32:59 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:32:59.382007    5313 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"postmortem\" (UniqueName: \"kubernetes.io/host-path/66d0a62b-7afb-11e9-86df-06fb8ce3114c-postmortem\") pod \"nsmd-dataplane-ip-192-168-218-117.us-east-2.compute.internal\" (UID: \"66d0a62b-7afb-11e9-86df-06fb8ce3114c\")\r\nMay 20 12:32:59 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: W0520 12:32:59.770575    5313 container.go:393] Failed to create summary reader for \"/system.slice/run-6208.scope\": none of the resources are being tracked.\r\nMay 20 12:33:02 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:02.200981    5313 remote_runtime.go:282] ContainerStatus \"1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\" from runtime service failed: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:02 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:02.201029    5313 kuberuntime_container.go:393] ContainerStatus for 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2 error: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:02 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:02.201040    5313 kuberuntime_manager.go:866] getPodContainerStatuses for pod \"nsmgr-ip-192-168-218-117.us-east-2.compute.internal_default(66b36f5b-7afb-11e9-86df-06fb8ce3114c)\" failed: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:02 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:02.201061    5313 generic.go:241] PLEG: Ignoring events for pod nsmgr-ip-192-168-218-117.us-east-2.compute.internal/default: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:03 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:03.214332    5313 remote_runtime.go:282] ContainerStatus \"1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\" from runtime service failed: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:03 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:03.214379    5313 kuberuntime_container.go:393] ContainerStatus for 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2 error: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:03 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:03.214390    5313 kuberuntime_manager.go:866] getPodContainerStatuses for pod \"nsmgr-ip-192-168-218-117.us-east-2.compute.internal_default(66b36f5b-7afb-11e9-86df-06fb8ce3114c)\" failed: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:03 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:03.214406    5313 generic.go:241] PLEG: Ignoring events for pod nsmgr-ip-192-168-218-117.us-east-2.compute.internal/default: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:03 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:03.220665    5313 remote_runtime.go:282] ContainerStatus \"1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\" from runtime service failed: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:03 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:03.220697    5313 kuberuntime_container.go:393] ContainerStatus for 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2 error: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:03 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:03.220707    5313 kuberuntime_manager.go:866] getPodContainerStatuses for pod \"nsmgr-ip-192-168-218-117.us-east-2.compute.internal_default(66b36f5b-7afb-11e9-86df-06fb8ce3114c)\" failed: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:03 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: E0520 12:33:03.220722    5313 generic.go:271] PLEG: pod nsmgr-ip-192-168-218-117.us-east-2.compute.internal/default failed reinspection: rpc error: code = Unknown desc = Error: No such container: 1db0f731319658bd639de5ce5564dce7a32b9bb8e6f1619513943ce9a48762c2\r\nMay 20 12:33:07 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:07.523839    5313 manager.go:354] Got registration request from device plugin with resource name \"networkservicemesh.io/socket\"\r\nMay 20 12:33:12 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:12.991614    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60420: no serving certificate available for the kubelet\r\nMay 20 12:33:13 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:13.191395    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60422: no serving certificate available for the kubelet\r\nMay 20 12:33:13 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:13.391201    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60424: no serving certificate available for the kubelet\r\nMay 20 12:33:13 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:13.597877    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60426: no serving certificate available for the kubelet\r\nMay 20 12:33:13 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:13.790964    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60428: no serving certificate available for the kubelet\r\nMay 20 12:33:13 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:13.991113    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60430: no serving certificate available for the kubelet\r\nMay 20 12:33:14 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:14.192078    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60432: no serving certificate available for the kubelet\r\nMay 20 12:33:14 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:14.390984    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60434: no serving certificate available for the kubelet\r\nMay 20 12:33:14 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:14.591258    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60436: no serving certificate available for the kubelet\r\nMay 20 12:33:14 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:14.791443    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60438: no serving certificate available for the kubelet\r\nMay 20 12:33:14 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:14.991598    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60440: no serving certificate available for the kubelet\r\nMay 20 12:33:15 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:15.191116    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60442: no serving certificate available for the kubelet\r\nMay 20 12:33:15 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:15.391084    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60444: no serving certificate available for the kubelet\r\nMay 20 12:33:15 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:15.591251    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60448: no serving certificate available for the kubelet\r\nMay 20 12:33:15 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:15.792734    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60450: no serving certificate available for the kubelet\r\nMay 20 12:33:15 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:15.990937    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60452: no serving certificate available for the kubelet\r\nMay 20 12:33:16 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:16.192745    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60456: no serving certificate available for the kubelet\r\nMay 20 12:33:16 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:16.391381    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60460: no serving certificate available for the kubelet\r\nMay 20 12:33:16 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:16.591547    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60462: no serving certificate available for the kubelet\r\nMay 20 12:33:16 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:16.791108    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60464: no serving certificate available for the kubelet\r\nMay 20 12:33:16 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:16.991139    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60466: no serving certificate available for the kubelet\r\nMay 20 12:33:17 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:17.190634    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60470: no serving certificate available for the kubelet\r\nMay 20 12:33:17 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:17.391573    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60472: no serving certificate available for the kubelet\r\nMay 20 12:33:17 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:17.590878    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60474: no serving certificate available for the kubelet\r\nMay 20 12:33:17 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:17.793777    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60476: no serving certificate available for the kubelet\r\nMay 20 12:33:17 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:17.990990    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60478: no serving certificate available for the kubelet\r\nMay 20 12:33:18 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:18.191262    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60480: no serving certificate available for the kubelet\r\nMay 20 12:33:18 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:18.392826    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60482: no serving certificate available for the kubelet\r\nMay 20 12:33:18 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:18.590773    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60484: no serving certificate available for the kubelet\r\nMay 20 12:33:18 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:18.791506    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60486: no serving certificate available for the kubelet\r\nMay 20 12:33:18 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:18.990875    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60488: no serving certificate available for the kubelet\r\nMay 20 12:33:19 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:19.191045    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60490: no serving certificate available for the kubelet\r\nMay 20 12:33:19 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:19.392484    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60492: no serving certificate available for the kubelet\r\nMay 20 12:33:19 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:19.591082    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60494: no serving certificate available for the kubelet\r\nMay 20 12:33:19 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:19.791205    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60496: no serving certificate available for the kubelet\r\nMay 20 12:33:19 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:19.992502    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60512: no serving certificate available for the kubelet\r\nMay 20 12:33:20 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:20.191150    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60514: no serving certificate available for the kubelet\r\nMay 20 12:33:20 ip-192-168-218-117.us-east-2.compute.internal kubelet[5313]: I0520 12:33:20.391271    5313 log.go:172] http: TLS handshake error from 192.168.138.59:60516: no serving certificate available for the kubelet\r\n```\r\n\r\nFull kubelet logs: \r\n[aws_kubelet_logs.txt](https://github.com/kubernetes/kubernetes/files/3202191/aws_kubelet_logs.txt)\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): 1.12.7\r\n- Cloud provider or hardware configuration: AWS EKS\r\n- OS (e.g: `cat /etc/os-release`): Amazon Linux 2\r\n- Kernel (e.g. `uname -a`):\r\n- Install tools:\r\n- Network plugin and version (if this is a network-related bug):\r\n- Others:\r\n",
  "closed_at": "2019-10-24T18:21:19Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 18,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/78164/comments",
  "created_at": "2019-05-21T11:14:00Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/78164/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/78164",
  "id": 446559649,
  "labels": [
    {
      "color": "0052cc",
      "default": false,
      "description": "Issues or PRs related to aws provider",
      "id": 852130657,
      "name": "area/provider/aws",
      "node_id": "MDU6TGFiZWw4NTIxMzA2NTc=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/area/provider/aws"
    },
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Cloud Provider.",
      "id": 958178286,
      "name": "sig/cloud-provider",
      "node_id": "MDU6TGFiZWw5NTgxNzgyODY=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/cloud-provider"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/78164/labels{/name}",
  "locked": false,
  "milestone": {
    "closed_at": null,
    "closed_issues": 1674,
    "created_at": "2019-01-21T20:12:05Z",
    "creator": {
      "avatar_url": "https://avatars0.githubusercontent.com/u/980082?v=4",
      "events_url": "https://api.github.com/users/liggitt/events{/privacy}",
      "followers_url": "https://api.github.com/users/liggitt/followers",
      "following_url": "https://api.github.com/users/liggitt/following{/other_user}",
      "gists_url": "https://api.github.com/users/liggitt/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/liggitt",
      "id": 980082,
      "login": "liggitt",
      "node_id": "MDQ6VXNlcjk4MDA4Mg==",
      "organizations_url": "https://api.github.com/users/liggitt/orgs",
      "received_events_url": "https://api.github.com/users/liggitt/received_events",
      "repos_url": "https://api.github.com/users/liggitt/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/liggitt/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/liggitt/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/liggitt"
    },
    "description": null,
    "due_on": null,
    "html_url": "https://github.com/kubernetes/kubernetes/milestone/43",
    "id": 3990944,
    "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/milestones/43/labels",
    "node_id": "MDk6TWlsZXN0b25lMzk5MDk0NA==",
    "number": 43,
    "open_issues": 17,
    "state": "open",
    "title": "v1.17",
    "updated_at": "2020-10-27T18:45:51Z",
    "url": "https://api.github.com/repos/kubernetes/kubernetes/milestones/43"
  },
  "node_id": "MDU6SXNzdWU0NDY1NTk2NDk=",
  "number": 78164,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Error on get logs from pod in AWS. SIGSEGV in kubelet logs",
  "updated_at": "2019-10-31T17:35:08Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/78164",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/12110261?v=4",
    "events_url": "https://api.github.com/users/lobkovilya/events{/privacy}",
    "followers_url": "https://api.github.com/users/lobkovilya/followers",
    "following_url": "https://api.github.com/users/lobkovilya/following{/other_user}",
    "gists_url": "https://api.github.com/users/lobkovilya/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/lobkovilya",
    "id": 12110261,
    "login": "lobkovilya",
    "node_id": "MDQ6VXNlcjEyMTEwMjYx",
    "organizations_url": "https://api.github.com/users/lobkovilya/orgs",
    "received_events_url": "https://api.github.com/users/lobkovilya/received_events",
    "repos_url": "https://api.github.com/users/lobkovilya/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/lobkovilya/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/lobkovilya/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/lobkovilya"
  }
}