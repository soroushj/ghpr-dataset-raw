{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "<!-- This form is for bug reports and feature requests ONLY!\r\n\r\nIf you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/.\r\n-->\r\n\r\n**Is this a BUG REPORT or FEATURE REQUEST?**:\r\n\r\n> Uncomment only one, leave it on its own line:\r\n>\r\n\r\n/kind bug\r\n\r\n> /kind feature\r\n\r\n\r\n**What happened**:\r\n\r\nwhen i run hollow-node deployment\uff0chollow-proxy will cause panic.\r\n\r\nhollow-node deployment is as follows:\r\n\r\n```\r\napiVersion: extensions/v1beta1\r\nkind: Deployment\r\nmetadata:\r\n  annotations:\r\n    deployment.kubernetes.io/revision: \"10\"\r\n  creationTimestamp: 2018-10-12T09:42:16Z\r\n  generation: 10\r\n  labels:\r\n    name: hollow-node\r\n  name: hollow-node\r\n  namespace: kubemark1\r\n  resourceVersion: \"14267172\"\r\n  selfLink: /apis/extensions/v1beta1/namespaces/kubemark1/deployments/hollow-node\r\n  uid: 1b11d8b2-ce03-11e8-95f8-525400c802e4\r\nspec:\r\n  progressDeadlineSeconds: 600\r\n  replicas: 1\r\n  revisionHistoryLimit: 10\r\n  selector:\r\n    matchLabels:\r\n      name: hollow-node\r\n  strategy:\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 1\r\n    type: RollingUpdate\r\n  template:\r\n    metadata:\r\n      creationTimestamp: null\r\n      labels:\r\n        name: hollow-node\r\n    spec:\r\n      containers:\r\n      - command:\r\n        - /bin/sh\r\n        - -c\r\n        - /kubemark --morph=kubelet --name=$(NODE_NAME) --v=4 --kubeconfig=/etc/kubernetes/kubectl.kubeconfig\r\n          --alsologtostderr 1>>/var/log/kubelet-$(NODE_NAME).log 2>&1\r\n        env:\r\n        - name: NODE_NAME\r\n          valueFrom:\r\n            fieldRef:\r\n              apiVersion: v1\r\n              fieldPath: metadata.name\r\n        image: kubemark:v1\r\n        imagePullPolicy: IfNotPresent\r\n        name: hollow-kubelet\r\n        ports:\r\n        - containerPort: 4194\r\n          protocol: TCP\r\n        - containerPort: 10250\r\n          protocol: TCP\r\n        - containerPort: 10255\r\n          protocol: TCP\r\n        resources:\r\n          requests:\r\n            cpu: 60m\r\n            memory: 100M\r\n        securityContext:\r\n          privileged: true\r\n        terminationMessagePath: /dev/termination-log\r\n        terminationMessagePolicy: File\r\n        volumeMounts:\r\n        - mountPath: /etc/kubernetes\r\n          name: kubeconfig-volume\r\n          readOnly: true\r\n        - mountPath: /var/log\r\n          name: logs-volume\r\n      - command:\r\n        - /bin/sh\r\n        - -c\r\n        - /kubemark --morph=proxy --use-real-proxier=true --name=$(NODE_NAME) --v=4\r\n          --kubeconfig=/etc/kubernetes/kubectl.kubeconfig --alsologtostderr 1>>/var/log/kubeproxy-$(NODE_NAME).log\r\n          2>&1\r\n        env:\r\n        - name: NODE_NAME\r\n          valueFrom:\r\n            fieldRef:\r\n              apiVersion: v1\r\n              fieldPath: metadata.name\r\n        image: kubemark:v1\r\n        imagePullPolicy: IfNotPresent\r\n        name: hollow-proxy\r\n        resources:\r\n          requests:\r\n            cpu: 30m\r\n            memory: 20M\r\n        securityContext:\r\n          privileged: true\r\n        terminationMessagePath: /dev/termination-log\r\n        terminationMessagePolicy: File\r\n        volumeMounts:\r\n        - mountPath: /etc/kubernetes\r\n          name: kubeconfig-volume\r\n          readOnly: true\r\n        - mountPath: /var/log\r\n          name: logs-volume\r\n      dnsPolicy: ClusterFirst\r\n      initContainers:\r\n      - command:\r\n        - sysctl\r\n        - -w\r\n        - fs.inotify.max_user_instances=200\r\n        image: busybox\r\n        imagePullPolicy: IfNotPresent\r\n        name: init-inotify-limit\r\n        resources: {}\r\n        securityContext:\r\n          privileged: true\r\n        terminationMessagePath: /dev/termination-log\r\n        terminationMessagePolicy: File\r\n      nodeSelector:\r\n        node: py_node\r\n      restartPolicy: Always\r\n      schedulerName: default-scheduler\r\n      securityContext: {}\r\n      terminationGracePeriodSeconds: 30\r\n      volumes:\r\n      - hostPath:\r\n          path: /var/log\r\n          type: \"\"\r\n        name: logs-volume\r\n      - hostPath:\r\n          path: /etc/kubernetes\r\n          type: Directory\r\n        name: kubeconfig-volume\r\nstatus:\r\n  availableReplicas: 1\r\n  conditions:\r\n  - lastTransitionTime: 2018-10-12T09:42:16Z\r\n    lastUpdateTime: 2018-10-12T09:42:16Z\r\n    message: Deployment has minimum availability.\r\n    reason: MinimumReplicasAvailable\r\n    status: \"True\"\r\n    type: Available\r\n  - lastTransitionTime: 2018-10-12T09:42:16Z\r\n    lastUpdateTime: 2018-10-12T13:13:05Z\r\n    message: ReplicaSet \"hollow-node-648bf9bb4\" has successfully progressed.\r\n    reason: NewReplicaSetAvailable\r\n    status: \"True\"\r\n    type: Progressing\r\n  observedGeneration: 10\r\n  readyReplicas: 1\r\n  replicas: 1\r\n  updatedReplicas: 1\r\n```\r\n\r\nhollow-proxy container logs:\r\n\r\nI1012 12:37:13.681139       7 proxier.go:638] syncProxyRules took 622.278\u00b5s\r\nI1012 12:37:13.681156       7 asm_amd64.s:573] Shutting down service config controller\r\nE1012 12:37:13.681249       7 runtime.go:66] Observed a panic: \"invalid memory address or nil pointer dereference\" (runtime error: invalid memory address or nil pointer dereference)\r\n/root/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:72\r\n/root/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:65\r\n/root/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:51\r\n/usr/local/go/src/runtime/asm_amd64.s:573\r\n/usr/local/go/src/runtime/panic.go:502\r\n/usr/local/go/src/runtime/panic.go:63\r\n/usr/local/go/src/runtime/signal_unix.go:388\r\n/root/go/src/k8s.io/kubernetes/vendor/k8s.io/utils/exec/testing/fake_exec.go:52\r\n/root/go/src/k8s.io/kubernetes/pkg/util/conntrack/conntrack.go:61\r\n/root/go/src/k8s.io/kubernetes/pkg/util/conntrack/conntrack.go:49\r\n/root/go/src/k8s.io/kubernetes/pkg/proxy/iptables/proxier.go:1367\r\n/root/go/src/k8s.io/kubernetes/pkg/proxy/iptables/proxier.go:532\r\n/root/go/src/k8s.io/kubernetes/pkg/proxy/config/config.go:211\r\n/usr/local/go/src/runtime/asm_amd64.s:2361\r\npanic: runtime error: invalid memory address or nil pointer dereference [recovered]\r\n\tpanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x2c68ad9]\r\n\r\ngoroutine 74 [running]:\r\nk8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)\r\n\t/root/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58 +0x107\r\npanic(0x32dc9e0, 0x5c20d20)\r\n\t/usr/local/go/src/runtime/panic.go:502 +0x229\r\nk8s.io/kubernetes/vendor/k8s.io/utils/exec/testing.(*FakeExec).LookPath(0xc420ad4450, 0x3ac6dba, 0x9, 0xc4208c51d0, 0x0, 0xc4200a4800, 0x4e7e83)\r\n\t/root/go/src/k8s.io/kubernetes/vendor/k8s.io/utils/exec/testing/fake_exec.go:52 +0x39\r\nk8s.io/kubernetes/pkg/util/conntrack.Exec(0x3e3c000, 0xc420ad4450, 0xc4208c51d0, 0x5, 0x5, 0x5, 0x5)\r\n\t/root/go/src/k8s.io/kubernetes/pkg/util/conntrack/conntrack.go:61 +0x5e\r\nk8s.io/kubernetes/pkg/util/conntrack.ClearEntriesForIP(0x3e3c000, 0xc420ad4450, 0xc420768c70, 0xc, 0x3abdd6d, 0x3, 0x0, 0x0)\r\n\t/root/go/src/k8s.io/kubernetes/pkg/util/conntrack/conntrack.go:49 +0x17d\r\nk8s.io/kubernetes/pkg/proxy/iptables.(*Proxier).syncProxyRules(0xc420d8fe40)\r\n\t/root/go/src/k8s.io/kubernetes/pkg/proxy/iptables/proxier.go:1367 +0xb643\r\nk8s.io/kubernetes/pkg/proxy/iptables.(*Proxier).OnServiceSynced(0xc420d8fe40)\r\n\t/root/go/src/k8s.io/kubernetes/pkg/proxy/iptables/proxier.go:532 +0x62\r\nk8s.io/kubernetes/pkg/proxy/config.(*ServiceConfig).Run(0xc420ad5080, 0xc4200b62a0)\r\n\t/root/go/src/k8s.io/kubernetes/pkg/proxy/config/config.go:211 +0x183\r\ncreated by k8s.io/kubernetes/cmd/kube-proxy/app.(*ProxyServer).Run\r\n\t/root/go/src/k8s.io/kubernetes/cmd/kube-proxy/app/server.go:555 +0x283\r\n\r\n**What you expected to happen**:\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nboot up hollow-proxy container with \"--use-real-proxier=true\"\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n- Cloud provider or hardware configuration:\r\n- OS (e.g. from /etc/os-release):\r\n- Kernel (e.g. `uname -a`):\r\n- Install tools:\r\n- Others:\r\n\r\nI found that in \"kubemark/hollow-node.go\" we init a 'FakeExec' struct, but we don't init LookPathFunc in 'FakeExec' struct.\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/7f23a743e8c23ac6489340bbb34fa6f1d392db9d/cmd/kubemark/hollow-node.go#L187\r\n\r\n```\r\n\tif config.Morph == \"proxy\" {\r\n\t\t...\r\n\t\tsysctl := fakesysctl.NewFake()\r\n\t\texecer := &fakeexec.FakeExec{}\r\n\t\teventBroadcaster := record.NewBroadcaster()\r\n\t\t...\r\n\t\thollowProxy.Run()\r\n\t}\r\n```\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/master/vendor/k8s.io/utils/exec/testing/fake_exec.go#L28\r\n\r\n```\r\n// A simple scripted Interface type.\r\ntype FakeExec struct {\r\n\tCommandScript []FakeCommandAction\r\n\tCommandCalls  int\r\n\tLookPathFunc  func(string) (string, error)\r\n}\r\n```\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/master/vendor/k8s.io/utils/exec/testing/fake_exec.go#L51\r\n\r\n```\r\nfunc (fake *FakeExec) LookPath(file string) (string, error) {\r\n\treturn fake.LookPathFunc(file)\r\n}\r\n```\r\n\r\nSo,  execer.LookPathFunc is a vild pointer, so it cause panic.\r\n\r\n\r\n[kubeproxy-hollow-node-85c74bd457-ctszb.log](https://github.com/kubernetes/kubernetes/files/2473437/kubeproxy-hollow-node-85c74bd457-ctszb.log)\r\n\r\n",
  "closed_at": "2019-04-23T07:46:11Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 7,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/69735/comments",
  "created_at": "2018-10-12T14:23:36Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/69735/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/69735",
  "id": 369573954,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Scalability.",
      "id": 125010198,
      "name": "sig/scalability",
      "node_id": "MDU6TGFiZWwxMjUwMTAxOTg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/scalability"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Testing.",
      "id": 483069764,
      "name": "sig/testing",
      "node_id": "MDU6TGFiZWw0ODMwNjk3NjQ=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/testing"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/69735/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUzNjk1NzM5NTQ=",
  "number": 69735,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "kubemark\uff1a--use-real-proxier=true will cause panic runtime error",
  "updated_at": "2019-04-23T07:46:11Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/69735",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/17756696?v=4",
    "events_url": "https://api.github.com/users/keontang/events{/privacy}",
    "followers_url": "https://api.github.com/users/keontang/followers",
    "following_url": "https://api.github.com/users/keontang/following{/other_user}",
    "gists_url": "https://api.github.com/users/keontang/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/keontang",
    "id": 17756696,
    "login": "keontang",
    "node_id": "MDQ6VXNlcjE3NzU2Njk2",
    "organizations_url": "https://api.github.com/users/keontang/orgs",
    "received_events_url": "https://api.github.com/users/keontang/received_events",
    "repos_url": "https://api.github.com/users/keontang/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/keontang/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/keontang/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/keontang"
  }
}