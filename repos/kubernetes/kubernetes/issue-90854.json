{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/12699319?v=4",
    "events_url": "https://api.github.com/users/andrewsykim/events{/privacy}",
    "followers_url": "https://api.github.com/users/andrewsykim/followers",
    "following_url": "https://api.github.com/users/andrewsykim/following{/other_user}",
    "gists_url": "https://api.github.com/users/andrewsykim/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/andrewsykim",
    "id": 12699319,
    "login": "andrewsykim",
    "node_id": "MDQ6VXNlcjEyNjk5MzE5",
    "organizations_url": "https://api.github.com/users/andrewsykim/orgs",
    "received_events_url": "https://api.github.com/users/andrewsykim/received_events",
    "repos_url": "https://api.github.com/users/andrewsykim/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/andrewsykim/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/andrewsykim/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/andrewsykim"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars0.githubusercontent.com/u/12699319?v=4",
      "events_url": "https://api.github.com/users/andrewsykim/events{/privacy}",
      "followers_url": "https://api.github.com/users/andrewsykim/followers",
      "following_url": "https://api.github.com/users/andrewsykim/following{/other_user}",
      "gists_url": "https://api.github.com/users/andrewsykim/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/andrewsykim",
      "id": 12699319,
      "login": "andrewsykim",
      "node_id": "MDQ6VXNlcjEyNjk5MzE5",
      "organizations_url": "https://api.github.com/users/andrewsykim/orgs",
      "received_events_url": "https://api.github.com/users/andrewsykim/received_events",
      "repos_url": "https://api.github.com/users/andrewsykim/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/andrewsykim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/andrewsykim/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/andrewsykim"
    }
  ],
  "author_association": "NONE",
  "body": "**What happened**:\r\n\r\nWe run `kube-proxy` in `ipvs` mode on `RHEL 8.1`. Upgrading from kubernetes `1.15.3` to `1.18.1` introduces an additional 1 second of latency when making an HTTP call from `host -> service IP -> pod`, when the pod is running on a different host than the host where the call is made. **No** additional latency is observed when making the same HTTP call from `host -> pod` OR from `pod -> service IP -> pod` OR when the pod is running on the same host where the call is being made.\r\n\r\nThis 1 second of additional latency is consistently seen on every new connection and was traced to the first SYN packet being dropped and being retransmitted. Furthermore, this appears to be related to the `--random-fully` flag introduced by [this PR](https://github.com/kubernetes/kubernetes/pull/78547). Removing the flag gets rid of the additional latency.\r\n\r\n**What you expected to happen**:\r\n\r\nNo additional latency when calling `host -> service IP -> pod`\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nOn a multiple host kubernetes cluster, run nginx and expose it as a service:\r\n\r\n```sh\r\n[cloud-user@kube-worker-1 ~]$ kubectl run --image=nginx nginx-app --port=80\r\npod/nginx-app created\r\n[cloud-user@kube-worker-1 ~]$ kubectl expose pod nginx-app --port=80 --name=nginx-http\r\nservice/nginx-http exposed\r\n[cloud-user@kube-worker-1 ~]$ kubectl get pods --all-namespaces -o wide\r\nNAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES\r\ndefault       nginx-app                             1/1     Running   0          23s   198.18.102.10   kube-worker-2   <none>           <none>\r\nkube-system   kube-state-metrics-6c574db878-ss5vt   1/1     Running   0          20d   198.18.77.2     kube-worker-3   <none>           <none>\r\n[cloud-user@kube-worker-1 ~]$ kubectl get svc --all-namespaces\r\nNAMESPACE     NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\r\ndefault       kubernetes           ClusterIP   198.19.0.1     <none>        443/TCP             21m\r\ndefault       nginx-http           ClusterIP   198.19.61.33   <none>        80/TCP              25s\r\nkube-system   kube-state-metrics   ClusterIP   None           <none>        8080/TCP,8081/TCP   20d\r\n[cloud-user@kube-worker-1 ~]$ kubectl get endpoints --all-namespaces\r\nNAMESPACE     NAME                      ENDPOINTS                                          AGE\r\ndefault       kubernetes                10.34.2.112:6443,10.34.6.23:6443,10.34.8.81:6443   21m\r\ndefault       nginx-http                198.18.102.10:80                                   33s\r\nkube-system   kube-controller-manager   <none>                                             20d\r\nkube-system   kube-scheduler            <none>                                             20d\r\nkube-system   kube-state-metrics        198.18.77.2:8081,198.18.77.2:8080                  20d\r\n```\r\n\r\nTrying to curl from `host -> service -> pod`\r\n\r\n```sh\r\n[cloud-user@kube-worker-1 ~]$ cat > time-format.txt <<EOF\r\n>     time_namelookup:  %{time_namelookup}s\\n\r\n>        time_connect:  %{time_connect}s\\n\r\n>     time_appconnect:  %{time_appconnect}s\\n\r\n>    time_pretransfer:  %{time_pretransfer}s\\n\r\n>       time_redirect:  %{time_redirect}s\\n\r\n>  time_starttransfer:  %{time_starttransfer}s\\n\r\n>                     ----------\\n\r\n>          time_total:  %{time_total}s\\n\r\n> EOF\r\n[cloud-user@kube-worker-1 ~]$ curl -I -w @time-format.txt 198.19.61.33\r\nHTTP/1.1 200 OK\r\nServer: nginx/1.17.10\r\nDate: Wed, 06 May 2020 22:32:45 GMT\r\nContent-Type: text/html\r\nContent-Length: 612\r\nLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\r\nConnection: keep-alive\r\nETag: \"5e95c66e-264\"\r\nAccept-Ranges: bytes\r\n\r\n    time_namelookup:  0.000024s\r\n       time_connect:  1.010933s\r\n    time_appconnect:  0.000000s\r\n   time_pretransfer:  1.010992s\r\n      time_redirect:  0.000000s\r\n time_starttransfer:  1.011581s\r\n                    ----------\r\n         time_total:  1.011689s\r\n[cloud-user@kube-worker-1 ~]$\r\n```\r\n\r\nNotice that the connect took `1.010933s`\r\n\r\nRunning tcpdump shows the retransmitted SYN packet after 1 second:\r\n\r\n```sh\r\n$ sudo tcpdump -i flannel.1 -nn\r\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\r\nlistening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes\r\n19:31:37.438631 IP 198.18.103.0.13758 > 198.18.102.10.80: Flags [S], seq 265989161, win 43690, options [mss 65495,sackOK,TS val 2631200673 ecr 0,nop,wscale 7], length 0\r\n19:31:38.493883 IP 198.18.103.0.13758 > 198.18.102.10.80: Flags [S], seq 265989161, win 43690, options [mss 65495,sackOK,TS val 2631201728 ecr 0,nop,wscale 7], length 0\r\n19:31:38.494723 IP 198.18.102.10.80 > 198.18.103.0.13758: Flags [S.], seq 222967005, ack 265989162, win 27960, options [mss 1410,sackOK,TS val 2955153471 ecr 2631201728,nop,wscale 7], length 0\r\n19:31:38.494798 IP 198.18.103.0.13758 > 198.18.102.10.80: Flags [.], ack 1, win 342, options [nop,nop,TS val 2631201729 ecr 2955153471], length 0\r\n19:31:38.494953 IP 198.18.103.0.13758 > 198.18.102.10.80: Flags [P.], seq 1:78, ack 1, win 342, options [nop,nop,TS val 2631201729 ecr 2955153471], length 77: HTTP: HEAD / HTTP/1.1\r\n19:31:38.495124 IP 198.18.102.10.80 > 198.18.103.0.13758: Flags [.], ack 78, win 219, options [nop,nop,TS val 2955153471 ecr 2631201729], length 0\r\n19:31:38.495216 IP 198.18.102.10.80 > 198.18.103.0.13758: Flags [P.], seq 1:240, ack 78, win 219, options [nop,nop,TS val 2955153472 ecr 2631201729], length 239: HTTP: HTTP/1.1 200 OK\r\n19:31:38.495230 IP 198.18.103.0.13758 > 198.18.102.10.80: Flags [.], ack 240, win 350, options [nop,nop,TS val 2631201730 ecr 2955153472], length 0\r\n19:31:38.495392 IP 198.18.103.0.13758 > 198.18.102.10.80: Flags [F.], seq 78, ack 240, win 350, options [nop,nop,TS val 2631201730 ecr 2955153472], length 0\r\n19:31:38.495579 IP 198.18.102.10.80 > 198.18.103.0.13758: Flags [F.], seq 240, ack 79, win 219, options [nop,nop,TS val 2955153472 ecr 2631201730], length 0\r\n19:31:38.495599 IP 198.18.103.0.13758 > 198.18.102.10.80: Flags [.], ack 241, win 350, options [nop,nop,TS val 2631201730 ecr 2955153472], length 0\r\n^C\r\n11 packets captured\r\n11 packets received by filter\r\n0 packets dropped by kernel\r\n```\r\n\r\nSimilarly [tcpretrans](https://github.com/iovisor/bcc/blob/master/tools/tcpretrans.py) shows the retransmission.\r\n\r\n```sh\r\n$ sudo ./tcpretrans\r\nTracing retransmits ... Hit Ctrl-C to end\r\nTIME     PID    IP LADDR:LPORT          T> RADDR:RPORT          STATE\r\n19:34:12 0      4  198.19.61.33:43826   R> 198.19.61.33:80      SYN_SENT\r\n```\r\n\r\nHowever, as best as I can tell, neither [tcpdrop](https://github.com/iovisor/bcc/blob/master/tools/tcpdrop.py) nor [dropwatch](https://github.com/nhorman/dropwatch) seem to show the kernel dropping the SYN packet. Conntrack does not show any insert-failed entries:\r\n\r\n```\r\n$ sudo conntrack -S\r\ncpu=0           found=325 invalid=19385 ignore=11274976 insert=0 insert_failed=0 drop=0 early_drop=0 error=0 search_restart=310036\r\ncpu=1           found=203 invalid=1364578 ignore=10818506 insert=0 insert_failed=0 drop=0 early_drop=0 error=0 search_restart=513826\r\n```\r\n\r\n<details>\r\n    <summary>The iptables rules</summary>\r\n\r\n```\r\n[cloud-user@kube-worker-1 ~]$ sudo iptables-save\r\n# Generated by xtables-save v1.8.2 on Wed May  6 20:05:43 2020\r\n*filter\r\n:INPUT ACCEPT [0:0]\r\n:FORWARD ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:KUBE-FIREWALL - [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\n:KUBE-FORWARD - [0:0]\r\n-A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD\r\n-A FORWARD -s 198.18.0.0/16 -j ACCEPT\r\n-A FORWARD -d 198.18.0.0/16 -j ACCEPT\r\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000/0x4000 -j ACCEPT\r\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod source rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\r\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod destination rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\r\nCOMMIT\r\n# Completed on Wed May  6 20:05:43 2020\r\n# Generated by xtables-save v1.8.2 on Wed May  6 20:05:43 2020\r\n*nat\r\n:PREROUTING ACCEPT [0:0]\r\n:INPUT ACCEPT [0:0]\r\n:POSTROUTING ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:KUBE-MARK-DROP - [0:0]\r\n:KUBE-MARK-MASQ - [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\n:KUBE-SERVICES - [0:0]\r\n:KUBE-POSTROUTING - [0:0]\r\n:KUBE-FIREWALL - [0:0]\r\n:KUBE-NODE-PORT - [0:0]\r\n:KUBE-LOAD-BALANCER - [0:0]\r\n-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING\r\n-A POSTROUTING ! -d 198.18.0.0/15 -j MASQUERADE\r\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000\r\n-A KUBE-SERVICES -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP src,dst -j KUBE-MARK-MASQ\r\n-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT\r\n-A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT\r\n-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE --random-fully\r\n-A KUBE-FIREWALL -j KUBE-MARK-DROP\r\n-A KUBE-LOAD-BALANCER -j KUBE-MARK-MASQ\r\nCOMMIT\r\n# Completed on Wed May  6 20:05:43 2020\r\n# Generated by xtables-save v1.8.2 on Wed May  6 20:05:43 2020\r\n*mangle\r\n:PREROUTING ACCEPT [0:0]\r\n:INPUT ACCEPT [0:0]\r\n:FORWARD ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:POSTROUTING ACCEPT [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\nCOMMIT\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n    <summary>ipvs rules</summary>\r\n\r\n```sh\r\n[cloud-user@kube-worker-1 ~]$ sudo ipvsadm -Ln\r\nIP Virtual Server version 1.2.1 (size=4096)\r\nProt LocalAddress:Port Scheduler Flags\r\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\r\nTCP  198.19.0.1:443 rr\r\n  -> 10.34.2.112:6443             Masq    1      0          0\r\n  -> 10.34.6.23:6443              Masq    1      0          0\r\n  -> 10.34.8.81:6443              Masq    1      0          0\r\nTCP  198.19.61.33:80 rr\r\n  -> 198.18.102.10:80             Masq    1      0          0\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n    <summary>ipset rules</summary>\r\n\r\n```sh\r\nName: KUBE-LOAD-BALANCER-SOURCE-CIDR\r\nType: hash:ip,port,net\r\nRevision: 7\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 384\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-NODE-PORT-LOCAL-TCP\r\nType: bitmap:port\r\nRevision: 3\r\nHeader: range 0-65535\r\nSize in memory: 8300\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\nName: KUBE-NODE-PORT-SCTP-HASH\r\nType: hash:ip,port\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 120\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-NODE-PORT-LOCAL-SCTP-HASH\r\nType: hash:ip,port\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 120\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-LOAD-BALANCER-FW\r\nType: hash:ip,port\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 120\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-LOAD-BALANCER-SOURCE-IP\r\nType: hash:ip,port,ip\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 128\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-LOAD-BALANCER\r\nType: hash:ip,port\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 120\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-LOOP-BACK\r\nType: hash:ip,port,ip\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 128\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-EXTERNAL-IP-LOCAL\r\nType: hash:ip,port\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 120\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-EXTERNAL-IP\r\nType: hash:ip,port\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 120\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-NODE-PORT-LOCAL-UDP\r\nType: bitmap:port\r\nRevision: 3\r\nHeader: range 0-65535\r\nSize in memory: 8300\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-NODE-PORT-TCP\r\nType: bitmap:port\r\nRevision: 3\r\nHeader: range 0-65535\r\nSize in memory: 8300\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-NODE-PORT-UDP\r\nType: bitmap:port\r\nRevision: 3\r\nHeader: range 0-65535\r\nSize in memory: 8300\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n\r\nName: KUBE-CLUSTER-IP\r\nType: hash:ip,port\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 248\r\nReferences: 2\r\nNumber of entries: 2\r\nMembers:\r\n198.19.0.1,tcp:443\r\n198.19.61.33,tcp:80\r\n\r\nName: KUBE-LOAD-BALANCER-LOCAL\r\nType: hash:ip,port\r\nRevision: 5\r\nHeader: family inet hashsize 1024 maxelem 65536\r\nSize in memory: 120\r\nReferences: 0\r\nNumber of entries: 0\r\nMembers:\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n    <summary>The interfaces/addresses</summary>\r\n\r\n```sh\r\n[cloud-user@kube-worker-1 ~]$ ip addr show\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\r\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r\n    inet 127.0.0.1/8 scope host lo\r\n       valid_lft forever preferred_lft forever\r\n    inet6 ::1/128 scope host\r\n       valid_lft forever preferred_lft forever\r\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\r\n    link/ether fa:16:3e:ff:df:46 brd ff:ff:ff:ff:ff:ff\r\n    inet 10.34.8.132/18 brd 10.34.63.255 scope global dynamic eth0\r\n       valid_lft 48497sec preferred_lft 48497sec\r\n    inet6 fe80::f816:3eff:feff:df46/64 scope link\r\n       valid_lft forever preferred_lft forever\r\n6: cni0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\r\n    link/ether 9a:0d:e0:ee:97:85 brd ff:ff:ff:ff:ff:ff\r\n    inet 198.18.103.1/24 brd 198.18.103.255 scope global cni0\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::980d:e0ff:feee:9785/64 scope link\r\n       valid_lft forever preferred_lft forever\r\n14: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default\r\n    link/ether ca:63:57:12:28:7e brd ff:ff:ff:ff:ff:ff\r\n    inet 198.19.0.1/32 brd 198.19.0.1 scope global kube-ipvs0\r\n       valid_lft forever preferred_lft forever\r\n    inet 198.19.61.33/32 brd 198.19.61.33 scope global kube-ipvs0\r\n       valid_lft forever preferred_lft forever\r\n15: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default\r\n    link/ether 7e:77:8a:72:91:29 brd ff:ff:ff:ff:ff:ff\r\n    inet 198.18.103.0/32 scope global flannel.1\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::7c77:8aff:fe72:9129/64 scope link\r\n       valid_lft forever preferred_lft forever\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n    <summary>The routing table</summary>\r\n\r\n```sh\r\n[cloud-user@kube-worker-1 ~]$ ip route list\r\ndefault via 10.34.0.1 dev eth0\r\n10.34.0.0/18 dev eth0 proto kernel scope link src 10.34.8.132\r\n198.18.77.0/24 via 198.18.77.0 dev flannel.1 onlink\r\n198.18.87.0/24 via 198.18.87.0 dev flannel.1 onlink\r\n198.18.89.0/24 via 198.18.89.0 dev flannel.1 onlink\r\n198.18.102.0/24 via 198.18.102.0 dev flannel.1 onlink\r\n198.18.103.0/24 dev cni0 proto kernel scope link src 198.18.103.1 linkdown\r\n[cloud-user@kube-worker-1 ~]$ ip route list table local\r\nbroadcast 10.34.0.0 dev eth0 proto kernel scope link src 10.34.8.132\r\nlocal 10.34.8.132 dev eth0 proto kernel scope host src 10.34.8.132\r\nbroadcast 10.34.63.255 dev eth0 proto kernel scope link src 10.34.8.132\r\nbroadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1\r\nlocal 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1\r\nlocal 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1\r\nbroadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1\r\nbroadcast 198.18.103.0 dev cni0 proto kernel scope link src 198.18.103.1 linkdown\r\nlocal 198.18.103.0 dev flannel.1 proto kernel scope host src 198.18.103.0\r\nlocal 198.18.103.1 dev cni0 proto kernel scope host src 198.18.103.1\r\nbroadcast 198.18.103.255 dev cni0 proto kernel scope link src 198.18.103.1 linkdown\r\nlocal 198.19.0.1 dev kube-ipvs0 proto kernel scope host src 198.19.0.1\r\nlocal 198.19.61.33 dev kube-ipvs0 proto kernel scope host src 198.19.61.33\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n    <summary>kube-proxy config</summary>\r\n\r\n```yaml\r\napiVersion: kubeproxy.config.k8s.io/v1alpha1\r\nclientConnection:\r\n  kubeconfig: \"/kubernetes/conf/kube-proxy-kube-config.yml\"\r\nhealthzBindAddress: 0.0.0.0:10256\r\niptables:\r\n  masqueradeAll: false\r\nipvs:\r\n  minSyncPeriod: 0s\r\n  scheduler: \"\"\r\n  syncPeriod: 30s\r\nkind: KubeProxyConfiguration\r\nmetricsBindAddress: 0.0.0.0:10249\r\nmode: ipvs\r\n```\r\n\r\n</details>\r\n\r\n\r\nWith an identical cluster running kubernetes 1.15.3, we were *NOT* able to reproduce the 1 second latency. We saw the following differences in the created iptables rules between the two versions `diff k8s-1.15.3-iptables.txt k8s-1.18.1-iptables.txt`:\r\n\r\n```diff\r\n1c1\r\n< # Generated by xtables-save v1.8.2 on Wed May  6 21:23:40 2020\r\n---\r\n> # Generated by xtables-save v1.8.2 on Wed May  6 20:05:43 2020\r\n6a7\r\n> :KUBE-KUBELET-CANARY - [0:0]\r\n8d8\r\n< -A INPUT -j KUBE-FIREWALL\r\n12,13d11\r\n< -A OUTPUT -j KUBE-FIREWALL\r\n< -A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP\r\n14a13,14\r\n> -A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod source rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\r\n> -A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod destination rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\r\n16,17c16,17\r\n< # Completed on Wed May  6 21:23:40 2020\r\n< # Generated by xtables-save v1.8.2 on Wed May  6 21:23:40 2020\r\n---\r\n> # Completed on Wed May  6 20:05:43 2020\r\n> # Generated by xtables-save v1.8.2 on Wed May  6 20:05:43 2020\r\n25c25\r\n< :KUBE-POSTROUTING - [0:0]\r\n---\r\n> :KUBE-KUBELET-CANARY - [0:0]\r\n26a27\r\n> :KUBE-POSTROUTING - [0:0]\r\n34d34\r\n< -A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000\r\n36d35\r\n< -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE\r\n39a39\r\n> -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE --random-fully\r\n43c43,52\r\n< # Completed on Wed May  6 21:23:40 2020\r\n---\r\n> # Completed on Wed May  6 20:05:43 2020\r\n> # Generated by xtables-save v1.8.2 on Wed May  6 20:05:43 2020\r\n> *mangle\r\n> :PREROUTING ACCEPT [0:0]\r\n> :INPUT ACCEPT [0:0]\r\n> :FORWARD ACCEPT [0:0]\r\n> :OUTPUT ACCEPT [0:0]\r\n> :POSTROUTING ACCEPT [0:0]\r\n> :KUBE-KUBELET-CANARY - [0:0]\r\n> COMMIT\r\n```\r\n\r\nWe tried systematically changing the iptables rules for our 1.18.1 cluster to match those in 1.15.3 and noticed that the 1 second latency/retransmission disappeared when we removed the `--random-fully` flag:\r\n\r\n```sh\r\n[cloud-user@kube-worker-1 ~]$ sudo systemctl stop kube-proxy\r\n[cloud-user@kube-worker-1 ~]$ sudo iptables -t nat -D KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE --random-fully\r\n[cloud-user@kube-worker-1 ~]$ sudo iptables -t nat -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE\r\n[cloud-user@kube-worker-1 ~]$ curl -I -w @time-format.txt 198.19.61.33\r\nHTTP/1.1 200 OK\r\nServer: nginx/1.17.10\r\nDate: Thu, 07 May 2020 01:39:31 GMT\r\nContent-Type: text/html\r\nContent-Length: 612\r\nLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\r\nConnection: keep-alive\r\nETag: \"5e95c66e-264\"\r\nAccept-Ranges: bytes\r\n\r\n    time_namelookup:  0.000044s\r\n       time_connect:  0.001296s\r\n    time_appconnect:  0.000000s\r\n   time_pretransfer:  0.001338s\r\n      time_redirect:  0.000000s\r\n time_starttransfer:  0.002616s\r\n                    ----------\r\n         time_total:  0.002700s\r\n```\r\n\r\nStarting `kube-proxy` again reset the `--random-fully` and reintroduced this bug. The `--random-fully` change was made in this PR: https://github.com/kubernetes/kubernetes/pull/78547\r\n\r\n**Anything else we need to know?**:\r\n\r\nThis behavior is not observed in HTTP calls that go from `host -> pod` OR `pod -> service IP -> pod`. See examples below (Click on the arrow to expand).\r\n\r\n<details>\r\n <summary>Trying to curl `host -> pod`</summary>\r\n\r\n  ```sh\r\n[cloud-user@kube-worker-1 ~]$ curl -I -w @time-format.txt 198.18.102.10\r\nHTTP/1.1 200 OK\r\nServer: nginx/1.17.10\r\nDate: Wed, 06 May 2020 22:37:00 GMT\r\nContent-Type: text/html\r\nContent-Length: 612\r\nLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\r\nConnection: keep-alive\r\nETag: \"5e95c66e-264\"\r\nAccept-Ranges: bytes\r\n\r\n    time_namelookup:  0.000024s\r\n       time_connect:  0.000877s\r\n    time_appconnect:  0.000000s\r\n   time_pretransfer:  0.000904s\r\n      time_redirect:  0.000000s\r\n time_starttransfer:  0.001457s\r\n                    ----------\r\n         time_total:  0.001506s\r\n  ```\r\n\r\n</details>\r\n\r\nNotice that the connect took `0.000877s`\r\n\r\n<details>\r\n    <summary>Trying `pod -> service IP -> pod`</summary>\r\n\r\n  ```sh\r\n[cloud-user@kube-worker-1 ~]$ kubectl run --image=curlimages/curl curl --attach -it --overrides='{ \"apiVersion\": \"v1\", \"spec\": { \"nodeName\": \"kube-worker-1\" } }' -- /bin/sh\r\nIf you don't see a command prompt, try pressing enter.\r\n/ $ cd\r\n~ $ cat > time-format.txt <<EOF\r\n>     time_namelookup:  %{time_namelookup}s\\n\r\n>        time_connect:  %{time_connect}s\\n\r\n>     time_appconnect:  %{time_appconnect}s\\n\r\n>    time_pretransfer:  %{time_pretransfer}s\\n\r\n>       time_redirect:  %{time_redirect}s\\n\r\n>  time_starttransfer:  %{time_starttransfer}s\\n\r\n>                     ----------\\n\r\n>          time_total:  %{time_total}s\\n\r\n> EOF\r\n~ $ curl -I -w @time-format.txt 198.19.61.33\r\nHTTP/1.1 200 OK\r\nServer: nginx/1.17.10\r\nDate: Wed, 06 May 2020 23:16:20 GMT\r\nContent-Type: text/html\r\nContent-Length: 612\r\nLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\r\nConnection: keep-alive\r\nETag: \"5e95c66e-264\"\r\nAccept-Ranges: bytes\r\n\r\n    time_namelookup:  0.000051s\r\n       time_connect:  0.001467s\r\n    time_appconnect:  0.000000s\r\n   time_pretransfer:  0.001523s\r\n      time_redirect:  0.000000s\r\n time_starttransfer:  0.003195s\r\n                    ----------\r\n         time_total:  0.003426s\r\n~ $\r\n  ```\r\n\r\n</details>\r\n\r\nNotice that the connect took `0.001467s`\r\n\r\n*We were able to reproduce the exact same issue with kube-proxy running in iptables mode. Again removing the `--random-fully` flag got rid of the extra 1 second latency.*\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n\r\n```\r\n[cloud-user@kube-worker-1 ~]$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.1\", GitCommit:\"7879fc12a63337efff607952a323df90cdc7a335\", GitTreeState:\"clean\", BuildDate:\"2020-04-08T17:38:50Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.1\", GitCommit:\"7879fc12a63337efff607952a323df90cdc7a335\", GitTreeState:\"clean\", BuildDate:\"2020-04-08T17:30:47Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n- Cloud provider or hardware configuration: `openstack rocky`\r\n\r\n- OS (e.g: `cat /etc/os-release`):\r\n\r\n```sh\r\n[cloud-user@kube-worker-1 ~]$ cat /etc/os-release\r\nNAME=\"Red Hat Enterprise Linux\"\r\nVERSION=\"8.1 (Ootpa)\"\r\nID=\"rhel\"\r\nID_LIKE=\"fedora\"\r\nVERSION_ID=\"8.1\"\r\nPLATFORM_ID=\"platform:el8\"\r\nPRETTY_NAME=\"Red Hat Enterprise Linux 8.1 (Ootpa)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.1:GA\"\r\nHOME_URL=\"https://www.redhat.com/\"\r\nBUG_REPORT_URL=\"https://bugzilla.redhat.com/\"\r\n\r\nREDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=8.1\r\nREDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"8.1\"\r\n```\r\n\r\n- Kernel (e.g. `uname -a`):\r\n\r\n```\r\n[cloud-user@kube-worker-1 ~]$ uname -a\r\nLinux kube-worker-1 4.18.0-147.el8.x86_64 #1 SMP Thu Sep 26 15:52:44 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n- Install tools: None\r\n- Network plugin and version (if this is a network-related bug):\r\n\r\n```\r\n[cloud-user@kube-worker-1 ~]$ iptables --version\r\niptables v1.8.2 (nf_tables)\r\n[cloud-user@kube-worker-1 ~]$ /usr/local/bin/flanneld --version\r\nv0.11.0\r\n[cloud-user@kube-worker-1 ~]$ /kubernetes/cni/bin/flannel --version\r\nCNI flannel plugin v0.8.2\r\n[cloud-user@kube-worker-1 ~]$ cat /kubernetes/cni/conf/10-flanneld.conf\r\n{\r\n    \"name\": \"cni0\",\r\n    \"cniVersion\": \"0.3.1\",\r\n    \"type\": \"flannel\",\r\n    \"delegate\": {\r\n        \"hairpinMode\": true,\r\n        \"isDefaultGateway\": true\r\n    }\r\n}\r\n```\r\n\r\n- Others:\r\n",
  "closed_at": "2020-06-16T07:50:05Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 14,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/90854/comments",
  "created_at": "2020-05-07T18:14:34Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/90854/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/90854",
  "id": 614249583,
  "labels": [
    {
      "color": "0052cc",
      "default": false,
      "description": null,
      "id": 755527763,
      "name": "area/ipvs",
      "node_id": "MDU6TGFiZWw3NTU1Mjc3NjM=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/area/ipvs"
    },
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Network.",
      "id": 116712108,
      "name": "sig/network",
      "node_id": "MDU6TGFiZWwxMTY3MTIxMDg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/network"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/90854/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU2MTQyNDk1ODM=",
  "number": 90854,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Additional 1s latency in `host -> service IP -> pod` when upgrading from `1.15.3 -> 1.18.1` on RHEL 8.1",
  "updated_at": "2020-07-20T18:13:06Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/90854",
  "user": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/297368?v=4",
    "events_url": "https://api.github.com/users/skamboj/events{/privacy}",
    "followers_url": "https://api.github.com/users/skamboj/followers",
    "following_url": "https://api.github.com/users/skamboj/following{/other_user}",
    "gists_url": "https://api.github.com/users/skamboj/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/skamboj",
    "id": 297368,
    "login": "skamboj",
    "node_id": "MDQ6VXNlcjI5NzM2OA==",
    "organizations_url": "https://api.github.com/users/skamboj/orgs",
    "received_events_url": "https://api.github.com/users/skamboj/received_events",
    "repos_url": "https://api.github.com/users/skamboj/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/skamboj/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/skamboj/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/skamboj"
  }
}