{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "MEMBER",
  "body": "<!-- This form is for bug reports and feature requests ONLY!\r\n\r\nIf you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).\r\n\r\nIf this may be security issue, please disclose it privately via https://kubernetes.io/security/.\r\n-->\r\n\r\n**Is this a BUG REPORT or FEATURE REQUEST?**:\r\n\r\n> Uncomment only one, leave it on its own line:\r\n>\r\n\r\n/kind bug\r\n> /kind feature\r\n\r\n\r\n**What happened**:\r\n\r\nIn 1.11, https://github.com/kubernetes/kubernetes/pull/61877 removed externalID from Node.Spec which used to be used by kubelet to detect node recreation in certain environments during node upgrade. This changed node upgrade behavior in those environments because in pkg/kubelet/kubelet_node_status.go tryRegisterWithAPIServer(node *v1.Node), without externalID checking, we now consider the recreated node after upgrade as a previously registered node and thus will get old node state from API server instead of regenerating them based on node local state.\r\n\r\nFor extended resource reported by a device plugin, this causes problem because after node upgrade/recreation, the device plugin DaemonSet pod needs to be restarted by Kubelet, finishes certain setup, and then report to kubelet that its resource is available at the node. However, the node status capacity and allocatable from the API server still contains the old state before the node upgrade. After kubelet syncs up with the API server, the node status field gets overwritten and previously reported extended resources will appear on the node status capacity/allocatable even though the node is not ready for pods to consume such resources. As the result, Kubelet may start a pod requesting such resource without proper container runtime setup it needs to learn from device plugin.\r\n\r\nNote to cope with the externalID removal change, cluster/gce/upgrade.sh has been updated (https://github.com/kubernetes/kubernetes/issues/63506) to explicitly uncordon the node after checking it is restarted and becomes ready. This however doesn't help device plugin case because node readiness doesn't depend on individual extended resource readiness.\r\n\r\nThis issue is to explore how we can fix this problem on head and 1.11.\r\n\r\nIn particular, I wonder whether we may switch to the model that we consider Kubelet as the only source to update extended resource capacity/allocatable in node status. I.e., don't support manually updating node status capacity/allocatable fields as documented in https://kubernetes.io/docs/tasks/administer-cluster/extended-resource-node/. This would allow us to re-generate node status capacity/allocatable every time in node status update, which I think is much simpler and more robust. As I heard, some folks have been using this mechanism to do simple resource exporting/accounting through a central controller. I wonder whether people are open to switch to the device plugin model in those cases. Even though those extended resources may not require special container setup, it seems a more secure model to have Kubelet totally own its node status.\r\n\r\nFYI, here is the related OSS issue: https://github.com/kubernetes/kubernetes/issues/50473 where we first introduced extended resource concept.\r\n\r\n\r\n**What you expected to happen**:\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nRunning the gpu upgrade test from https://github.com/kubernetes/kubernetes/pull/63631 with --upgrade-target set to any 1.11+ version would hit the issue.\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n- Cloud provider or hardware configuration:\r\n- OS (e.g. from /etc/os-release):\r\n- Kernel (e.g. `uname -a`):\r\n- Install tools:\r\n- Others:\r\n",
  "closed_at": "2018-06-06T08:24:21Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/13653959?v=4",
    "events_url": "https://api.github.com/users/k8s-github-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-github-robot/followers",
    "following_url": "https://api.github.com/users/k8s-github-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-github-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-github-robot",
    "id": 13653959,
    "login": "k8s-github-robot",
    "node_id": "MDQ6VXNlcjEzNjUzOTU5",
    "organizations_url": "https://api.github.com/users/k8s-github-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-github-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-github-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-github-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-github-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-github-robot"
  },
  "comments": 33,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/64632/comments",
  "created_at": "2018-06-01T20:29:34Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/64632/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/64632",
  "id": 328651032,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Auth.",
      "id": 357119284,
      "name": "sig/auth",
      "node_id": "MDU6TGFiZWwzNTcxMTkyODQ=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/auth"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Cluster Lifecycle.",
      "id": 173494222,
      "name": "sig/cluster-lifecycle",
      "node_id": "MDU6TGFiZWwxNzM0OTQyMjI=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/cluster-lifecycle"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Node.",
      "id": 173493665,
      "name": "sig/node",
      "node_id": "MDU6TGFiZWwxNzM0OTM2NjU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/node"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/64632/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUzMjg2NTEwMzI=",
  "number": 64632,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "From 1.11, an extended resource reported by a device plugin can be left on a node after node upgrade even though its device plugin never registers back",
  "updated_at": "2019-04-02T02:48:32Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/64632",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/29497748?v=4",
    "events_url": "https://api.github.com/users/jiayingz/events{/privacy}",
    "followers_url": "https://api.github.com/users/jiayingz/followers",
    "following_url": "https://api.github.com/users/jiayingz/following{/other_user}",
    "gists_url": "https://api.github.com/users/jiayingz/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/jiayingz",
    "id": 29497748,
    "login": "jiayingz",
    "node_id": "MDQ6VXNlcjI5NDk3NzQ4",
    "organizations_url": "https://api.github.com/users/jiayingz/orgs",
    "received_events_url": "https://api.github.com/users/jiayingz/received_events",
    "repos_url": "https://api.github.com/users/jiayingz/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/jiayingz/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/jiayingz/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/jiayingz"
  }
}