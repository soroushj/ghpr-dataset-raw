{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "<!-- This form is for bug reports and feature requests ONLY!\r\n\r\nIf you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).\r\n\r\nIf this may be security issue, please disclose it privately via https://kubernetes.io/security/.\r\n-->\r\n\r\n**Is this a BUG REPORT or FEATURE REQUEST?**:\r\n\r\n> /kind bug\r\n\r\n\r\n\r\n**What happened**:\r\n\r\nFor these 2 test suites visible in the testgrid at network-gce :\r\nhttp://k8s-testgrid.appspot.com/sig-network-gce#gce-coredns-performance\r\nhttp://k8s-testgrid.appspot.com/sig-network-gce#gce-kubedns-performance\r\n\r\nthe testsuite cannot setup correctly the cluster, due to a firewall issue (I think)\r\nSee part of the build log below:\r\n\r\n\r\n```\r\nI0605 12:39:36.975] NAME                                        STATUS                     ROLES     AGE       VERSION\r\nI0605 12:39:36.976] gce-coredns-performance-master              Ready,SchedulingDisabled   <none>    27s       v1.12.0-alpha.0.838+def5179c661df6\r\nI0605 12:39:36.976] gce-coredns-performance-minion-group-377q   Ready                      <none>    25s       v1.12.0-alpha.0.838+def5179c661df6\r\nI0605 12:39:36.976] gce-coredns-performance-minion-group-6w50   Ready                      <none>    25s       v1.12.0-alpha.0.838+def5179c661df6\r\nI0605 12:39:36.976] gce-coredns-performance-minion-group-vf6h   Ready                      <none>    25s       v1.12.0-alpha.0.838+def5179c661df6\r\nI0605 12:39:37.308] Validate output:\r\nI0605 12:39:37.645] NAME                 STATUS    MESSAGE              ERROR\r\nI0605 12:39:37.646] etcd-1               Healthy   {\"health\": \"true\"}   \r\nI0605 12:39:37.646] controller-manager   Healthy   ok                   \r\nI0605 12:39:37.646] scheduler            Healthy   ok                   \r\nI0605 12:39:37.647] etcd-0               Healthy   {\"health\": \"true\"}   \r\nI0605 12:39:37.652] Cluster validation succeeded\r\nW0605 12:39:37.752] Done, listing cluster services:\r\nW0605 12:39:37.752] \r\nI0605 12:39:37.853] Kubernetes master is running at https://35.231.139.160\r\nI0605 12:39:37.955] GLBCDefaultBackend is running at https://35.231.139.160/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\r\nI0605 12:39:37.955] Heapster is running at https://35.231.139.160/api/v1/namespaces/kube-system/services/heapster/proxy\r\nI0605 12:39:37.956] CoreDNS is running at https://35.231.139.160/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\r\nI0605 12:39:37.956] kubernetes-dashboard is running at https://35.231.139.160/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\r\nI0605 12:39:37.956] Metrics-server is running at https://35.231.139.160/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\r\nI0605 12:39:37.956] \r\nI0605 12:39:37.957] To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\r\nI0605 12:39:37.958] \r\nI0605 12:40:00.832] NAME                                                             NETWORK                  DIRECTION  PRIORITY  ALLOW            DENY\r\nI0605 12:40:00.832] gce-coredns-performance-minion-gce-coredns-performance-http-alt  gce-coredns-performance  INGRESS    1000      tcp:80,tcp:8080\r\nI0605 12:40:01.785] allowed:\r\nI0605 12:40:01.785] - IPProtocol: tcp\r\nI0605 12:40:01.785]   ports:\r\nI0605 12:40:01.786]   - '80'\r\nI0605 12:40:01.786] - IPProtocol: tcp\r\nI0605 12:40:01.786]   ports:\r\nI0605 12:40:01.786]   - '8080'\r\nI0605 12:40:01.786] creationTimestamp: '2018-06-05T05:39:39.559-07:00'\r\nI0605 12:40:01.786] description: ''\r\nI0605 12:40:01.786] direction: INGRESS\r\nI0605 12:40:01.787] id: '4755345594321156708'\r\nI0605 12:40:01.787] kind: compute#firewall\r\nI0605 12:40:01.787] name: gce-coredns-performance-minion-gce-coredns-performance-http-alt\r\nI0605 12:40:01.787] network: https://www.googleapis.com/compute/v1/projects/k8s-jkns-e2e-gce-prl-flaky/global/networks/gce-coredns-performance\r\nI0605 12:40:01.787] priority: 1000\r\nI0605 12:40:01.787] selfLink: https://www.googleapis.com/compute/v1/projects/k8s-jkns-e2e-gce-prl-flaky/global/firewalls/gce-coredns-performance-minion-gce-coredns-performance-http-alt\r\nI0605 12:40:01.788] sourceRanges:\r\nI0605 12:40:01.788] - 0.0.0.0/0\r\nI0605 12:40:01.788] targetTags:\r\nI0605 12:40:01.788] - gce-coredns-performance-minion\r\n=======\r\nW0605 13:00:04.876] Failed to create firewall gce-coredns-performance-minion-gce-coredns-performance-nodeports in k8s-jkns-e2e-gce-prl-flaky\r\n=======\r\nW0605 13:00:04.878] 2018/06/05 13:00:04 process.go:152: Step './hack/e2e-internal/e2e-up.sh' finished in 26m29.118690007s\r\nW0605 13:00:04.879] 2018/06/05 13:00:04 e2e.go:523: Dumping logs from nodes to GCS directly at path: gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-coredns-performance/4/artifacts\r\nW0605 13:00:04.879] 2018/06/05 13:00:04 process.go:150: Running: ./cluster/log-dump/log-dump.sh /workspace/_artifacts gs://kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-coredns-performance/4/artifacts\r\nW0605 13:00:04.924] Trying to find master named 'gce-coredns-performance-master'\r\nW0605 13:00:04.924] Looking for address 'gce-coredns-performance-master-ip'\r\nI0605 13:00:05.025] Checking for custom logdump instances, if any\r\nI0605 13:00:05.025] Sourcing kube-util.sh\r\nI0605 13:00:05.025] Detecting project\r\nI0605 13:00:05.026] Project: k8s-jkns-e2e-gce-prl-flaky\r\nI0605 13:00:05.026] Network Project: k8s-jkns-e2e-gce-prl-flaky\r\nI0605 13:00:05.026] Zone: us-east1-b\r\nI0605 13:00:05.026] Dumping logs from master locally to '/workspace/_artifacts'\r\nW0605 13:00:05.833] Using master: gce-coredns-performance-master (external IP: 35.231.139.160)\r\nI0605 13:00:33.644] Changing logfiles to be world-readable for download\r\nI0605 13:00:36.930] Copying 'kube-apiserver kube-apiserver-audit kube-scheduler rescheduler kube-controller-manager etcd etcd-events glbc cluster-autoscaler kube-addon-manager fluentd startupscript' from gce-coredns-performance-master\r\nW0605 13:00:38.107] \r\nW0605 13:00:38.108] Specify --start=42264 in the next get-serial-port-output invocation to get only the new output starting from here.\r\nW0605 13:00:41.259] scp: /var/log/cluster-autoscaler.log*: No such file or directory\r\nW0605 13:00:41.333] scp: /var/log/fluentd.log*: No such file or directory\r\nW0605 13:00:41.333] scp: /var/log/startupscript.log*: No such file or directory\r\nW0605 13:00:41.337] ERROR: (gcloud.compute.scp) [/usr/bin/scp] exited with return code [1].\r\n```\r\n\r\n\r\n**What you expected to happen**:\r\nI would think that setup the cluster is part of the process of the test-infra job, not the e2e test itself.\r\n\r\nAny missing flag or option on the job definition (see below) ?\r\nNeed guidance on how to find the root cause of this error.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nJust look at the testgrid, it happen every 6 hours.\r\nhttp://k8s-testgrid.appspot.com/sig-network-gce#gce-coredns-performance\r\nhttp://k8s-testgrid.appspot.com/sig-network-gce#gce-kubedns-performance\r\n\r\n**Anything else we need to know?**:\r\n\r\njob definition is:\r\n\r\n```\r\n  \"ci-kubernetes-e2e-gce-coredns-performance\": {\r\n    \"args\": [\r\n      \"--cluster=gce-coredns-performance\",\r\n      \"--env=CLUSTER_DNS_CORE_DNS=true\",\r\n      \"--extract=ci/latest\",\r\n      \"--gcp-master-image=gci\",\r\n      \"--gcp-node-image=gci\",\r\n      \"--gcp-nodes=3\",\r\n      \"--gcp-zone=us-east1-b\",\r\n      \"--provider=gce\",\r\n      \"--test_args=--ginkgo.focus=\\\\[Feature:PerformanceDNS\\\\]\",\r\n      \"--timeout=60m\",\r\n      \"--use-logexporter\"\r\n    ],\r\n    \"scenario\": \"kubernetes_e2e\",\r\n    \"sigOwners\": [\r\n      \"sig-network\"\r\n    ]\r\n  },\r\n\r\n```\r\n\r\n\r\nNOTE: this job definition was copied from another one that seems working fine:\r\n\r\n```\r\n  \"ci-kubernetes-e2e-gci-gce-coredns\": {\r\n    \"args\": [\r\n      \"--check-leaked-resources\",\r\n      \"--cluster=\",\r\n      \"--env=CLUSTER_DNS_CORE_DNS=true\",\r\n      \"--extract=ci/latest\",\r\n      \"--gcp-zone=us-central1-f\",\r\n      \"--ginkgo-parallel=30\",\r\n      \"--provider=gce\",\r\n      \"--test_args=--ginkgo.skip=\\\\[Serial\\\\]|\\\\[Disruptive\\\\]|\\\\[Flaky\\\\]|\\\\[Feature:.+\\\\] --minStartupPods=8\",\r\n      \"--timeout=150m\"\r\n    ],\r\n    \"scenario\": \"kubernetes_e2e\",\r\n    \"sigOwners\": [\r\n      \"sig-network\"\r\n    ]\r\n  },\r\n```\r\n\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): current (branch master)\r\n- Cloud provider or hardware configuration: gce\r\n",
  "closed_at": "2018-07-02T23:52:09Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/13653959?v=4",
    "events_url": "https://api.github.com/users/k8s-github-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-github-robot/followers",
    "following_url": "https://api.github.com/users/k8s-github-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-github-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-github-robot",
    "id": 13653959,
    "login": "k8s-github-robot",
    "node_id": "MDQ6VXNlcjEzNjUzOTU5",
    "organizations_url": "https://api.github.com/users/k8s-github-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-github-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-github-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-github-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-github-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-github-robot"
  },
  "comments": 29,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/64774/comments",
  "created_at": "2018-06-05T15:21:45Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/64774/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/64774",
  "id": 329506631,
  "labels": [
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Network.",
      "id": 116712108,
      "name": "sig/network",
      "node_id": "MDU6TGFiZWwxMTY3MTIxMDg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/network"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Scalability.",
      "id": 125010198,
      "name": "sig/scalability",
      "node_id": "MDU6TGFiZWwxMjUwMTAxOTg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/scalability"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/64774/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUzMjk1MDY2MzE=",
  "number": 64774,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "New e2e testsuites for DNS scale fail to setup the cluster",
  "updated_at": "2018-07-02T23:52:09Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/64774",
  "user": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/19315664?v=4",
    "events_url": "https://api.github.com/users/fturib/events{/privacy}",
    "followers_url": "https://api.github.com/users/fturib/followers",
    "following_url": "https://api.github.com/users/fturib/following{/other_user}",
    "gists_url": "https://api.github.com/users/fturib/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/fturib",
    "id": 19315664,
    "login": "fturib",
    "node_id": "MDQ6VXNlcjE5MzE1NjY0",
    "organizations_url": "https://api.github.com/users/fturib/orgs",
    "received_events_url": "https://api.github.com/users/fturib/received_events",
    "repos_url": "https://api.github.com/users/fturib/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/fturib/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/fturib/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/fturib"
  }
}