{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**What happened**:\r\nA container which had exclusive CPUs assigned to it (and an entry in /var/lib/kubelet/cpu_manager_state specifying those CPUs) was killed.  When it came back, it was using the default cpuset, and its entry in /var/lib/kubelet/cpu_manager_state was gone.\r\n\r\n**What you expected to happen**:\r\nIt should have been given exclusive CPUs.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nCreate a pod/container with \"guaranteed\" QoS.  Determine the exclusive CPUs allocated to the container.  From the host, kill pid1 within the container.  (\"kill -9\" works.)  The container will restart within the existing pod, but it will be allocated cpus from the default cpuset (which will now include the CPUs that had been exclusively allocated to the container earlier).\r\n\r\n**Anything else we need to know?**:\r\n\r\nI added some instrumentation to kubelet.  What seems to be happening is this:\r\n\r\n1) The container is killed (or dies).  Kubelet then immediately creates a new container and calls PolicyStatic.Allocate() to allocate exclusive CPUs for it.  This updates the entry in /var/lib/kubelet/cpu_manager_state (and the in-memory cache) which is mapped by podUID and container name, such that it contains the cpuset for the new container.\r\n\r\n2) The InternalContainerLifecycle.PostStopContainer() code runs, which calls i.cpuManager.RemoveContainer(containerID).  This uses the containerID of the exited container to look up the podUID/container-name tuple, then deletes the cpuset entry from  /var/lib/kubelet/cpu_manager_state (and the in-memory cache).  But the entry that was deleted was actually the one for the *new* container.\r\n\r\n3) Kubelet calls the CPU manager's AddContainer() function, which calls m.state.GetCPUSetOrDefault(string(p.UID), c.Name) to get the cpuset based on the podUID and container name of the new container.  The cpuset entry was erroneously deleted, so this returns the default cpuset, and the container which *should* have gotten exclusive CPUs ends up running on the default cpuset.\r\n\r\nAn solution suggested by @klueska is to delay calling cpuManager.RemoveContainer() until the whole pod is deleted.  This would preserve the CPU allocations.\r\n\r\nAn alternate solution also suggested by @klueska is to avoid calling cpuManager.RemoveContainer() from the container lifecycle code, but instead call removeStaleState() in the CpuManager.Allocate() function.\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): 1.18.1\r\n- Cloud provider or hardware configuration:   x86, SMP\r\n- OS (e.g: `cat /etc/os-release`):  CentOS 7\r\n- Kernel (e.g. `uname -a`): 3.10.0-1062.1.2.rt56.1025.el7\r\n",
  "closed_at": "2020-04-27T20:40:06Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 5,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/90303/comments",
  "created_at": "2020-04-20T17:44:20Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/90303/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/90303",
  "id": 603416853,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Node.",
      "id": 173493665,
      "name": "sig/node",
      "node_id": "MDU6TGFiZWwxNzM0OTM2NjU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/node"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/90303/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU2MDM0MTY4NTM=",
  "number": 90303,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Container cpuset lost, apparently due to race between PostStopContainer() and new container creation",
  "updated_at": "2020-07-21T13:13:06Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/90303",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/4241755?v=4",
    "events_url": "https://api.github.com/users/cbf123/events{/privacy}",
    "followers_url": "https://api.github.com/users/cbf123/followers",
    "following_url": "https://api.github.com/users/cbf123/following{/other_user}",
    "gists_url": "https://api.github.com/users/cbf123/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/cbf123",
    "id": 4241755,
    "login": "cbf123",
    "node_id": "MDQ6VXNlcjQyNDE3NTU=",
    "organizations_url": "https://api.github.com/users/cbf123/orgs",
    "received_events_url": "https://api.github.com/users/cbf123/received_events",
    "repos_url": "https://api.github.com/users/cbf123/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/cbf123/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/cbf123/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/cbf123"
  }
}