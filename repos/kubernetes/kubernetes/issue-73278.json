{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "MEMBER",
  "body": "As part of kubernetes/enhancements#719 we encountered a potential bug.\r\n\r\nEnvironment:\r\n\r\nKubernetes: 1.10 and 1.11\r\nDocker version: from 17.03.1\r\n\r\nchecked on multiple cloudproviders.\r\n\r\nWhen a node restarts, and the kubelet cannot reach the apiserver, dockerd restarts all the containers in the node.\r\n\r\nthe following logs are emitted:\r\n\r\n```\r\nJan 09 22:24:39  kubelet[8557]: I0109 22:24:39.807617    8557 kubelet.go:1941] SyncLoop (housekeeping, skipped): sources aren't ready yet.\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.129785    8557 eviction_manager.go:229] eviction manager: synchronize housekeeping\r\nJan 09 22:24:40  kubelet[8557]: E0109 22:24:40.129827    8557 eviction_manager.go:246] eviction manager: failed to get get summary stats: failed to get node info: node \"\" not found\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.132131    8557 kubelet.go:2122] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.667002    8557 kubelet_node_status.go:271] Setting node annotation to enable volume controller attach/detach\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.697763    8557 kubelet_node_status.go:327] Adding node label from cloud provider: beta.kubernetes.io/instance-type=Standard_D3_v2\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.697785    8557 kubelet_node_status.go:338] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/zone=0\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.697792    8557 kubelet_node_status.go:342] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/region=northeurope\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.704122    8557 kubelet_node_status.go:488] Using Node Hostname from cloudprovider: \"\"\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705654    8557 kubelet_node_status.go:422] Recording NodeHasSufficientDisk event message for node \r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705694    8557 kubelet_node_status.go:422] Recording NodeHasSufficientMemory event message for node \r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705705    8557 kubelet_node_status.go:422] Recording NodeHasNoDiskPressure event message for node \r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705712    8557 kubelet_node_status.go:422] Recording NodeHasSufficientPID event message for node \r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705728    8557 kubelet_node_status.go:82] Attempting to register node \r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705739    8557 server.go:428] Event(v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"\", UID:\"\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'NodeHasSufficientDisk' Node  status is now: NodeHasSufficientDisk\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705761    8557 server.go:428] Event(v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"\", UID:\"\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'NodeHasSufficientMemory' Node  status is now: NodeHasSufficientMemory\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705773    8557 server.go:428] Event(v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"\", UID:\"\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'NodeHasNoDiskPressure' Node  status is now: NodeHasNoDiskPressure\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.705823    8557 server.go:428] Event(v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"\", UID:\"\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'NodeHasSufficientPID' Node  status is now: NodeHasSufficientPID\r\nJan 09 22:24:40  kubelet[8557]: E0109 22:24:40.707239    8557 kubelet_node_status.go:106] Unable to register node \"\" with API server: Post https://apiserver_ip/api/v1/nodes: dial tcp apiserver_ip: getsockopt: connection refused\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.790181    8557 reflector.go:240] Listing and watching *v1.Pod from k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.791162    8557 reflector.go:240] Listing and watching *v1.Service from k8s.io/kubernetes/pkg/kubelet/kubelet.go:451\r\nJan 09 22:24:40  kubelet[8557]: E0109 22:24:40.791665    8557 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://apiserver_ip/api/v1/pods?fieldSelector=spec.nodeName%3D&limit=500&resourceVersion=0: dial tcp apiserver_ip: getsockopt: connection refused\r\nJan 09 22:24:40  kubelet[8557]: E0109 22:24:40.792660    8557 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list *v1.Service: Get https://apiserver_ip/api/v1/services?limit=500&resourceVersion=0: dial tcp apiserver_ip: getsockopt: connection refused\r\nJan 09 22:24:40  kubelet[8557]: I0109 22:24:40.793359    8557 reflector.go:240] Listing and watching *v1.Node from k8s.io/kubernetes/pkg/kubelet/kubelet.go:460\r\nJan 09 22:24:40  kubelet[8557]: E0109 22:24:40.795195    8557 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to list *v1.Node: Get https://apiserver_ip/api/v1/nodes?fieldSelector=metadata.name%3D&limit=500&resourceVersion=0: dial tcp apiserver_ip: getsockopt: connection refused\r\nJan 09 22:24:41  kubelet[8557]: I0109 22:24:41.791818    8557 reflector.go:240] Listing and watching *v1.Pod from k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47\r\nJan 09 22:24:41  kubelet[8557]: I0109 22:24:41.792916    8557 reflector.go:240] Listing and watching *v1.Service from k8s.io/kubernetes/pkg/kubelet/kubelet.go:451\r\nJan 09 22:24:41  kubelet[8557]: E0109 22:24:41.793714    8557 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://apiserver_ip/api/v1/pods?fieldSelector=spec.nodeName%3D&limit=500&resourceVersion=0: dial tcp apiserver_ip: getsockopt: connection refused\r\nJan 09 22:24:41  kubelet[8557]: E0109 22:24:41.794424    8557 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list *v1.Service: Get https://apiserver_ip/api/v1/services?limit=500&resourceVersion=0: dial tcp apiserver_ip: getsockopt: connection refused\r\nJan 09 22:24:41  kubelet[8557]: I0109 22:24:41.795435    8557 reflector.go:240] Listing and watching *v1.Node from k8s.io/kubernetes/pkg/kubelet/kubelet.go:460\r\nJan 09 22:24:41  kubelet[8557]: E0109 22:24:41.796962    8557 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to list *v1.Node: Get https://apiserver_ip/api/v1/nodes?fieldSelector=metadata.name%3D&limit=500&resourceVersion=0: dial tcp apiserver_ip: getsockopt: connection refused\r\nJan 09 22:24:49  kubelet[8557]: I0109 22:24:49.807598    8557 kubelet.go:1941] SyncLoop (housekeeping, skipped): sources aren't ready yet.\r\n```\r\n\r\ndockerd logs shows the following: \r\n\r\n```\r\nJan 09 22:53:16  systemd[1]: Stopped Docker Application Container Engine.\r\nJan 09 22:53:36  systemd[1]: Starting Docker Application Container Engine...\r\nJan 09 22:53:36  docker[14748]: Command \"daemon\" is deprecated, and will be removed in Docker 1.16. Please run `dockerd` directly.\r\nJan 09 22:53:36  docker[14748]: time=\"2019-01-09T22:53:36.130706655Z\" level=info msg=\"libcontainerd: new containerd process, pid: 14760\"\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.192707070Z\" level=info msg=\"Graph migration to content-addressability took 0.00 seconds\"\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.192940870Z\" level=warning msg=\"Your kernel does not support swap memory limit\"\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.192977370Z\" level=warning msg=\"Your kernel does not support cgroup rt period\"\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.192985970Z\" level=warning msg=\"Your kernel does not support cgroup rt runtime\"\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.192995770Z\" level=warning msg=\"Your kernel does not support cgroup blkio weight\"\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.193003670Z\" level=warning msg=\"Your kernel does not support cgroup blkio weight_device\"\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.193405771Z\" level=info msg=\"Loading containers: start.\"\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.256683521Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.289359398Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container dc8ead248e9ecd25bfa5540\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.296771216Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.332745801Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container 7381df6431eddc3f15a3021\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.339801318Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.436661047Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container 9da90d4adee91367e48f574\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.445349068Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.466067117Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container eae6ec26c4a6c10fe0c9e41\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.473333134Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.506976014Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container 065ebcc65b4ac6537ad0d6c\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.513722830Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.531190971Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container 37f6983d056b6a5065cf155\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.539830491Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.574863574Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container d5cabd4aa7773b5dc900672\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.582313492Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.621339384Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container e5a9337a8d0a1532a014564\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.628576502Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.696209662Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container 042b41d37847ae1f3950277\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.703788080Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.726445433Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container 91204cf7b48f833ae72a60d\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.733397450Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.801586911Z\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container a0858428173ded7861c15db\r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.808029026Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.843199010Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.877666091Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.940210039Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:37  docker[14748]: time=\"2019-01-09T22:53:37.970266611Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:38  docker[14748]: time=\"2019-01-09T22:53:38.027987847Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:38  docker[14748]: time=\"2019-01-09T22:53:38.079269869Z\" level=warning msg=\"libcontainerd: client is out of sync, restore was called on a fully synced container \r\nJan 09 22:53:38  docker[14748]: time=\"2019-01-09T22:53:38.134096598Z\" level=info msg=\"Firewalld running: false\"\r\nJan 09 22:53:38  docker[14748]: time=\"2019-01-09T22:53:38.216353793Z\" level=info msg=\"There are old running containers, the network config will not take affect\"\r\nJan 09 22:53:38  docker[14748]: time=\"2019-01-09T22:53:38.625723962Z\" level=info msg=\"Loading containers: done.\"\r\nJan 09 22:53:38  docker[14748]: time=\"2019-01-09T22:53:38.727042602Z\" level=info msg=\"Daemon has completed initialization\"\r\nJan 09 22:53:38  docker[14748]: time=\"2019-01-09T22:53:38.727108402Z\" level=info msg=\"Docker daemon\" commit=c6d412e graphdriver=overlay2 version=17.03.1-ce\r\nJan 09 22:53:38  docker[14748]: time=\"2019-01-09T22:53:38.737987228Z\" level=info msg=\"API listen on /var/run/docker.sock\"\r\n```\r\n\r\nwhen inspecting containers created by the kubelet we see that we create containers with the following restart policy:\r\n\r\n```\r\n            \"RestartPolicy\": {\r\n                \"Name\": \"\",\r\n                \"MaximumRetryCount\": 0\r\n            },\r\n```\r\n\r\nThis is a blocker for kubernetes/enhancements#719 as when the node restart the controller-manager cannot assume that it is safe to detach volumes as they might be mounted by the pods.\r\n\r\n@jingxu97 @yujuhong @liggitt \r\n\r\n/priority critical-urgent\r\n/kind bug\r\n/sig node\r\n",
  "closed_at": "2019-03-08T00:12:40Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 7,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/73278/comments",
  "created_at": "2019-01-24T16:36:20Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/73278/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/73278",
  "id": 402793597,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "e11d21",
      "default": false,
      "description": "Highest priority. Must be actively worked on as someone's top priority right now.",
      "id": 114528068,
      "name": "priority/critical-urgent",
      "node_id": "MDU6TGFiZWwxMTQ1MjgwNjg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/priority/critical-urgent"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Node.",
      "id": 173493665,
      "name": "sig/node",
      "node_id": "MDU6TGFiZWwxNzM0OTM2NjU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/node"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/73278/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU0MDI3OTM1OTc=",
  "number": 73278,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "when node restarts, docker restarts the containers automatically",
  "updated_at": "2019-03-08T00:12:40Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/73278",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/7813699?v=4",
    "events_url": "https://api.github.com/users/yastij/events{/privacy}",
    "followers_url": "https://api.github.com/users/yastij/followers",
    "following_url": "https://api.github.com/users/yastij/following{/other_user}",
    "gists_url": "https://api.github.com/users/yastij/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/yastij",
    "id": 7813699,
    "login": "yastij",
    "node_id": "MDQ6VXNlcjc4MTM2OTk=",
    "organizations_url": "https://api.github.com/users/yastij/orgs",
    "received_events_url": "https://api.github.com/users/yastij/received_events",
    "repos_url": "https://api.github.com/users/yastij/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/yastij/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/yastij/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/yastij"
  }
}