{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "<!-- This form is for bug reports and feature requests ONLY!\r\n\r\nIf you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/.\r\n-->\r\n\r\n**Is this a BUG REPORT or FEATURE REQUEST?**:\r\n\r\n/kind bug\r\n\r\n**What happened**:\r\n\r\nTried to deploy the metrics server on top of an existing bootkube installation, but HPA reports FailedRescale.\r\n\r\n```\r\napiVersion: autoscaling/v1\r\nkind: HorizontalPodAutoscaler\r\nmetadata:\r\n  name: hpatest\r\n  namespace: default\r\nspec:\r\n  maxReplicas: 3\r\n  minReplicas: 1\r\n  scaleTargetRef:\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    name: hpatest\r\n  targetCPUUtilizationPercentage: 2500\r\n---\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  labels:\r\n    run: hpatest\r\n  name: hpatest\r\n  namespace: default\r\nspec:\r\n  progressDeadlineSeconds: 600\r\n  replicas: 1\r\n  revisionHistoryLimit: 10\r\n  selector:\r\n    matchLabels:\r\n      run: hpatest\r\n  strategy:\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 1\r\n    type: RollingUpdate\r\n  template:\r\n    metadata:\r\n      labels:\r\n        run: hpatest\r\n    spec:\r\n      containers:\r\n      - name: hpatest\r\n        image: \"centos:7\"\r\n        command: [ \"/bin/sh\", \"-c\", \"--\" ]\r\n        args: [ \"while true; do yum -y install epel-release && yum -y install stress && stress --cpu 1 --timeout 900s; sleep 300; done\" ]\r\n        resources:\r\n          requests:\r\n            cpu: 10m\r\n        terminationMessagePath: /dev/termination-log\r\n        terminationMessagePolicy: File\r\n      dnsPolicy: ClusterFirst\r\n      restartPolicy: Always\r\n      schedulerName: default-scheduler\r\n      securityContext: {}\r\n      terminationGracePeriodSeconds: 30\r\n```\r\n\r\n```\r\n$ kubectl describe hpa hpatest\r\nName:                                                  hpatest\r\nNamespace:                                             default\r\nLabels:                                                <none>\r\nAnnotations:                                           kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"autoscaling/v1\",\"kind\":\"HorizontalPodAutoscaler\",\"metadata\":{\"annotations\":{},\"name\":\"hpatest\",\"namespace\":\"default\"},\"spec\":{\"maxReplic...\r\nCreationTimestamp:                                     Thu, 09 Aug 2018 22:46:02 +0000\r\nReference:                                             Deployment/hpatest\r\nMetrics:                                               ( current / target )\r\n  resource cpu on pods  (as a percentage of request):  <unknown> / 2500%\r\nMin replicas:                                          1\r\nMax replicas:                                          3\r\nConditions:\r\n  Type            Status  Reason             Message\r\n  ----            ------  ------             -------\r\n  AbleToScale     False   FailedUpdateScale  the HPA controller was unable to update the target scale: could not update the scale for deployments.apps hpatest: Internal error occurred: converting (extensions.Deployment).Replicas to (v1.Scale).Replicas: Selector not present in src\r\n  ScalingActive   True    ValidMetricFound   the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\r\n  ScalingLimited  True    TooManyReplicas    the desired replica count is more than the maximum replica count\r\nEvents:\r\n  Type     Reason                        Age                From                       Message\r\n  ----     ------                        ----               ----                       -------\r\n  Warning  FailedGetResourceMetric       1m (x4 over 2m)    horizontal-pod-autoscaler  unable to get metrics for resource cpu: no metrics returned from resource metrics API\r\n  Warning  FailedComputeMetricsReplicas  1m (x4 over 2m)    horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API\r\n  Warning  FailedRescale                 27s (x2 over 57s)  horizontal-pod-autoscaler  New size: 3; reason: cpu resource utilization (percentage of request) above target; error: could not update the scale for deployments.apps hpatest: Internal error occurred: converting (extensions.Deployment).Replicas to (v1.Scale).Replicas: Selector not present in src\r\n```\r\n\r\n**What you expected to happen**:\r\n\r\nExpected pods to scale up from 1 to 3.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n1. Run bootkube to provision a 1.10 cluster on aws\r\n2. Add apiserver flags\r\n3. Deploy metrics-server\r\n4. Deploy hpa\r\n\r\n**Anything else we need to know?**:\r\n\r\napiserver flags:\r\n```\r\n+        - --requestheader-client-ca-file=/etc/kubernetes/secrets/ca.crt\r\n+        - --requestheader-allowed-names=aggregator,kube-apiserver\r\n+        - --requestheader-extra-headers-prefix=X-Remote-Extra-\r\n+        - --requestheader-group-headers=X-Remote-Group\r\n+        - --requestheader-username-headers=X-Remote-User\r\n+        - --proxy-client-cert-file=/etc/kubernetes/secrets/apiserver.crt\r\n+        - --proxy-client-key-file=/etc/kubernetes/secrets/apiserver.key\r\n```\r\n\r\n```\r\n$ kubectl describe hpa hpatest\r\nName:                                                  hpatest\r\nNamespace:                                             default\r\nLabels:                                                <none>\r\nAnnotations:                                           kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"autoscaling/v1\",\"kind\":\"HorizontalPodAutoscaler\",\"metadata\":{\"annotations\":{},\"name\":\"hpatest\",\"namespace\":\"default\"},\"spec\":{\"maxReplic...\r\nCreationTimestamp:                                     Thu, 09 Aug 2018 22:46:02 +0000\r\nReference:                                             Deployment/hpatest\r\nMetrics:                                               ( current / target )\r\n  resource cpu on pods  (as a percentage of request):  <unknown> / 2500%\r\nMin replicas:                                          1\r\nMax replicas:                                          3\r\nConditions:\r\n  Type            Status  Reason             Message\r\n  ----            ------  ------             -------\r\n  AbleToScale     False   FailedUpdateScale  the HPA controller was unable to update the target scale: could not update the scale for deployments.apps hpatest: Internal error occurred: converting (extensions.Deployment).Replicas to (v1.Scale).Replicas: Selector not present in src\r\n  ScalingActive   True    ValidMetricFound   the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\r\n  ScalingLimited  True    TooManyReplicas    the desired replica count is more than the maximum replica count\r\nEvents:\r\n  Type     Reason                        Age                From                       Message\r\n  ----     ------                        ----               ----                       -------\r\n  Warning  FailedGetResourceMetric       1m (x4 over 2m)    horizontal-pod-autoscaler  unable to get metrics for resource cpu: no metrics returned from resource metrics API\r\n  Warning  FailedComputeMetricsReplicas  1m (x4 over 2m)    horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API\r\n  Warning  FailedRescale                 27s (x2 over 57s)  horizontal-pod-autoscaler  New size: 3; reason: cpu resource utilization (percentage of request) above target; error: could not update the scale for deployments.apps hpatest: Internal error occurred: converting (extensions.Deployment).Replicas to (v1.Scale).Replicas: Selector not present in src\r\n```\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\nClient Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.6\", GitCommit:\"a21fdbd78dde8f5447f5f6c331f7eb6f80bd684e\", GitTreeState:\"clean\", BuildDate:\"2018-07-26T10:04:08Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.6\", GitCommit:\"a21fdbd78dde8f5447f5f6c331f7eb6f80bd684e\", GitTreeState:\"clean\", BuildDate:\"2018-07-26T10:04:08Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n\r\n- Cloud provider or hardware configuration: \r\nAWS\r\n\r\n- OS (e.g. from /etc/os-release):\r\nNAME=\"Container Linux by CoreOS\"\r\nID=coreos\r\nVERSION=1745.7.0\r\nVERSION_ID=1745.7.0\r\nBUILD_ID=2018-06-14-0909\r\nPRETTY_NAME=\"Container Linux by CoreOS 1745.7.0 (Rhyolite)\"\r\nANSI_COLOR=\"38;5;75\"\r\nHOME_URL=\"https://coreos.com/\"\r\nBUG_REPORT_URL=\"https://issues.coreos.com\"\r\nCOREOS_BOARD=\"amd64-usr\"\r\n\r\n- Kernel (e.g. `uname -a`):\r\nLinux ip-10-11-13-13.ec2.internal 4.14.48-coreos-r2 #1 SMP Thu Jun 14 08:23:03 UTC 2018 x86_64 Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz GenuineIntel GNU/Linux\r\n\r\n- Install tools:\r\nbootkube\r\n\r\n- Others:\r\ndocker version\r\nClient:\r\n Version:      17.03.2-ce\r\n API version:  1.27\r\n Go version:   go1.7.6\r\n Git commit:   2360430\r\n Built:        Thu Jun 14 08:42:43 2018\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.03.2-ce\r\n API version:  1.27 (minimum version 1.12)\r\n Go version:   go1.7.6\r\n Git commit:   2360430\r\n Built:        Thu Jun 14 08:42:43 2018\r\n OS/Arch:      linux/amd64\r\n Experimental: false",
  "closed_at": "2019-04-22T17:44:19Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 22,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/67221/comments",
  "created_at": "2018-08-09T23:02:19Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/67221/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/67221",
  "id": 349320781,
  "labels": [
    {
      "color": "0052cc",
      "default": false,
      "description": null,
      "id": 486445796,
      "name": "area/admission-control",
      "node_id": "MDU6TGFiZWw0ODY0NDU3OTY=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/area/admission-control"
    },
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG API Machinery.",
      "id": 173493835,
      "name": "sig/api-machinery",
      "node_id": "MDU6TGFiZWwxNzM0OTM4MzU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/api-machinery"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Autoscaling.",
      "id": 238245616,
      "name": "sig/autoscaling",
      "node_id": "MDU6TGFiZWwyMzgyNDU2MTY=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/autoscaling"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/67221/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUzNDkzMjA3ODE=",
  "number": 67221,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "FailedRescale from horizontal-pod-autoscaler",
  "updated_at": "2019-04-22T17:44:19Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/67221",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20053575?v=4",
    "events_url": "https://api.github.com/users/kh34/events{/privacy}",
    "followers_url": "https://api.github.com/users/kh34/followers",
    "following_url": "https://api.github.com/users/kh34/following{/other_user}",
    "gists_url": "https://api.github.com/users/kh34/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/kh34",
    "id": 20053575,
    "login": "kh34",
    "node_id": "MDQ6VXNlcjIwMDUzNTc1",
    "organizations_url": "https://api.github.com/users/kh34/orgs",
    "received_events_url": "https://api.github.com/users/kh34/received_events",
    "repos_url": "https://api.github.com/users/kh34/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/kh34/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/kh34/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/kh34"
  }
}