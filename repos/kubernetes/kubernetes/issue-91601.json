{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/230981?v=4",
    "events_url": "https://api.github.com/users/pancernik/events{/privacy}",
    "followers_url": "https://api.github.com/users/pancernik/followers",
    "following_url": "https://api.github.com/users/pancernik/following{/other_user}",
    "gists_url": "https://api.github.com/users/pancernik/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/pancernik",
    "id": 230981,
    "login": "pancernik",
    "node_id": "MDQ6VXNlcjIzMDk4MQ==",
    "organizations_url": "https://api.github.com/users/pancernik/orgs",
    "received_events_url": "https://api.github.com/users/pancernik/received_events",
    "repos_url": "https://api.github.com/users/pancernik/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/pancernik/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/pancernik/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/pancernik"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars1.githubusercontent.com/u/230981?v=4",
      "events_url": "https://api.github.com/users/pancernik/events{/privacy}",
      "followers_url": "https://api.github.com/users/pancernik/followers",
      "following_url": "https://api.github.com/users/pancernik/following{/other_user}",
      "gists_url": "https://api.github.com/users/pancernik/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/pancernik",
      "id": 230981,
      "login": "pancernik",
      "node_id": "MDQ6VXNlcjIzMDk4MQ==",
      "organizations_url": "https://api.github.com/users/pancernik/orgs",
      "received_events_url": "https://api.github.com/users/pancernik/received_events",
      "repos_url": "https://api.github.com/users/pancernik/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/pancernik/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pancernik/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/pancernik"
    }
  ],
  "author_association": "MEMBER",
  "body": "**What happened**: We upgraded 15 kubernetes clusters from 1.17.5 to 1.18.2/1.18.3 and started to see that daemonsets does not work properly anymore.\r\n\r\nThe problem is that all daemonset pods does not provision. It will return following error message to events:\r\n\r\n```\r\nEvents:\r\n  Type     Reason            Age               From               Message\r\n  ----     ------            ----              ----               -------\r\n  Warning  FailedScheduling  9s (x5 over 71s)  default-scheduler  0/13 nodes are available: 12 node(s) didn't match node selector.\r\n```\r\n\r\nHowever, all nodes are available and it does not have node selector. Nodes does not have taints either.\r\n\r\ndaemonset https://gist.github.com/zetaab/4a605cb3e15e349934cb7db29ec72bd8\r\n\r\n```\r\n% kubectl get nodes\r\nNAME                                   STATUS   ROLES    AGE   VERSION\r\ne2etest-1-kaasprod-k8s-local           Ready    node     46h   v1.18.3\r\ne2etest-2-kaasprod-k8s-local           Ready    node     46h   v1.18.3\r\ne2etest-3-kaasprod-k8s-local           Ready    node     44h   v1.18.3\r\ne2etest-4-kaasprod-k8s-local           Ready    node     44h   v1.18.3\r\nmaster-zone-1-1-1-kaasprod-k8s-local   Ready    master   47h   v1.18.3\r\nmaster-zone-2-1-1-kaasprod-k8s-local   Ready    master   47h   v1.18.3\r\nmaster-zone-3-1-1-kaasprod-k8s-local   Ready    master   47h   v1.18.3\r\nnodes-z1-1-kaasprod-k8s-local          Ready    node     47h   v1.18.3\r\nnodes-z1-2-kaasprod-k8s-local          Ready    node     47h   v1.18.3\r\nnodes-z2-1-kaasprod-k8s-local          Ready    node     46h   v1.18.3\r\nnodes-z2-2-kaasprod-k8s-local          Ready    node     46h   v1.18.3\r\nnodes-z3-1-kaasprod-k8s-local          Ready    node     47h   v1.18.3\r\nnodes-z3-2-kaasprod-k8s-local          Ready    node     46h   v1.18.3\r\n\r\n% kubectl get pods -n weave -l weave-scope-component=agent -o wide\r\nNAME                      READY   STATUS    RESTARTS   AGE     IP           NODE                                   NOMINATED NODE   READINESS GATES\r\nweave-scope-agent-2drzw   1/1     Running   0          26h     10.1.32.23   e2etest-1-kaasprod-k8s-local           <none>           <none>\r\nweave-scope-agent-4kpxc   1/1     Running   3          26h     10.1.32.12   nodes-z1-2-kaasprod-k8s-local          <none>           <none>\r\nweave-scope-agent-78n7r   1/1     Running   0          26h     10.1.32.7    e2etest-4-kaasprod-k8s-local           <none>           <none>\r\nweave-scope-agent-9m4n8   1/1     Running   0          26h     10.1.96.4    master-zone-1-1-1-kaasprod-k8s-local   <none>           <none>\r\nweave-scope-agent-b2gnk   1/1     Running   1          26h     10.1.96.12   master-zone-3-1-1-kaasprod-k8s-local   <none>           <none>\r\nweave-scope-agent-blwtx   1/1     Running   2          26h     10.1.32.20   nodes-z1-1-kaasprod-k8s-local          <none>           <none>\r\nweave-scope-agent-cbhjg   1/1     Running   0          26h     10.1.64.15   e2etest-2-kaasprod-k8s-local           <none>           <none>\r\nweave-scope-agent-csp49   1/1     Running   0          26h     10.1.96.14   e2etest-3-kaasprod-k8s-local           <none>           <none>\r\nweave-scope-agent-g4k2x   1/1     Running   1          26h     10.1.64.10   nodes-z2-2-kaasprod-k8s-local          <none>           <none>\r\nweave-scope-agent-kx85h   1/1     Running   2          26h     10.1.96.6    nodes-z3-1-kaasprod-k8s-local          <none>           <none>\r\nweave-scope-agent-lllqc   0/1     Pending   0          5m56s   <none>       <none>                                 <none>           <none>\r\nweave-scope-agent-nls2h   1/1     Running   0          26h     10.1.96.17   master-zone-2-1-1-kaasprod-k8s-local   <none>           <none>\r\nweave-scope-agent-p8njs   1/1     Running   2          26h     10.1.96.19   nodes-z3-2-kaasprod-k8s-local          <none>           <none>\r\n```\r\n\r\nI have tried to restart apiserver/schedulers/controller-managers but it does not help. Also I have tried to restart that single node that is stuck (nodes-z2-1-kaasprod-k8s-local) but it does not help either. Only deleting that node and recreating it helps.\r\n\r\n\r\n```\r\n% kubectl describe node nodes-z2-1-kaasprod-k8s-local\r\nName:               nodes-z2-1-kaasprod-k8s-local\r\nRoles:              node\r\nLabels:             beta.kubernetes.io/arch=amd64\r\n                    beta.kubernetes.io/instance-type=59cf4871-de1b-4294-9e9f-2ea7ca4b771f\r\n                    beta.kubernetes.io/os=linux\r\n                    failure-domain.beta.kubernetes.io/region=regionOne\r\n                    failure-domain.beta.kubernetes.io/zone=zone-2\r\n                    kops.k8s.io/instancegroup=nodes-z2\r\n                    kubernetes.io/arch=amd64\r\n                    kubernetes.io/hostname=nodes-z2-1-kaasprod-k8s-local\r\n                    kubernetes.io/os=linux\r\n                    kubernetes.io/role=node\r\n                    node-role.kubernetes.io/node=\r\n                    node.kubernetes.io/instance-type=59cf4871-de1b-4294-9e9f-2ea7ca4b771f\r\n                    topology.cinder.csi.openstack.org/zone=zone-2\r\n                    topology.kubernetes.io/region=regionOne\r\n                    topology.kubernetes.io/zone=zone-2\r\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"faf14d22-010f-494a-9b34-888bdad1d2df\"}\r\n                    node.alpha.kubernetes.io/ttl: 0\r\n                    projectcalico.org/IPv4Address: 10.1.64.32/19\r\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.98.136.0\r\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\r\nCreationTimestamp:  Thu, 28 May 2020 13:28:24 +0300\r\nTaints:             <none>\r\nUnschedulable:      false\r\nLease:\r\n  HolderIdentity:  nodes-z2-1-kaasprod-k8s-local\r\n  AcquireTime:     <unset>\r\n  RenewTime:       Sat, 30 May 2020 12:02:13 +0300\r\nConditions:\r\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\r\n  ----                 ------  -----------------                 ------------------                ------                       -------\r\n  NetworkUnavailable   False   Fri, 29 May 2020 09:40:51 +0300   Fri, 29 May 2020 09:40:51 +0300   CalicoIsUp                   Calico is running on this node\r\n  MemoryPressure       False   Sat, 30 May 2020 11:59:53 +0300   Fri, 29 May 2020 09:40:45 +0300   KubeletHasSufficientMemory   kubelet has sufficient memory available\r\n  DiskPressure         False   Sat, 30 May 2020 11:59:53 +0300   Fri, 29 May 2020 09:40:45 +0300   KubeletHasNoDiskPressure     kubelet has no disk pressure\r\n  PIDPressure          False   Sat, 30 May 2020 11:59:53 +0300   Fri, 29 May 2020 09:40:45 +0300   KubeletHasSufficientPID      kubelet has sufficient PID available\r\n  Ready                True    Sat, 30 May 2020 11:59:53 +0300   Fri, 29 May 2020 09:40:45 +0300   KubeletReady                 kubelet is posting ready status. AppArmor enabled\r\nAddresses:\r\n  InternalIP:  10.1.64.32\r\n  Hostname:    nodes-z2-1-kaasprod-k8s-local\r\nCapacity:\r\n  cpu:                4\r\n  ephemeral-storage:  10287360Ki\r\n  hugepages-1Gi:      0\r\n  hugepages-2Mi:      0\r\n  memory:             8172420Ki\r\n  pods:               110\r\nAllocatable:\r\n  cpu:                4\r\n  ephemeral-storage:  9480830961\r\n  hugepages-1Gi:      0\r\n  hugepages-2Mi:      0\r\n  memory:             8070020Ki\r\n  pods:               110\r\nSystem Info:\r\n  Machine ID:                 c94284656ff04cf090852c1ddee7bcc2\r\n  System UUID:                faf14d22-010f-494a-9b34-888bdad1d2df\r\n  Boot ID:                    295dc3d9-0a90-49ee-92f3-9be45f2f8e3d\r\n  Kernel Version:             4.19.0-8-cloud-amd64\r\n  OS Image:                   Debian GNU/Linux 10 (buster)\r\n  Operating System:           linux\r\n  Architecture:               amd64\r\n  Container Runtime Version:  docker://19.3.8\r\n  Kubelet Version:            v1.18.3\r\n  Kube-Proxy Version:         v1.18.3\r\nPodCIDR:                      100.96.12.0/24\r\nPodCIDRs:                     100.96.12.0/24\r\nProviderID:                   openstack:///faf14d22-010f-494a-9b34-888bdad1d2df\r\nNon-terminated Pods:          (3 in total)\r\n  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\r\n  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---\r\n  kube-system                 calico-node-77pqs                           100m (2%)     200m (5%)   100Mi (1%)       100Mi (1%)     46h\r\n  kube-system                 kube-proxy-nodes-z2-1-kaasprod-k8s-local    100m (2%)     200m (5%)   100Mi (1%)       100Mi (1%)     46h\r\n  volume                      csi-cinder-nodeplugin-5jbvl                 100m (2%)     400m (10%)  200Mi (2%)       200Mi (2%)     46h\r\nAllocated resources:\r\n  (Total limits may be over 100 percent, i.e., overcommitted.)\r\n  Resource           Requests    Limits\r\n  --------           --------    ------\r\n  cpu                300m (7%)   800m (20%)\r\n  memory             400Mi (5%)  400Mi (5%)\r\n  ephemeral-storage  0 (0%)      0 (0%)\r\nEvents:\r\n  Type    Reason                   Age    From                                    Message\r\n  ----    ------                   ----   ----                                    -------\r\n  Normal  Starting                 7m27s  kubelet, nodes-z2-1-kaasprod-k8s-local  Starting kubelet.\r\n  Normal  NodeHasSufficientMemory  7m26s  kubelet, nodes-z2-1-kaasprod-k8s-local  Node nodes-z2-1-kaasprod-k8s-local status is now: NodeHasSufficientMemory\r\n  Normal  NodeHasNoDiskPressure    7m26s  kubelet, nodes-z2-1-kaasprod-k8s-local  Node nodes-z2-1-kaasprod-k8s-local status is now: NodeHasNoDiskPressure\r\n  Normal  NodeHasSufficientPID     7m26s  kubelet, nodes-z2-1-kaasprod-k8s-local  Node nodes-z2-1-kaasprod-k8s-local status is now: NodeHasSufficientPID\r\n  Normal  NodeAllocatableEnforced  7m26s  kubelet, nodes-z2-1-kaasprod-k8s-local  Updated Node Allocatable limit across pods\r\n```\r\n\r\nWe are seeing this randomly in all of our clusters.\r\n\r\n**What you expected to happen**: I expect that daemonset will provision to all nodes.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**: No idea really, install 1.18.x kubernetes and deploy daemonset and after that wait days(?)\r\n\r\n**Anything else we need to know?**: When this happens we cannot provision any other daemonsets to that node either. Like you can see logging fluent-bit is also missing. I cannot see any errors in that node kubelet logs and like said, restarting does not help.\r\n\r\n```\r\n% kubectl get ds --all-namespaces\r\nNAMESPACE     NAME                       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE\r\nfalco         falco-daemonset            13        13        12      13           12          <none>                            337d\r\nkube-system   audit-webhook-deployment   3         3         3       3            3           node-role.kubernetes.io/master=   174d\r\nkube-system   calico-node                13        13        13      13           13          kubernetes.io/os=linux            36d\r\nkube-system   kops-controller            3         3         3       3            3           node-role.kubernetes.io/master=   193d\r\nkube-system   metricbeat                 6         6         5       6            5           <none>                            35d\r\nkube-system   openstack-cloud-provider   3         3         3       3            3           node-role.kubernetes.io/master=   337d\r\nlogging       fluent-bit                 13        13        12      13           12          <none>                            337d\r\nmonitoring    node-exporter              13        13        12      13           12          kubernetes.io/os=linux            58d\r\nvolume        csi-cinder-nodeplugin      6         6         6       6            6           <none>                            239d\r\nweave         weave-scope-agent          13        13        12      13           12          <none>                            193d\r\nweave         weavescope-iowait-plugin   6         6         5       6            5           <none>                            193d\r\n```\r\n\r\nLike you can see, most of the daemonsets are missing one pod\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): 1.18.3\r\n- Cloud provider or hardware configuration: openstack\r\n- OS (e.g: `cat /etc/os-release`): debian buster\r\n- Kernel (e.g. `uname -a`): Linux nodes-z2-1-kaasprod-k8s-local 4.19.0-8-cloud-amd64 #1 SMP Debian 4.19.98-1+deb10u1 (2020-04-27) x86_64 GNU/Linux\r\n- Install tools: kops\r\n- Network plugin and version (if this is a network-related bug): calico\r\n- Others:\r\n",
  "closed_at": "2020-07-24T19:10:23Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 129,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/91601/comments",
  "created_at": "2020-05-30T09:04:36Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/91601/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/91601",
  "id": 627685109,
  "labels": [
    {
      "color": "006b75",
      "default": true,
      "description": "Denotes an issue that needs help from a contributor. Must meet \"help wanted\" guidelines.",
      "id": 433686790,
      "name": "help wanted",
      "node_id": "MDU6TGFiZWw0MzM2ODY3OTA=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/help%20wanted"
    },
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "eb6420",
      "default": false,
      "description": "Must be staffed and worked on either currently, or very soon, ideally in time for the next release.",
      "id": 114528223,
      "name": "priority/important-soon",
      "node_id": "MDU6TGFiZWwxMTQ1MjgyMjM=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/priority/important-soon"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Scheduling.",
      "id": 125550211,
      "name": "sig/scheduling",
      "node_id": "MDU6TGFiZWwxMjU1NTAyMTE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/scheduling"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/91601/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU2Mjc2ODUxMDk=",
  "number": 91601,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Some nodes are not considered in scheduling when there is zone imbalance",
  "updated_at": "2020-09-15T20:49:01Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/91601",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/22482503?v=4",
    "events_url": "https://api.github.com/users/zetaab/events{/privacy}",
    "followers_url": "https://api.github.com/users/zetaab/followers",
    "following_url": "https://api.github.com/users/zetaab/following{/other_user}",
    "gists_url": "https://api.github.com/users/zetaab/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/zetaab",
    "id": 22482503,
    "login": "zetaab",
    "node_id": "MDQ6VXNlcjIyNDgyNTAz",
    "organizations_url": "https://api.github.com/users/zetaab/orgs",
    "received_events_url": "https://api.github.com/users/zetaab/received_events",
    "repos_url": "https://api.github.com/users/zetaab/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/zetaab/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/zetaab/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/zetaab"
  }
}