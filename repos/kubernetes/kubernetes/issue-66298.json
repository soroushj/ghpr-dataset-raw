{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "CONTRIBUTOR",
  "body": "**Is this a BUG REPORT or FEATURE REQUEST?**:\r\n\r\n/kind bug\r\n\r\n\r\n**What happened**:\r\n\r\nAfter creating a _DaemonSet_ that uses node affinity to request only creating pods on nodes with a label matching a set of candidate values, pods get created on the appropriate nodes\u2014and, fortunately, only on the appropriate nodes\u2014but the pods get terminated and deleted quickly before they can start running. The cycle takes about ten seconds from pod creation through completion of deletion; the pod appears to be deleted within one second after its creation, but it takes about ten seconds for the deletion to complete.\r\n\r\nIf I vary the _DaemonSet_'s scheduling predicates, I find the following work to get pods running successfully:\r\n- Use `spec.template.spec.nodeSelector` to choose a specific node.\r\n- Use `spec.template.spec.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms` with an operator of \"Exists\" or \"NotIn\" to choose some nodes.\r\n- Use `spec.template.spec.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms` with an operator of \"In,\" but **with only one value** to choose some nodes.\r\n- Use `spec.template.spec.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms` with a label that is not applied by the cloud provider.  \r\n  If I come up with my own label such as \"special-hardware\" and use _kubectl label node_ to apply it, and use that label as a match expression key, it seems to work fine. It's the dynamically applied labels like \"beta.kubernetes.io/instance-type\" and \"kubernetes.io/hostname\" that trigger this problem.\r\n\r\nWithout the above concessions, if I use `spec.template.spec.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms` with an operator of \"In,\" but **with more than one value**, the pods don't start correctly.\r\n\r\nNote that this behavior is a **regression from Kubernetes version 1.10.4**. The same configuration works as intended in a cluster running that earlier version.\r\n\r\n**What you expected to happen**:\r\n\r\nAfter creating the _DaemonSet_, pods would start successfully on the nodes with label values matching one of the _DaemonSet_'s candidates values.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n1. Create a _DaemonSet_ that uses node affinity to request only creating pods on nodes with a label matching a set of candidate values.\r\n``` yaml\r\napiVersion: apps/v1\r\nkind: DaemonSet\r\nmetadata:\r\n  name: problem\r\n  labels:\r\n    purpose: demonstrate\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: sleeper\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: sleeper\r\n    spec:\r\n      affinity:\r\n        nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n            - matchExpressions:\r\n              - key: beta.kubernetes.io/instance-type\r\n                operator: In\r\n                values:\r\n                - p2.xlarge\r\n                - g3.8xlarge\r\n      tolerations:\r\n      - operator: Exists\r\n        effect: NoSchedule\r\n      containers:\r\n      - name: busybox\r\n        image: busybox\r\n        command: [\"/bin/sleep\", \"7200\"]\r\n```\r\n2. Confirm that at least one node has a label with a value that matches a member of that candidate set.\r\n2. Use _kubectl get pods -o wide_ to observe that there are pods created on behalf of the _DaemonSet_ on nodes that match the predicate, but they have status \"Terminating.\" By watching the pods, you can see new pods arrive with status \"Pending,\" then \"ContainerCreating,\" then \"Terminating,\" which they'll retain until deletion completes and a replacement arrives.\r\n2. Create a similar pod directly, without a supervising _DaemonSet_.  \r\n  If the \"beta.kubernetes.io/instance-type\" label is inconvenient within your cluster, instead consider using \"kubernetes.io/hostname\" and a few hostnames as the candidate values.\r\n``` yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: created-directly\r\n  labels:\r\n    purpose: demonstrate\r\nspec:\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: beta.kubernetes.io/instance-type\r\n            operator: In\r\n            values:\r\n            - p2.xlarge\r\n            - g3.8xlarge\r\n  tolerations:\r\n  - operator: Exists\r\n    effect: NoSchedule\r\n  containers:\r\n  - name: busybox\r\n    image: busybox\r\n    command: [\"/bin/sleep\", \"7200\"]\r\n```\r\n5. Confirm that one pod starts running successfully on one of the nodes that match that the predicate.\r\n5. Delete that lone pod (for clarity), and vary the predicate on the _DaemonSet_ to see which changes allow pods to start running successfully. Try each of these individually:\r\n    - Remove one of the values in the `matchExpressions[0].values` sequence.  \r\n      That is, leave only one candidate value.\r\n    - Change the `matchExpressions[0].values` operator to \"NotIn,\" and adjust the values to select some subset of the nodes.  \r\n    Alternately, try \"Exists\" with no values.\r\n    - Remove the node affinity stanza's `nodeSelectorTerms` and add a node selector in its place:\r\n``` yaml\r\nspec:\r\n  template:\r\n    spec:\r\n      nodeSelector:\r\n        beta.kubernetes.io/instance-type: g3.8xlarge\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nHere is the _DaemonSet_ object as captured via _kubectl get daemonset -o yaml_:\r\n``` yaml\r\napiVersion: extensions/v1beta1\r\nkind: DaemonSet\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"apps/v1\",\"kind\":\"DaemonSet\",\"metadata\":{\"annotations\":{},\"labels\":{\"purpose\":\"demonstrate\"},\"name\":\"problem\",\"namespace\":\"kube-system\"},\"spec\":{\"selector\":{\"matchLabels\":{\"app\":\"sleeper\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"sleeper\"}},\"spec\":{\"affinity\":{\"nodeAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":{\"nodeSelectorTerms\":[{\"matchExpressions\":[{\"key\":\"kubernetes.io/hostname\",\"operator\":\"In\",\"values\":[\"ip-10-103-0-201.ec2.internal\",\"ip-10-103-0-123.ec2.internal\"]}]}]}}},\"containers\":[{\"command\":[\"/bin/sleep\",\"7200\"],\"image\":\"busybox\",\"name\":\"busybox\"}],\"tolerations\":[{\"effect\":\"NoSchedule\",\"operator\":\"Exists\"}]}}}}\r\n  creationTimestamp: 2018-07-17T14:43:58Z\r\n  generation: 22\r\n  labels:\r\n    purpose: demonstrate\r\n  name: problem\r\n  namespace: kube-system\r\n  resourceVersion: \"574438\"\r\n  selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/problem\r\n  uid: d696a76e-89cf-11e8-b5fd-0a5cd3064e60\r\nspec:\r\n  revisionHistoryLimit: 10\r\n  selector:\r\n    matchLabels:\r\n      app: sleeper\r\n  template:\r\n    metadata:\r\n      creationTimestamp: null\r\n      labels:\r\n        app: sleeper\r\n    spec:\r\n      affinity:\r\n        nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n            - matchExpressions:\r\n              - key: kubernetes.io/hostname\r\n                operator: In\r\n                values:\r\n                - ip-10-103-0-201.ec2.internal\r\n                - ip-10-103-0-123.ec2.internal\r\n      containers:\r\n      - command:\r\n        - /bin/sleep\r\n        - \"7200\"\r\n        image: busybox\r\n        imagePullPolicy: Always\r\n        name: busybox\r\n        resources: {}\r\n        terminationMessagePath: /dev/termination-log\r\n        terminationMessagePolicy: File\r\n      dnsPolicy: ClusterFirst\r\n      restartPolicy: Always\r\n      schedulerName: default-scheduler\r\n      securityContext: {}\r\n      terminationGracePeriodSeconds: 30\r\n      tolerations:\r\n      - effect: NoSchedule\r\n        operator: Exists\r\n  templateGeneration: 22\r\n  updateStrategy:\r\n    rollingUpdate:\r\n      maxUnavailable: 1\r\n    type: RollingUpdate\r\nstatus:\r\n  currentNumberScheduled: 1\r\n  desiredNumberScheduled: 1\r\n  numberMisscheduled: 0\r\n  numberReady: 0\r\n  numberUnavailable: 1\r\n  observedGeneration: 22\r\n```\r\nHere is one of the pods created on behalf of the _DaemonSet_:\r\n``` yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  annotations:\r\n    cni.projectcalico.org/podIP: 192.168.11.46/32\r\n  creationTimestamp: 2018-07-17T15:15:17Z\r\n  deletionGracePeriodSeconds: 30\r\n  deletionTimestamp: 2018-07-17T15:15:49Z\r\n  generateName: problem-\r\n  labels:\r\n    app: sleeper\r\n    controller-revision-hash: \"335345753\"\r\n    pod-template-generation: \"22\"\r\n  name: problem-jp226\r\n  namespace: kube-system\r\n  ownerReferences:\r\n  - apiVersion: apps/v1\r\n    blockOwnerDeletion: true\r\n    controller: true\r\n    kind: DaemonSet\r\n    name: problem\r\n    uid: d696a76e-89cf-11e8-b5fd-0a5cd3064e60\r\n  resourceVersion: \"574816\"\r\n  selfLink: /api/v1/namespaces/kube-system/pods/problem-jp226\r\n  uid: 3676e744-89d4-11e8-9f7e-0e98abf5b5fa\r\nspec:\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: kubernetes.io/hostname\r\n            operator: In\r\n            values:\r\n            - ip-10-103-0-123.ec2.internal\r\n            - ip-10-103-0-201.ec2.internal\r\n  containers:\r\n  - command:\r\n    - /bin/sleep\r\n    - \"7200\"\r\n    image: busybox\r\n    imagePullPolicy: Always\r\n    name: busybox\r\n    resources: {}\r\n    terminationMessagePath: /dev/termination-log\r\n    terminationMessagePolicy: File\r\n    volumeMounts:\r\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\r\n      name: default-token-mnjh7\r\n      readOnly: true\r\n  dnsPolicy: ClusterFirst\r\n  nodeName: ip-10-103-0-201.ec2.internal\r\n  restartPolicy: Always\r\n  schedulerName: default-scheduler\r\n  securityContext: {}\r\n  serviceAccount: default\r\n  serviceAccountName: default\r\n  terminationGracePeriodSeconds: 30\r\n  tolerations:\r\n  - effect: NoSchedule\r\n    operator: Exists\r\n  - effect: NoExecute\r\n    key: node.kubernetes.io/not-ready\r\n    operator: Exists\r\n  - effect: NoExecute\r\n    key: node.kubernetes.io/unreachable\r\n    operator: Exists\r\n  - effect: NoSchedule\r\n    key: node.kubernetes.io/disk-pressure\r\n    operator: Exists\r\n  - effect: NoSchedule\r\n    key: node.kubernetes.io/memory-pressure\r\n    operator: Exists\r\n  volumes:\r\n  - name: default-token-mnjh7\r\n    secret:\r\n      defaultMode: 420\r\n      secretName: default-token-mnjh7\r\nstatus:\r\n  conditions:\r\n  - lastProbeTime: null\r\n    lastTransitionTime: 2018-07-17T15:15:17Z\r\n    status: \"True\"\r\n    type: Initialized\r\n  - lastProbeTime: null\r\n    lastTransitionTime: 2018-07-17T15:15:51Z\r\n    message: 'containers with unready status: [busybox]'\r\n    reason: ContainersNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastProbeTime: null\r\n    lastTransitionTime: null\r\n    message: 'containers with unready status: [busybox]'\r\n    reason: ContainersNotReady\r\n    status: \"False\"\r\n    type: ContainersReady\r\n  - lastProbeTime: null\r\n    lastTransitionTime: 2018-07-17T15:15:17Z\r\n    status: \"True\"\r\n    type: PodScheduled\r\n  containerStatuses:\r\n  - containerID: docker://b9e645e9ba1d21777dd7f868342bc6f4cf4170b931a7dcc5e9bec5de5cbaa7f5\r\n    image: busybox:latest\r\n    imageID: docker-pullable://busybox@sha256:d21b79794850b4b15d8d332b451d95351d14c951542942a816eea69c9e04b240\r\n    lastState: {}\r\n    name: busybox\r\n    ready: false\r\n    restartCount: 0\r\n    state:\r\n      terminated:\r\n        containerID: docker://b9e645e9ba1d21777dd7f868342bc6f4cf4170b931a7dcc5e9bec5de5cbaa7f5\r\n        exitCode: 137\r\n        finishedAt: 2018-07-17T15:15:50Z\r\n        reason: Error\r\n        startedAt: 2018-07-17T15:15:18Z\r\n  hostIP: 10.103.0.201\r\n  phase: Running\r\n  podIP: 192.168.11.46\r\n  qosClass: BestEffort\r\n  startTime: 2018-07-17T15:15:17Z\r\n```\r\nHere is one of the nodes on which pods like these should run:\r\n``` yaml\r\napiVersion: v1\r\nkind: Node\r\nmetadata:\r\n  annotations:\r\n    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\r\n    node.alpha.kubernetes.io/ttl: \"0\"\r\n    volumes.kubernetes.io/controller-managed-attach-detach: \"true\"\r\n  creationTimestamp: 2018-07-17T13:36:31Z\r\n  labels:\r\n    beta.kubernetes.io/arch: amd64\r\n    beta.kubernetes.io/instance-type: g3.8xlarge\r\n    beta.kubernetes.io/os: linux\r\n    failure-domain.beta.kubernetes.io/region: us-east-1\r\n    failure-domain.beta.kubernetes.io/zone: us-east-1c\r\n    kubernetes.io/hostname: ip-10-103-0-201.ec2.internal\r\n  name: ip-10-103-0-201.ec2.internal\r\n  resourceVersion: \"583985\"\r\n  selfLink: /api/v1/nodes/ip-10-103-0-201.ec2.internal\r\n  uid: 6a3e0b7a-89c6-11e8-b5fd-0a5cd3064e60\r\nspec:\r\n  podCIDR: 192.168.11.0/24\r\n  providerID: aws:///us-east-1c/i-07b77dcf1cf44e9f1\r\n  taints:\r\n  - effect: NoSchedule\r\n    key: nvidia.com/gpu\r\n    value: \"true\"\r\nstatus:\r\n  addresses:\r\n  - address: 10.103.0.201\r\n    type: InternalIP\r\n  - address: ip-10-103-0-201.ec2.internal\r\n    type: InternalDNS\r\n  - address: ip-10-103-0-201.ec2.internal\r\n    type: Hostname\r\n  allocatable:\r\n    cpu: \"32\"\r\n    ephemeral-storage: \"5258999800\"\r\n    hugepages-1Gi: \"0\"\r\n    hugepages-2Mi: \"0\"\r\n    memory: 251642948Ki\r\n    nvidia.com/gpu: \"0\"\r\n    pods: \"110\"\r\n  capacity:\r\n    cpu: \"32\"\r\n    ephemeral-storage: 5706380Ki\r\n    hugepages-1Gi: \"0\"\r\n    hugepages-2Mi: \"0\"\r\n    memory: 251745348Ki\r\n    nvidia.com/gpu: \"0\"\r\n    pods: \"110\"\r\n  conditions:\r\n  - lastHeartbeatTime: 2018-07-17T15:57:41Z\r\n    lastTransitionTime: 2018-07-17T13:36:31Z\r\n    message: kubelet has sufficient disk space available\r\n    reason: KubeletHasSufficientDisk\r\n    status: \"False\"\r\n    type: OutOfDisk\r\n  - lastHeartbeatTime: 2018-07-17T15:57:41Z\r\n    lastTransitionTime: 2018-07-17T13:36:31Z\r\n    message: kubelet has sufficient memory available\r\n    reason: KubeletHasSufficientMemory\r\n    status: \"False\"\r\n    type: MemoryPressure\r\n  - lastHeartbeatTime: 2018-07-17T15:57:41Z\r\n    lastTransitionTime: 2018-07-17T13:36:31Z\r\n    message: kubelet has no disk pressure\r\n    reason: KubeletHasNoDiskPressure\r\n    status: \"False\"\r\n    type: DiskPressure\r\n  - lastHeartbeatTime: 2018-07-17T15:57:41Z\r\n    lastTransitionTime: 2018-07-17T13:36:31Z\r\n    message: kubelet has sufficient PID available\r\n    reason: KubeletHasSufficientPID\r\n    status: \"False\"\r\n    type: PIDPressure\r\n  - lastHeartbeatTime: 2018-07-17T15:57:41Z\r\n    lastTransitionTime: 2018-07-17T13:36:47Z\r\n    message: kubelet is posting ready status\r\n    reason: KubeletReady\r\n    status: \"True\"\r\n    type: Ready\r\n  daemonEndpoints:\r\n    kubeletEndpoint:\r\n      Port: 10250\r\n  images:\r\n  - names:\r\n    - quay.io/calico/node@sha256:19fdccdd4a90c4eb0301b280b50389a56e737e2349828d06c7ab397311638d29\r\n    - quay.io/calico/node:v3.1.1\r\n    sizeBytes: 248203187\r\n  - names:\r\n    - quay.io/calico/node@sha256:a35541153f7695b38afada46843c64a2c546548cd8c171f402621736c6cf3f0b\r\n    - quay.io/calico/node:v3.1.3\r\n    sizeBytes: 248202699\r\n  - names:\r\n    - k8s.gcr.io/kube-proxy-amd64@sha256:3c908257f494b60c0913eae6db3d35fa99825d487b2bcf89eed0a7d8e34c1539\r\n    - k8s.gcr.io/kube-proxy-amd64:v1.11.0\r\n    sizeBytes: 97772373\r\n  - names:\r\n    - quay.io/calico/cni@sha256:ed172c28bc193bb09bce6be6ed7dc6bfc85118d55e61d263cee8bbb0fd464a9d\r\n    - quay.io/calico/cni:v3.1.3\r\n    sizeBytes: 68849270\r\n  - names:\r\n    - quay.io/calico/cni@sha256:dc345458d136ad9b4d01864705895e26692d2356de5c96197abff0030bf033eb\r\n    - quay.io/calico/cni:v3.1.1\r\n    sizeBytes: 68844820\r\n  - names:\r\n    - quay.io/calico/typha@sha256:095d040ed75a5c9751f92c5282e8defad9dc66495eb865af5f130b624f612a69\r\n    - quay.io/calico/typha:v0.7.2\r\n    sizeBytes: 56938089\r\n  - names:\r\n    - k8s.gcr.io/nvidia-gpu-device-plugin@sha256:0842734032018be107fa2490c98156992911e3e1f2a21e059ff0105b07dd8e9e\r\n    sizeBytes: 17574483\r\n  - names:\r\n    - busybox@sha256:d21b79794850b4b15d8d332b451d95351d14c951542942a816eea69c9e04b240\r\n    - busybox:latest\r\n    sizeBytes: 1162745\r\n  - names:\r\n    - k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea\r\n    - k8s.gcr.io/pause:3.1\r\n    sizeBytes: 742472\r\n  nodeInfo:\r\n    architecture: amd64\r\n    bootID: 59f59427-8db6-4406-9366-2ced8606477a\r\n    containerRuntimeVersion: docker://18.3.1\r\n    kernelVersion: 4.14.48-coreos-r2\r\n    kubeProxyVersion: v1.11.0\r\n    kubeletVersion: v1.11.0\r\n    machineID: 03523ebe7a23416a81527cf38db8ceb6\r\n    operatingSystem: linux\r\n    osImage: Container Linux by CoreOS 1745.7.0 (Rhyolite)\r\n    systemUUID: EC2D5427-2D56-79D5-AD7C-27F7511AA436\r\n```\r\nThere some preceding discussion in the \"sig-node\" channel of the \"Kubernetes\" Slack team [starting on Sunday, 15 July 2018](https://kubernetes.slack.com/archives/C0BP8PW9G/p1531673863000087).\r\nPossibly related issues: #22205, #61886\r\nPossibly related PRs: #28803\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):  \r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"11\", GitVersion:\"v1.11.0\", GitCommit:\"91e7b4fd31fcd3d5f436da26c980becec37ceefe\", GitTreeState:\"clean\", BuildDate:\"2018-06-27T22:29:25Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"11\", GitVersion:\"v1.11.0\", GitCommit:\"91e7b4fd31fcd3d5f436da26c980becec37ceefe\", GitTreeState:\"clean\", BuildDate:\"2018-06-27T20:08:34Z\", GoVersion:\"go1.10.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n- Cloud provider or hardware configuration:  \r\n  AWS EC2 (g3.8xlarge instance, but other instance types exhibit the same behavior)\r\n\r\n- OS (e.g. from /etc/os-release):  \r\n```\r\nNAME=\"Container Linux by CoreOS\"\r\nID=coreos\r\nVERSION=1745.7.0\r\nVERSION_ID=1745.7.0\r\nBUILD_ID=2018-06-14-0909\r\nPRETTY_NAME=\"Container Linux by CoreOS 1745.7.0 (Rhyolite)\"\r\nANSI_COLOR=\"38;5;75\"\r\nHOME_URL=\"https://coreos.com/\"\r\nBUG_REPORT_URL=\"https://issues.coreos.com\"\r\nCOREOS_BOARD=\"amd64-usr\"\r\n```\r\n\r\n- Kernel (e.g. `uname -a`):  \r\n```\r\nLinux ip-10-103-0-201 4.14.48-coreos-r2 #1 SMP Thu Jun 14 08:23:03 UTC 2018 x86_64 Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz GenuineIntel GNU/Linux\r\n```\r\n\r\n- Install tools: _kubeadm init_, _kubeadm join_",
  "closed_at": "2018-08-07T21:28:00Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/13653959?v=4",
    "events_url": "https://api.github.com/users/k8s-github-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-github-robot/followers",
    "following_url": "https://api.github.com/users/k8s-github-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-github-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-github-robot",
    "id": 13653959,
    "login": "k8s-github-robot",
    "node_id": "MDQ6VXNlcjEzNjUzOTU5",
    "organizations_url": "https://api.github.com/users/k8s-github-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-github-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-github-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-github-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-github-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-github-robot"
  },
  "comments": 6,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/66298/comments",
  "created_at": "2018-07-17T16:06:12Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/66298/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/66298",
  "id": 341985870,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Apps.",
      "id": 404091735,
      "name": "sig/apps",
      "node_id": "MDU6TGFiZWw0MDQwOTE3MzU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/apps"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Node.",
      "id": 173493665,
      "name": "sig/node",
      "node_id": "MDU6TGFiZWwxNzM0OTM2NjU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/node"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Scheduling.",
      "id": 125550211,
      "name": "sig/scheduling",
      "node_id": "MDU6TGFiZWwxMjU1NTAyMTE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/scheduling"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/66298/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUzNDE5ODU4NzA=",
  "number": 66298,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "DaemonSet with node affinity for \"dynamic\" labels only works with one candidate value",
  "updated_at": "2018-08-07T21:28:00Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/66298",
  "user": {
    "avatar_url": "https://avatars1.githubusercontent.com/u/175841?v=4",
    "events_url": "https://api.github.com/users/seh/events{/privacy}",
    "followers_url": "https://api.github.com/users/seh/followers",
    "following_url": "https://api.github.com/users/seh/following{/other_user}",
    "gists_url": "https://api.github.com/users/seh/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/seh",
    "id": 175841,
    "login": "seh",
    "node_id": "MDQ6VXNlcjE3NTg0MQ==",
    "organizations_url": "https://api.github.com/users/seh/orgs",
    "received_events_url": "https://api.github.com/users/seh/received_events",
    "repos_url": "https://api.github.com/users/seh/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/seh/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/seh/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/seh"
  }
}