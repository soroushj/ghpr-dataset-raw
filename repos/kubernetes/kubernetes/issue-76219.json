{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "MEMBER",
  "body": "Context\r\n======\r\n Kubernetes 1.14, ServerSideApply feature enabled on kube-apiserver.\r\n\r\nIntroduction\r\n=========\r\n\r\nServer-Side Apply (SSA) seems to be triggering a performance issue on protobuf serialization of objects, specifically while serializing the new `managedFields` recursive map. We detected the problem because the scalability tests mostly indicated problems in LISTing objects (pods and nodes), even though SSA shouldn't impact this code path.\r\n\r\nAt this point the problems are probably related to the size of the objects (increased significantly by the additional field) which would lead to:\r\n- more serialization/deserialization, \r\n- more data transfer between client and apiserver, \r\n- more data transfer between apiserver and etcd\r\n\r\nProbably many other things, let's try to reproduce.\r\n\r\nSteps to reproduce\r\n===============\r\nI'm now comparing the following to set-up:\r\n\r\n```bash\r\n# Running without the feature enabled\r\nSTORAGE_MEDIA_TYPE=application/vnd.kubernetes.protobuf \\ \r\nFEATURE_GATES=ServerSideApply=false \\\r\nPATH=$PWD/third_party/etcd:$PATH \\\r\nhack/local-up-cluster.sh -O\r\n\r\n# Running with the feature enabled\r\nSTORAGE_MEDIA_TYPE=application/vnd.kubernetes.protobuf \\\r\nFEATURE_GATES=ServerSideApply=true \\\r\nPATH=$PWD/third_party/etcd:$PATH \\\r\nhack/local-up-cluster.sh -O\r\n```\r\n\r\nI first create a random deployment with a 1000 replicas, the pods don't have to actually start, but should be successfully created.\r\n\r\nMy benchmark is to run the following python3 script:\r\n```python\r\nimport urllib.request\r\nimport time\r\nimport statistics\r\n\r\nrequest = urllib.request.Request(\r\n    \"http://localhost:8080/api/v1/pods\",\r\n    headers={\r\n        \"Accept\": \"application/vnd.kubernetes.protobuf\",\r\n    }\r\n)\r\n\r\nsize = []\r\nduration = []\r\nNUM_RUNS = 50\r\nfor _ in range(NUM_RUNS):\r\n  b = time.perf_counter()\r\n  size += [len(urllib.request.urlopen(request).read())]\r\n  duration += [(time.perf_counter() - b) * 1000]\r\n\r\nprint(\"%d runs:\" % NUM_RUNS)\r\nprint(\"size min/max/avg/stddev: %d/%d/%d/%d\" % (\r\n    min(size), max(size), statistics.mean(size), statistics.stdev(size)\r\n))\r\nprint(\"duration min/max/avg/stddev: %d/%d/%d/%d\" % (\r\n    min(duration),\r\n    max(duration),\r\n    statistics.mean(duration),\r\n    statistics.stdev(duration)\r\n))\r\n```\r\n\r\nWithout the feature-flag, the results look like this (YMMV):\r\n```bash\r\n> python3 test.py \r\n50 runs:\r\nsize min/max/avg/stddev: 980436/980470/980450/17\r\nduration min/max/avg/stddev: 22/47/31/5\r\n```\r\n\r\nwith the feature-flag, the results looks like this:\r\n```bash\r\n50 runs:\r\nsize min/max/avg/stddev: 2465211/2465211/2465211/0\r\nduration min/max/avg/stddev: 97/146/110/13\r\n``` \r\n\r\nInvestigation\r\n==========\r\n\r\nSince we can see that there is a clear difference between the two, let's try to profile the difference.\r\n\r\nLet's send queries infinitely to the apiserver so we can see what it's doing:\r\n```bash\r\nwhile true\r\ndo curl --silent -H \"Accept: application/vnd.kubernetes.protobuf\" \\\r\n  http://localhost:8080/api/v1/pods >/dev/null\r\ndone\r\n```\r\n\r\nIn a different window, start profiling:\r\n```bash\r\ngo tool pprof http://localhost:8080/debug/pprof/profile?seconds=30\r\n... (wait)\r\n> kcachegrind # If you don't know this, it's absolutely awesome.\r\n```\r\n\r\n![Screenshot from 2019-04-05 15-03-13](https://user-images.githubusercontent.com/1929644/55658842-fc969400-57b3-11e9-99a6-0303fc4c3650.png)\r\n\r\nLet's look at that method:\r\nhttps://github.com/kubernetes/kubernetes/blob/v1.14.0/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go#L2107-L2120\r\n\r\nIt basically does a full tree traversal to know how big the buffer needs to be allocated, that's OK.\r\n\r\nThe problem is that this method is called again and again at every depth of the tree while serializing the map:\r\nhttps://github.com/kubernetes/kubernetes/blob/v1.14.0/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go#L737-L774\r\n\r\nwhich means that the marshaling complexity is going to be proportional to the square of the depth of the map.\r\n\r\nMy suspicion thought is that this problem is not limited to \"maps\", but to all objects we serialize in kubernetes, let's look at it:\r\nTaking a random other types from the same generated files (but you can look at any other protobuf generated files, they always do the same thing):\r\nhttps://github.com/kubernetes/kubernetes/blob/v1.14.0/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go#L1672-L1711\r\n\r\nWe can see that it's computing the size of the entire sub-object (by recursing as deeply as it can) before serializing each individual field, which are all  going to recurse AGAIN the entire object, and again and again ...\r\n\r\nDetecting these with profiling is much harder because they don't all call the same method (each field has a different type). Running on the recursive map made it obvious because all the calls are aggregated under the same function.\r\n\r\nSolutions\r\n=======\r\nMarshaling protobuf objects requires to marshal the size of sub-objects before the actual bytes of these objects. Also this size is marshaled as a \"varint\", which prevents you from computing the bytes and then update the size in place. You would have to copy the data very frequently in order to do that. \r\n\r\nAn alternate, good solution is to memoize the size of each object so that you don't have to re-compute them very frequently. Interestingly, https://github.com/golang/protobuf does add a `XXX_CacheSize int` variable in go objects when it generates the go code from proto files. Ideally, the memoization would be \"local\" to the individual serialization process to avoid problems of invalidating the cache, running multiple serialization at the same time, or modifying objects themselves.\r\n\r\nNext step would probably be to come-up with a relevant experiment to see how bad this actually is, both on the SSA map and on the rest of the objects.",
  "closed_at": "2019-07-25T23:01:15Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 20,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/76219/comments",
  "created_at": "2019-04-05T22:09:36Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/76219/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/76219",
  "id": 429952927,
  "labels": [
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d3e2f0",
      "default": false,
      "description": "Indicates that an issue or PR should not be auto-closed due to staleness.",
      "id": 778118403,
      "name": "lifecycle/frozen",
      "node_id": "MDU6TGFiZWw3NzgxMTg0MDM=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/lifecycle/frozen"
    },
    {
      "color": "eb6420",
      "default": false,
      "description": "Must be staffed and worked on either currently, or very soon, ideally in time for the next release.",
      "id": 114528223,
      "name": "priority/important-soon",
      "node_id": "MDU6TGFiZWwxMTQ1MjgyMjM=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/priority/important-soon"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG API Machinery.",
      "id": 173493835,
      "name": "sig/api-machinery",
      "node_id": "MDU6TGFiZWwxNzM0OTM4MzU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/api-machinery"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Scalability.",
      "id": 125010198,
      "name": "sig/scalability",
      "node_id": "MDU6TGFiZWwxMjUwMTAxOTg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/scalability"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/76219/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU0Mjk5NTI5Mjc=",
  "number": 76219,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "Server-side apply protobuf serialization performance issue",
  "updated_at": "2019-07-25T23:01:15Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/76219",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/1929644?v=4",
    "events_url": "https://api.github.com/users/apelisse/events{/privacy}",
    "followers_url": "https://api.github.com/users/apelisse/followers",
    "following_url": "https://api.github.com/users/apelisse/following{/other_user}",
    "gists_url": "https://api.github.com/users/apelisse/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/apelisse",
    "id": 1929644,
    "login": "apelisse",
    "node_id": "MDQ6VXNlcjE5Mjk2NDQ=",
    "organizations_url": "https://api.github.com/users/apelisse/orgs",
    "received_events_url": "https://api.github.com/users/apelisse/received_events",
    "repos_url": "https://api.github.com/users/apelisse/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/apelisse/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/apelisse/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/apelisse"
  }
}