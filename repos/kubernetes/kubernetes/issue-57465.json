{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "/kind bug\r\n\r\n\r\n**What happened**:\r\n\r\nDuring pod teardown, we sometimes see the following error-level kubelet logs:\r\n\r\n```\r\ntime=\"2017-12-20T15:42:07Z\" level=info msg=\"Released address using workloadID\" Workload=mynamespace.my-cron-pz1b7\r\nCalico CNI deleting device in netns /proc/115577/ns/net\r\nE1220 15:42:09.401448    1396 cni.go:278] Error deleting network: failed to Statfs \"/proc/115577/ns/net\": no such file or directory\r\nE1220 15:42:09.554117    1396 remote_runtime.go:109] StopPodSandbox \"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\" from runtime service failed: rpc error: code = 2 desc = NetworkPlugin cni failed to teardown pod \"my-cron-pz1b7_mynamespace\" network: failed to Statfs \"/proc/115577/ns/net\": no such file or directory\r\nE1220 15:42:09.554162    1396 kuberuntime_manager.go:784] Failed to stop sandbox {\"docker\" \"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\"}\r\nE1220 15:42:09.554192    1396 kubelet_pods.go:920] Failed killing the pod \"my-cron-pz1b7\": failed to \"KillPodSandbox\" for \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" with KillPodSandboxError: \"rpc error: code = 2 desc = NetworkPlugin cni failed to teardown pod \\\"my-cron-pz1b7_mynamespace\\\" network: failed to Statfs \\\"/proc/115577/ns/net\\\": no such file or directory\"\r\n```\r\n\r\n**What you expected to happen**:\r\n\r\nClean pod shutdown, no error-level logs.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nWe've got two kube crons that run every minute, across 12 regions in production -- so 34560 related pods daily -- and we see this about 3-12 times per day.\r\n\r\n**Anything else we need to know?**:\r\n\r\nRelevant kubelet log context of a cron pod that manifested this error:\r\n```\r\nI1220 15:42:02.636144    1396 kubelet.go:1808] SyncLoop (ADD, \"api\"): \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\"\r\nI1220 15:42:02.824020    1396 reconciler.go:242] VerifyControllerAttachedVolume operation started for volume \"kubernetes.io/host-path/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-etc-ssh\" (spec.Name: \"etc-ssh\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\")\r\nI1220 15:42:02.824039    1396 reconciler.go:242] VerifyControllerAttachedVolume operation started for volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (spec.Name: \"configmap-mount\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\")\r\nI1220 15:42:02.824205    1396 reconciler.go:242] VerifyControllerAttachedVolume operation started for volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (spec.Name: \"default-token-wphxs\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\")\r\nI1220 15:42:02.924419    1396 reconciler.go:318] MountVolume operation started for volume \"kubernetes.io/host-path/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-etc-ssh\" (spec.Name: \"etc-ssh\") to pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:02.924479    1396 reconciler.go:318] MountVolume operation started for volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (spec.Name: \"configmap-mount\") to pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:02.924637    1396 reconciler.go:318] MountVolume operation started for volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (spec.Name: \"default-token-wphxs\") to pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:02.927428    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/host-path/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-etc-ssh\" (spec.Name: \"etc-ssh\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:02.932912    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (spec.Name: \"default-token-wphxs\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:02.935035    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (spec.Name: \"configmap-mount\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:04.230307    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (spec.Name: \"default-token-wphxs\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:04.233500    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (spec.Name: \"configmap-mount\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:04.459932    1396 kuberuntime_manager.go:370] No sandbox for pod \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\" can be found. Need to start a new one\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Extracted identifiers\" Node=knode7 Orchestrator=k8s Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Loaded CNI NetConf\" NetConfg={ calico-k8s-network calico { calico-ipam  <nil> <nil> [] []} 0     http://127.0.0.1:4001 info {     } { kubeconfig } {{{ {[]}}}}    } Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Configured environment: [LANG=en_US LANGUAGE=en_US: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HYPERKUBE_IMAGE=myregistry:5000/hyperkube:v1.6.8 PAUSE_IMAGE=myregistry:5000/pause:3.0 API_ENDPOINT=1.2.3.4 CADVISOR_PORT=4194 KUBE_API_SECURE_PORT=6443 KUBE_CONFIG=kubeconfig KUBELET_PORT=10250 LOGLEVEL=2 POD_MANIFEST_DIR=/etc/kubernetes/manifests SELF_HOSTNAME=knode7 CNI_COMMAND=ADD CNI_CONTAINERID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_NETNS=/proc/115577/ns/net CNI_ARGS=IgnoreUnknown=1;IgnoreUnknown=1;K8S_POD_NAMESPACE=mynamespace;K8S_POD_NAME=my-cron-123-1513784520-pz1b7;K8S_POD_INFRA_CONTAINER_ID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_IFNAME=eth0 CNI_PATH=/opt/cni/bin:/opt/calico/bin ETCD_ENDPOINTS=http://127.0.0.1:4001 KUBECONFIG=kubeconfig]\"\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Extracted identifiers for CmdAddK8s\" Node=knode7 Orchestrator=k8s Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Configured environment: [LANG=en_US LANGUAGE=en_US: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HYPERKUBE_IMAGE=myregistry:5000/hyperkube:v1.6.8 PAUSE_IMAGE=myregistry:5000/pause:3.0 API_ENDPOINT=1.2.3.4 CADVISOR_PORT=4194 KUBE_API_SECURE_PORT=6443 KUBE_CONFIG=kubeconfig KUBELET_PORT=10250 LOGLEVEL=2 POD_MANIFEST_DIR=/etc/kubernetes/manifests SELF_HOSTNAME=knode7 CNI_COMMAND=ADD CNI_CONTAINERID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_NETNS=/proc/115577/ns/net CNI_ARGS=IgnoreUnknown=1;IgnoreUnknown=1;K8S_POD_NAMESPACE=mynamespace;K8S_POD_NAME=my-cron-123-1513784520-pz1b7;K8S_POD_INFRA_CONTAINER_ID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_IFNAME=eth0 CNI_PATH=/opt/cni/bin:/opt/calico/bin ETCD_ENDPOINTS=http://127.0.0.1:4001 KUBECONFIG=kubeconfig]\"\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Auto assigning IP\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 assignArgs={1 0 0xc420306270 map[]  [] []}\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Creating new handle: mynamespace.my-cron-123-1513784520-pz1b7\"\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Decremented handle 'mynamespace.my-cron-123-1513784520-pz1b7' by 1\"\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Creating new handle: mynamespace.my-cron-123-1513784520-pz1b7\"\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"IPAM Result\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 result.IPs=[{Version:4 Interface:0 Address:{IP:1.2.3.5 Mask:ffffffff} Gateway:<nil>}]\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Populated endpoint\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 endpoint=&{{workloadEndpoint v1} {{<nil>} eth0 mynamespace.my-cron-123-1513784520-pz1b7 k8s knode7 159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e map[]} {[1.2.3.5/32] [] <nil> <nil> [calico-k8s-network]  <nil>}}\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Added Mac and interface name to endpoint\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 endpoint=&{{workloadEndpoint v1} {{<nil>} eth0 mynamespace.my-cron-123-1513784520-pz1b7 k8s knode7 159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e map[]} {[1.2.3.5/32] [] <nil> <nil> [calico-k8s-network] cali2c05b40c93f e6:5c:b8:41:b5:04}}\r\ntime=\"2017-12-20T15:42:04Z\" level=info msg=\"Wrote updated endpoint to datastore\" Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\nI1220 15:42:05.007393    1396 kubelet.go:1842] SyncLoop (PLEG): \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\", event: &pleg.PodLifecycleEvent{ID:\"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\", Type:\"ContainerStarted\", Data:\"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\"}\r\nI1220 15:42:06.047444    1396 kubelet.go:1842] SyncLoop (PLEG): \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\", event: &pleg.PodLifecycleEvent{ID:\"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\", Type:\"ContainerStarted\", Data:\"f056efb76b15d038a4f30b3e42f8d52557e83da7a7e9391e12e81bc261e0cf0a\"}\r\nI1220 15:42:06.146032    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (spec.Name: \"default-token-wphxs\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:06.147526    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (spec.Name: \"configmap-mount\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:07.063207    1396 kubelet.go:1842] SyncLoop (PLEG): \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\", event: &pleg.PodLifecycleEvent{ID:\"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\", Type:\"ContainerDied\", Data:\"f056efb76b15d038a4f30b3e42f8d52557e83da7a7e9391e12e81bc261e0cf0a\"}\r\nI1220 15:42:07.125113    1396 kubelet_pods.go:917] Killing unwanted pod \"my-cron-123-1513784520-pz1b7\"\r\nI1220 15:42:07.129145    1396 kuberuntime_container.go:545] Killing container \"docker://f056efb76b15d038a4f30b3e42f8d52557e83da7a7e9391e12e81bc261e0cf0a\" with 30 second grace period\r\nI1220 15:42:07.155666    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (spec.Name: \"configmap-mount\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:07.156450    1396 operation_generator.go:597] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (spec.Name: \"default-token-wphxs\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:07.243117    1396 reconciler.go:201] UnmountVolume operation started for volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (spec.Name: \"configmap-mount\") from pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:07.243520    1396 reconciler.go:201] UnmountVolume operation started for volume \"kubernetes.io/host-path/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-etc-ssh\" (spec.Name: \"etc-ssh\") from pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:07.243801    1396 reconciler.go:201] UnmountVolume operation started for volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (spec.Name: \"default-token-wphxs\") from pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\").\r\nI1220 15:42:07.245697    1396 operation_generator.go:672] UnmountVolume.TearDown succeeded for volume \"kubernetes.io/host-path/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-etc-ssh\" (OuterVolumeSpecName: \"etc-ssh\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\"). InnerVolumeSpecName \"etc-ssh\". PluginName \"kubernetes.io/host-path\", VolumeGidValue \"\"\r\nI1220 15:42:07.252149    1396 operation_generator.go:672] UnmountVolume.TearDown succeeded for volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (OuterVolumeSpecName: \"configmap-mount\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\"). InnerVolumeSpecName \"configmap-mount\". PluginName \"kubernetes.io/configmap\", VolumeGidValue \"\"\r\ntime=\"2017-12-20T15:42:07Z\" level=info msg=\"Extracted identifiers\" Node=knode7 Orchestrator=k8s Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:42:07Z\" level=info msg=\"Configured environment: [LANG=en_US LANGUAGE=en_US: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HYPERKUBE_IMAGE=myregistry:5000/hyperkube:v1.6.8 PAUSE_IMAGE=myregistry:5000/pause:3.0 API_ENDPOINT=1.2.3.4 CADVISOR_PORT=4194 KUBE_API_SECURE_PORT=6443 KUBE_CONFIG=kubeconfig KUBELET_PORT=10250 LOGLEVEL=2 POD_MANIFEST_DIR=/etc/kubernetes/manifests SELF_HOSTNAME=knode7 CNI_COMMAND=DEL CNI_CONTAINERID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_NETNS=/proc/115577/ns/net CNI_ARGS=IgnoreUnknown=1;IgnoreUnknown=1;K8S_POD_NAMESPACE=mynamespace;K8S_POD_NAME=my-cron-123-1513784520-pz1b7;K8S_POD_INFRA_CONTAINER_ID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_IFNAME=eth0 CNI_PATH=/opt/cni/bin:/opt/calico/bin ETCD_ENDPOINTS=http://127.0.0.1:4001 KUBECONFIG=kubeconfig]\"\r\nI1220 15:42:07.265518    1396 operation_generator.go:672] UnmountVolume.TearDown succeeded for volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (OuterVolumeSpecName: \"default-token-wphxs\") pod \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" (UID: \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\"). InnerVolumeSpecName \"default-token-wphxs\". PluginName \"kubernetes.io/secret\", VolumeGidValue \"\"\r\nI1220 15:42:07.344049    1396 reconciler.go:363] Detached volume \"kubernetes.io/host-path/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-etc-ssh\" (spec.Name: \"etc-ssh\") devicePath: \"\"\r\nI1220 15:42:07.344187    1396 reconciler.go:363] Detached volume \"kubernetes.io/secret/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-default-token-wphxs\" (spec.Name: \"default-token-wphxs\") devicePath: \"\"\r\nI1220 15:42:07.344202    1396 reconciler.go:363] Detached volume \"kubernetes.io/configmap/52d9b9b4-e59c-11e7-b6c8-005056ae60f5-configmap-mount\" (spec.Name: \"configmap-mount\") devicePath: \"\"\r\ntime=\"2017-12-20T15:42:07Z\" level=info msg=\"Configured environment: [LANG=en_US LANGUAGE=en_US: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HYPERKUBE_IMAGE=myregistry:5000/hyperkube:v1.6.8 PAUSE_IMAGE=myregistry:5000/pause:3.0 API_ENDPOINT=1.2.3.4 CADVISOR_PORT=4194 KUBE_API_SECURE_PORT=6443 KUBE_CONFIG=kubeconfig KUBELET_PORT=10250 LOGLEVEL=2 POD_MANIFEST_DIR=/etc/kubernetes/manifests SELF_HOSTNAME=knode7 CNI_COMMAND=DEL CNI_CONTAINERID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_NETNS=/proc/115577/ns/net CNI_ARGS=IgnoreUnknown=1;IgnoreUnknown=1;K8S_POD_NAMESPACE=mynamespace;K8S_POD_NAME=my-cron-123-1513784520-pz1b7;K8S_POD_INFRA_CONTAINER_ID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_IFNAME=eth0 CNI_PATH=/opt/cni/bin:/opt/calico/bin ETCD_ENDPOINTS=http://127.0.0.1:4001 KUBECONFIG=kubeconfig]\"\r\ntime=\"2017-12-20T15:42:07Z\" level=info msg=\"Releasing address using workloadID\" Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:42:07Z\" level=info msg=\"Releasing all IPs with handle 'mynamespace.my-cron-123-1513784520-pz1b7'\"\r\ntime=\"2017-12-20T15:42:07Z\" level=info msg=\"Decremented handle 'mynamespace.my-cron-123-1513784520-pz1b7' by 1\"\r\ntime=\"2017-12-20T15:42:07Z\" level=info msg=\"Released address using workloadID\" Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\nCalico CNI deleting device in netns /proc/115577/ns/net\r\nE1220 15:42:09.401448    1396 cni.go:278] Error deleting network: failed to Statfs \"/proc/115577/ns/net\": no such file or directory\r\nE1220 15:42:09.554117    1396 remote_runtime.go:109] StopPodSandbox \"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\" from runtime service failed: rpc error: code = 2 desc = NetworkPlugin cni failed to teardown pod \"my-cron-123-1513784520-pz1b7_mynamespace\" network: failed to Statfs \"/proc/115577/ns/net\": no such file or directory\r\nE1220 15:42:09.554162    1396 kuberuntime_manager.go:784] Failed to stop sandbox {\"docker\" \"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\"}\r\nE1220 15:42:09.554192    1396 kubelet_pods.go:920] Failed killing the pod \"my-cron-123-1513784520-pz1b7\": failed to \"KillPodSandbox\" for \"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\" with KillPodSandboxError: \"rpc error: code = 2 desc = NetworkPlugin cni failed to teardown pod \\\"my-cron-123-1513784520-pz1b7_mynamespace\\\" network: failed to Statfs \\\"/proc/115577/ns/net\\\": no such file or directory\"\r\nW1220 15:42:10.108168    1396 docker_sandbox.go:263] NetworkPlugin cni failed on the status hook for pod \"my-cron-123-1513784520-pz1b7_mynamespace\": Cannot find the network namespace, skipping pod network status for container {\"docker\" \"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\"}\r\nI1220 15:42:10.112898    1396 kubelet.go:1842] SyncLoop (PLEG): \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\", event: &pleg.PodLifecycleEvent{ID:\"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\", Type:\"ContainerDied\", Data:\"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\"}\r\nW1220 15:42:10.112936    1396 pod_container_deletor.go:77] Container \"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\" not found in pod's containers\r\nI1220 15:42:11.125066    1396 kubelet_pods.go:917] Killing unwanted pod \"my-cron-123-1513784520-pz1b7\"\r\nW1220 15:42:11.127222    1396 docker_sandbox.go:263] NetworkPlugin cni failed on the status hook for pod \"my-cron-123-1513784520-pz1b7_mynamespace\": Cannot find the network namespace, skipping pod network status for container {\"docker\" \"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\"}\r\ntime=\"2017-12-20T15:42:11Z\" level=info msg=\"Extracted identifiers\" Node=knode7 Orchestrator=k8s Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:42:11Z\" level=info msg=\"Configured environment: [LANG=en_US LANGUAGE=en_US: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HYPERKUBE_IMAGE=myregistry:5000/hyperkube:v1.6.8 PAUSE_IMAGE=myregistry:5000/pause:3.0 API_ENDPOINT=1.2.3.4 CADVISOR_PORT=4194 KUBE_API_SECURE_PORT=6443 KUBE_CONFIG=kubeconfig KUBELET_PORT=10250 LOGLEVEL=2 POD_MANIFEST_DIR=/etc/kubernetes/manifests SELF_HOSTNAME=knode7 CNI_COMMAND=DEL CNI_CONTAINERID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_NETNS= CNI_ARGS=IgnoreUnknown=1;IgnoreUnknown=1;K8S_POD_NAMESPACE=mynamespace;K8S_POD_NAME=my-cron-123-1513784520-pz1b7;K8S_POD_INFRA_CONTAINER_ID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_IFNAME=eth0 CNI_PATH=/opt/cni/bin:/opt/calico/bin ETCD_ENDPOINTS=http://127.0.0.1:4001 KUBECONFIG=kubeconfig]\"\r\ntime=\"2017-12-20T15:42:11Z\" level=warning msg=\"WorkloadEndpoint does not exist in the datastore, moving forward with the clean up\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 WorkloadEndpoint={{<nil>} eth0 mynamespace.my-cron-123-1513784520-pz1b7 k8s knode7  map[]}\r\ntime=\"2017-12-20T15:42:11Z\" level=info msg=\"Configured environment: [LANG=en_US LANGUAGE=en_US: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HYPERKUBE_IMAGE=myregistry:5000/hyperkube:v1.6.8 PAUSE_IMAGE=myregistry:5000/pause:3.0 API_ENDPOINT=1.2.3.4 CADVISOR_PORT=4194 KUBE_API_SECURE_PORT=6443 KUBE_CONFIG=kubeconfig KUBELET_PORT=10250 LOGLEVEL=2 POD_MANIFEST_DIR=/etc/kubernetes/manifests SELF_HOSTNAME=knode7 CNI_COMMAND=DEL CNI_CONTAINERID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_NETNS= CNI_ARGS=IgnoreUnknown=1;IgnoreUnknown=1;K8S_POD_NAMESPACE=mynamespace;K8S_POD_NAME=my-cron-123-1513784520-pz1b7;K8S_POD_INFRA_CONTAINER_ID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_IFNAME=eth0 CNI_PATH=/opt/cni/bin:/opt/calico/bin ETCD_ENDPOINTS=http://127.0.0.1:4001 KUBECONFIG=kubeconfig]\"\r\ntime=\"2017-12-20T15:42:11Z\" level=info msg=\"Releasing address using workloadID\" Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:42:11Z\" level=info msg=\"Releasing all IPs with handle 'mynamespace.my-cron-123-1513784520-pz1b7'\"\r\ntime=\"2017-12-20T15:42:11Z\" level=warning msg=\"Asked to release address but it doesn't exist. Ignoring\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 workloadId=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:42:11Z\" level=info msg=\"Teardown processing complete.\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 endpoint=<nil>\r\nE1220 15:44:07.063669    1396 kubelet.go:1549] Unable to mount volumes for pod \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\": timeout expired waiting for volumes to attach/mount for pod \"mynamespace\"/\"my-cron-123-1513784520-pz1b7\". list of unattached/unmounted volumes=[etc-ssh configmap-mount default-token-wphxs]; skipping pod\r\nE1220 15:44:07.063698    1396 pod_workers.go:182] Error syncing pod 52d9b9b4-e59c-11e7-b6c8-005056ae60f5 (\"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\"), skipping: timeout expired waiting for volumes to attach/mount for pod \"mynamespace\"/\"my-cron-123-1513784520-pz1b7\". list of unattached/unmounted volumes=[etc-ssh configmap-mount default-token-wphxs]\r\nI1220 15:44:14.104154    1396 kubelet.go:1824] SyncLoop (DELETE, \"api\"): \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\"\r\nI1220 15:44:14.110689    1396 kubelet.go:1818] SyncLoop (REMOVE, \"api\"): \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\"\r\nI1220 15:44:14.110726    1396 kubelet.go:2002] Failed to delete pod \"my-cron-123-1513784520-pz1b7_mynamespace(52d9b9b4-e59c-11e7-b6c8-005056ae60f5)\", err: pod not found\r\nW1220 15:45:01.989628    1396 docker_sandbox.go:263] NetworkPlugin cni failed on the status hook for pod \"my-cron-123-1513784520-pz1b7_mynamespace\": Cannot find the network namespace, skipping pod network status for container {\"docker\" \"159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e\"}\r\ntime=\"2017-12-20T15:45:02Z\" level=info msg=\"Extracted identifiers\" Node=knode7 Orchestrator=k8s Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:45:02Z\" level=info msg=\"Configured environment: [LANG=en_US LANGUAGE=en_US: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HYPERKUBE_IMAGE=myregistry:5000/hyperkube:v1.6.8 PAUSE_IMAGE=myregistry:5000/pause:3.0 API_ENDPOINT=1.2.3.4 CADVISOR_PORT=4194 KUBE_API_SECURE_PORT=6443 KUBE_CONFIG=kubeconfig KUBELET_PORT=10250 LOGLEVEL=2 POD_MANIFEST_DIR=/etc/kubernetes/manifests SELF_HOSTNAME=knode7 CNI_COMMAND=DEL CNI_CONTAINERID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_NETNS= CNI_ARGS=IgnoreUnknown=1;IgnoreUnknown=1;K8S_POD_NAMESPACE=mynamespace;K8S_POD_NAME=my-cron-123-1513784520-pz1b7;K8S_POD_INFRA_CONTAINER_ID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_IFNAME=eth0 CNI_PATH=/opt/cni/bin:/opt/calico/bin ETCD_ENDPOINTS=http://127.0.0.1:4001 KUBECONFIG=kubeconfig]\"\r\ntime=\"2017-12-20T15:45:02Z\" level=warning msg=\"WorkloadEndpoint does not exist in the datastore, moving forward with the clean up\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 WorkloadEndpoint={{<nil>} eth0 mynamespace.my-cron-123-1513784520-pz1b7 k8s knode7  map[]}\r\ntime=\"2017-12-20T15:45:02Z\" level=info msg=\"Configured environment: [LANG=en_US LANGUAGE=en_US: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HYPERKUBE_IMAGE=myregistry:5000/hyperkube:v1.6.8 PAUSE_IMAGE=myregistry:5000/pause:3.0 API_ENDPOINT=1.2.3.4 CADVISOR_PORT=4194 KUBE_API_SECURE_PORT=6443 KUBE_CONFIG=kubeconfig KUBELET_PORT=10250 LOGLEVEL=2 POD_MANIFEST_DIR=/etc/kubernetes/manifests SELF_HOSTNAME=knode7 CNI_COMMAND=DEL CNI_CONTAINERID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_NETNS= CNI_ARGS=IgnoreUnknown=1;IgnoreUnknown=1;K8S_POD_NAMESPACE=mynamespace;K8S_POD_NAME=my-cron-123-1513784520-pz1b7;K8S_POD_INFRA_CONTAINER_ID=159b8a23dccc9a3e8747309b4ba0e968493cc216197dc44046ff0e688989074e CNI_IFNAME=eth0 CNI_PATH=/opt/cni/bin:/opt/calico/bin ETCD_ENDPOINTS=http://127.0.0.1:4001 KUBECONFIG=kubeconfig]\"\r\ntime=\"2017-12-20T15:45:02Z\" level=info msg=\"Releasing address using workloadID\" Workload=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:45:02Z\" level=info msg=\"Releasing all IPs with handle 'mynamespace.my-cron-123-1513784520-pz1b7'\"\r\ntime=\"2017-12-20T15:45:02Z\" level=warning msg=\"Asked to release address but it doesn't exist. Ignoring\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 workloadId=mynamespace.my-cron-123-1513784520-pz1b7\r\ntime=\"2017-12-20T15:45:02Z\" level=info msg=\"Teardown processing complete.\" Workload=mynamespace.my-cron-123-1513784520-pz1b7 endpoint=<nil>\r\n```\r\n\r\nController manager for the pod:\r\n```\r\nI1220 15:42:02.588094       1 event.go:217] Event(v1.ObjectReference{Kind:\"Job\", Namespace:\"mynamespace\", Name:\"my-cron-123-1513784520\", UID:\"52d4c52e-e59c-11e7-b6c8-005056ae60f5\", APIVersion:\"batch\", ResourceVersion:\"104511222\", FieldPath:\"\"}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: my-cron-123-1513784520-pz1b7\r\nI1220 15:44:14.091244       1 cronjob_controller.go:370] CronJob controller is deleting Pod mynamespace/my-cron-123-1513784520-pz1b7\r\n```\r\n\r\nScheduler for the pod:\r\n```\r\nI1220 15:42:02.630681       1 event.go:217] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"mynamespace\", Name:\"my-cron-123-1513784520-pz1b7\", UID:\"52d9b9b4-e59c-11e7-b6c8-005056ae60f5\", APIVersion:\"v1\", ResourceVersion:\"104511225\", FieldPath:\"\"}): type: 'Normal' reason: 'Scheduled' Successfully assigned my-cron-123-1513784520-pz1b7 to knode7\r\n```\r\n\r\nCron yaml:\r\n```\r\napiVersion: batch/v2alpha1\r\nkind: CronJob\r\nmetadata:\r\n  annotations:\r\n    kubernetes.io/change-cause: kubectl create --kubeconfig=kubeconfig --namespace=mynamespace --filename=mytemplate.yaml --record=true\r\n  creationTimestamp: 2017-12-12T22:28:54Z\r\n  labels:\r\n    name: my-cron\r\n    version: 123\r\n  name: my-cron-123\r\n  namespace: mynamespace \r\n  resourceVersion: \"104537731\"\r\n  selfLink: /apis/batch/v2alpha1/namespaces/mynamespace/cronjobs/my-cron-123 \r\n  uid: d63bdd94-df8b-11e7-a687-005056ae60f5\r\nspec:\r\n  concurrencyPolicy: Forbid \r\n  failedJobsHistoryLimit: 5\r\n  jobTemplate:\r\n    metadata:\r\n      creationTimestamp: null\r\n    spec:\r\n      template:\r\n        metadata:\r\n          creationTimestamp: null\r\n          labels:\r\n            app: hosting_platform\r\n            name: my-cron\r\n            version: 123\r\n        spec:\r\n          containers:\r\n          - command:\r\n            - task.py\r\n            image: myregistry:5000/mycronimage:tag\r\n            imagePullPolicy: IfNotPresent\r\n            livenessProbe:\r\n              exec:\r\n                command: \r\n                - /control-scripts/livenessCheck.sh\r\n              failureThreshold: 3\r\n              initialDelaySeconds: 30\r\n              periodSeconds: 30\r\n              successThreshold: 1\r\n              timeoutSeconds: 1\r\n            name: my-cron\r\n            resources:\r\n              limits:\r\n                cpu: 1500m\r\n                memory: 250Mi\r\n              requests:\r\n                cpu: 500m\r\n                memory: 150Mi\r\n            terminationMessagePath: /dev/termination-log\r\n            terminationMessagePolicy: File\r\n            volumeMounts:\r\n            - mountPath: /tmp/ssh\r\n              name: etc-ssh\r\n            - mountPath: /configs\r\n              name: configmap-mount\r\n          dnsPolicy: ClusterFirst\r\n          restartPolicy: OnFailure\r\n          schedulerName: default-scheduler\r\n          securityContext: {}\r\n          terminationGracePeriodSeconds: 30\r\n          volumes:\r\n          - hostPath:\r\n              path: /etc/ssh\r\n            name: etc-ssh\r\n          - configMap:\r\n              defaultMode: 420\r\n              items: ...\r\n              name: 1ed50810899225c2955984cc360e87dd\r\n            name: configmap-mount\r\n  schedule: '* * * * *'\r\n  successfulJobsHistoryLimit: 2\r\n  suspend: false\r\nstatus:       \r\n  lastScheduleTime: 2017-12-20T17:36:00Z\r\n```\r\n\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.8\", GitCommit:\"d74e09bb4e4e7026f45becbed8310665ddcb8514\", GitTreeState:\"clean\", BuildDate:\"2017-08-03T18:12:08Z\", GoVersion:\"go1.7.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.8\", GitCommit:\"d74e09bb4e4e7026f45becbed8310665ddcb8514\", GitTreeState:\"clean\", BuildDate:\"2017-08-03T18:01:01Z\", GoVersion:\"go1.7.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n- Cloud provider or hardware configuration: Bare metal\r\n- OS (e.g. from /etc/os-release): 16.04.1 LTS (Xenial Xerus)\r\n- Kernel (e.g. `uname -a`): 4.4.0-89-generic #112-Ubuntu\r\n- Install tools: N/A\r\n- Others: N/A",
  "closed_at": "2018-01-03T08:30:33Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/13653959?v=4",
    "events_url": "https://api.github.com/users/k8s-github-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-github-robot/followers",
    "following_url": "https://api.github.com/users/k8s-github-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-github-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-github-robot",
    "id": 13653959,
    "login": "k8s-github-robot",
    "node_id": "MDQ6VXNlcjEzNjUzOTU5",
    "organizations_url": "https://api.github.com/users/k8s-github-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-github-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-github-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-github-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-github-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-github-robot"
  },
  "comments": 7,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/57465/comments",
  "created_at": "2017-12-20T17:54:18Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/57465/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/57465",
  "id": 283646219,
  "labels": [
    {
      "color": "0052cc",
      "default": false,
      "description": null,
      "id": 116719829,
      "name": "area/kubelet",
      "node_id": "MDU6TGFiZWwxMTY3MTk4Mjk=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/area/kubelet"
    },
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Network.",
      "id": 116712108,
      "name": "sig/network",
      "node_id": "MDU6TGFiZWwxMTY3MTIxMDg=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/network"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/57465/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUyODM2NDYyMTk=",
  "number": 57465,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "\"Error deleting network: failed to Statfs\" during pod shutdown",
  "updated_at": "2018-01-03T08:30:33Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/57465",
  "user": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/1418951?v=4",
    "events_url": "https://api.github.com/users/DreadPirateShawn/events{/privacy}",
    "followers_url": "https://api.github.com/users/DreadPirateShawn/followers",
    "following_url": "https://api.github.com/users/DreadPirateShawn/following{/other_user}",
    "gists_url": "https://api.github.com/users/DreadPirateShawn/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/DreadPirateShawn",
    "id": 1418951,
    "login": "DreadPirateShawn",
    "node_id": "MDQ6VXNlcjE0MTg5NTE=",
    "organizations_url": "https://api.github.com/users/DreadPirateShawn/orgs",
    "received_events_url": "https://api.github.com/users/DreadPirateShawn/received_events",
    "repos_url": "https://api.github.com/users/DreadPirateShawn/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/DreadPirateShawn/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/DreadPirateShawn/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/DreadPirateShawn"
  }
}