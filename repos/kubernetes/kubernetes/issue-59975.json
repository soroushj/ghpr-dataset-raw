{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "\r\n**Is this a BUG REPORT or FEATURE REQUEST?**:\r\n\r\n> Uncomment only one, leave it on its own line: \r\n>\r\n> /kind bug\r\n\r\n/kind feature\r\n\r\n\r\n**What happened**:\r\nWe are  starting  to use the HPA (horizontal pod  autoscaling) for one of  our components. What is  special about this  particular  component is  that the warm up period is very long (sometimes  even more  than 30 minutes). During that time, the pod is in Running state but its containers are in Not Ready state.\r\nThis  pod  is of course not  able to receive any traffic so the  metric we  use in the HPA is ignoring those  pods that are not  Ready.\r\n\r\nThe  equation used  by HPA  to calculate the  new  number  of replicas is as  follows\r\n`new replica count=(current replica  count)*(current metric value)/(expected metric)`\r\n\r\nOur problem is  that the equation used  by HPA to calculate the  new number  of  replicas  is including in the \"current  replica count\" the pods that  are  \"Not  Ready\" so the value  for \"new replica count\" is not correct. Since this warm up is so long  we end up by having a lot of more instances of our services of what it should  be necessary.  They eventually get deleted once  gradually the metrics start going down but  that will  take a lot of time and a big waste of resources without mentioning  that with a  changing traffic this HPA will never stabilize  \r\n\r\n**What you expected to happen**:\r\nI would like to have a flag somewhere  where  I tell HPA to don't include  in the  \"current replica count\" the pods  that are  in \"Not Ready\" state\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nHave a component that takes a lot of time to start and being  able to receive traffic. Then define  a metric that  will ignore  the pods  that  are  Not Ready. Finally create  a  metric that  will scale that component based on  the mentioned  metric\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version 1.7\r\n- Cloud provider or hardware configuration:\r\nAzure\r\n",
  "closed_at": "2018-03-21T08:34:23Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/13653959?v=4",
    "events_url": "https://api.github.com/users/k8s-github-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-github-robot/followers",
    "following_url": "https://api.github.com/users/k8s-github-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-github-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-github-robot",
    "id": 13653959,
    "login": "k8s-github-robot",
    "node_id": "MDQ6VXNlcjEzNjUzOTU5",
    "organizations_url": "https://api.github.com/users/k8s-github-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-github-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-github-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-github-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-github-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-github-robot"
  },
  "comments": 24,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/59975/comments",
  "created_at": "2018-02-16T16:45:44Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/59975/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/59975",
  "id": 297848954,
  "labels": [
    {
      "color": "c7def8",
      "default": false,
      "description": "Categorizes issue or PR as related to a new feature.",
      "id": 267761362,
      "name": "kind/feature",
      "node_id": "MDU6TGFiZWwyNjc3NjEzNjI=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/feature"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Autoscaling.",
      "id": 238245616,
      "name": "sig/autoscaling",
      "node_id": "MDU6TGFiZWwyMzgyNDU2MTY=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/autoscaling"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/59975/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUyOTc4NDg5NTQ=",
  "number": 59975,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "For  Horizontal pod autoscaling enable flag to don't count in currentReplicas pods that are  not Ready",
  "updated_at": "2018-03-21T08:34:23Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/59975",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/10172530?v=4",
    "events_url": "https://api.github.com/users/lumeche/events{/privacy}",
    "followers_url": "https://api.github.com/users/lumeche/followers",
    "following_url": "https://api.github.com/users/lumeche/following{/other_user}",
    "gists_url": "https://api.github.com/users/lumeche/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/lumeche",
    "id": 10172530,
    "login": "lumeche",
    "node_id": "MDQ6VXNlcjEwMTcyNTMw",
    "organizations_url": "https://api.github.com/users/lumeche/orgs",
    "received_events_url": "https://api.github.com/users/lumeche/received_events",
    "repos_url": "https://api.github.com/users/lumeche/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/lumeche/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/lumeche/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/lumeche"
  }
}