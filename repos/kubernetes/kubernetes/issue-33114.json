{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "NONE",
  "body": "Having an issue with how the vSphere provider is initialization and getting the VM name from vCenter.  Currently the provider tries to get the VM name by looking for the VM by IP address.  The issues arises here in that if you are using an overlay network, like flannel, and have multiple clusters running in the same datacenter, this method of finding the VM becomes non-deterministic.  This is because if you are lucky enough for flannel to pick IPs that are different from the other cluster, fine, you will find the right VM, but if flannel happens to pick the same IPs, which is very likely, then this method may, or may not, pick the right VM.\n\nFor example, if I have a host that has the IPs of 25.1.9.1 & 25.1.9.0 because of flannel which is configured to use a 25.1.0.0/16 network, then very easily another cluster could be spun up with flannel configured for this same network and have another host have these same IPs.\n\nNow, when in `readInstance` we loop through the addresses of the host and then ask vCenter if it has a VM for this IP, you will now get the wrong VM returned.\n\nI see a couple potential directions here:\n1.  Allow the cloud config to specify an interface to \"bind\" to.  This way we don't ask the host for all IPs but just for the IPs on a specific interface.\n2.  Ask vCenter if it has a VM for the hostname first, if this is returned move on.  Most of the time this will work as it is a strong convention that the VM name matches the hostname.  Albeit this is not guaranteed to always work.\n\nIf I were to choose I would think 1 would be the strongest option because it allows for the most user control over behavior.\n\nRegardless, wanted to bring this up, as others may run in to the same thing.  Currently the only work around I have is to maintain flannel network segmentations among clusters.  This means all of my clusters now are indirectly coupled with each other and must maintain a cluster to network mapping.  This is not ideal in the long run, especially with the goal of making my clusters act as cattle as well.\n",
  "closed_at": "2017-02-10T21:35:42Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/13653959?v=4",
    "events_url": "https://api.github.com/users/k8s-github-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-github-robot/followers",
    "following_url": "https://api.github.com/users/k8s-github-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-github-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-github-robot",
    "id": 13653959,
    "login": "k8s-github-robot",
    "node_id": "MDQ6VXNlcjEzNjUzOTU5",
    "organizations_url": "https://api.github.com/users/k8s-github-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-github-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-github-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-github-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-github-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-github-robot"
  },
  "comments": 0,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/33114/comments",
  "created_at": "2016-09-20T19:07:46Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/33114/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/33114",
  "id": 178148142,
  "labels": [
    {
      "color": "0052cc",
      "default": false,
      "description": null,
      "id": 138247961,
      "name": "area/kubectl",
      "node_id": "MDU6TGFiZWwxMzgyNDc5NjE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/area/kubectl"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/33114/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWUxNzgxNDgxNDI=",
  "number": 33114,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "vSphere Cloud Provider invalid server name",
  "updated_at": "2017-02-10T21:35:42Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/33114",
  "user": {
    "avatar_url": "https://avatars2.githubusercontent.com/u/1779323?v=4",
    "events_url": "https://api.github.com/users/moserke/events{/privacy}",
    "followers_url": "https://api.github.com/users/moserke/followers",
    "following_url": "https://api.github.com/users/moserke/following{/other_user}",
    "gists_url": "https://api.github.com/users/moserke/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/moserke",
    "id": 1779323,
    "login": "moserke",
    "node_id": "MDQ6VXNlcjE3NzkzMjM=",
    "organizations_url": "https://api.github.com/users/moserke/orgs",
    "received_events_url": "https://api.github.com/users/moserke/received_events",
    "repos_url": "https://api.github.com/users/moserke/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/moserke/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/moserke/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/moserke"
  }
}