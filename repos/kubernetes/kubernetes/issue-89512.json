{
  "active_lock_reason": null,
  "assignee": {
    "avatar_url": "https://avatars3.githubusercontent.com/u/331852?v=4",
    "events_url": "https://api.github.com/users/neolit123/events{/privacy}",
    "followers_url": "https://api.github.com/users/neolit123/followers",
    "following_url": "https://api.github.com/users/neolit123/following{/other_user}",
    "gists_url": "https://api.github.com/users/neolit123/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/neolit123",
    "id": 331852,
    "login": "neolit123",
    "node_id": "MDQ6VXNlcjMzMTg1Mg==",
    "organizations_url": "https://api.github.com/users/neolit123/orgs",
    "received_events_url": "https://api.github.com/users/neolit123/received_events",
    "repos_url": "https://api.github.com/users/neolit123/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/neolit123/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/neolit123/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/neolit123"
  },
  "assignees": [
    {
      "avatar_url": "https://avatars3.githubusercontent.com/u/331852?v=4",
      "events_url": "https://api.github.com/users/neolit123/events{/privacy}",
      "followers_url": "https://api.github.com/users/neolit123/followers",
      "following_url": "https://api.github.com/users/neolit123/following{/other_user}",
      "gists_url": "https://api.github.com/users/neolit123/gists{/gist_id}",
      "gravatar_id": "",
      "html_url": "https://github.com/neolit123",
      "id": 331852,
      "login": "neolit123",
      "node_id": "MDQ6VXNlcjMzMTg1Mg==",
      "organizations_url": "https://api.github.com/users/neolit123/orgs",
      "received_events_url": "https://api.github.com/users/neolit123/received_events",
      "repos_url": "https://api.github.com/users/neolit123/repos",
      "site_admin": false,
      "starred_url": "https://api.github.com/users/neolit123/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/neolit123/subscriptions",
      "type": "User",
      "url": "https://api.github.com/users/neolit123"
    }
  ],
  "author_association": "CONTRIBUTOR",
  "body": "**What happened**:\r\n\r\nAfter upgrading to v1.18.0 not possible to join existing node into cluster:\r\n\r\n```bash\r\n# /usr/bin/kubeadm join 10.28.36.157:8443 --token <redacted> --discovery-token-ca-cert-hash \r\n...\r\nI0326 11:57:39.638919   12149 kubelet.go:145] [kubelet-start] Checking for an existing Node in the cluster with name \"m1c4\" and status \"Ready\"\r\na Node with name \"m1c4\" and status \"Ready\" already exists in the cluster. You must delete the existing Node or change the name of this new joining Node\r\n```\r\n\r\n<details>\r\n<summary>Full log</summary>\r\n\r\n<pre>\r\nW0326 11:57:39.322862   12149 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.\r\nI0326 11:57:39.322941   12149 join.go:371] [preflight] found NodeName empty; using OS hostname as NodeName\r\nI0326 11:57:39.322968   12149 initconfiguration.go:103] detected and using CRI socket: /var/run/dockershim.sock\r\n[preflight] Running pre-flight checks\r\nI0326 11:57:39.323001   12149 preflight.go:90] [preflight] Running general checks\r\nI0326 11:57:39.323031   12149 checks.go:249] validating the existence and emptiness of directory /etc/kubernetes/manifests\r\nI0326 11:57:39.323070   12149 checks.go:286] validating the existence of file /etc/kubernetes/kubelet.conf\r\nI0326 11:57:39.323077   12149 checks.go:286] validating the existence of file /etc/kubernetes/bootstrap-kubelet.conf\r\nI0326 11:57:39.323086   12149 checks.go:102] validating the container runtime\r\nI0326 11:57:39.407093   12149 checks.go:128] validating if the service is enabled and active\r\nI0326 11:57:39.487777   12149 checks.go:335] validating the contents of file /proc/sys/net/bridge/bridge-nf-call-iptables\r\nI0326 11:57:39.487834   12149 checks.go:335] validating the contents of file /proc/sys/net/ipv4/ip_forward\r\nI0326 11:57:39.487861   12149 checks.go:649] validating whether swap is enabled or not\r\nI0326 11:57:39.487890   12149 checks.go:376] validating the presence of executable conntrack\r\nI0326 11:57:39.487922   12149 checks.go:376] validating the presence of executable ip\r\nI0326 11:57:39.487954   12149 checks.go:376] validating the presence of executable iptables\r\nI0326 11:57:39.487982   12149 checks.go:376] validating the presence of executable mount\r\nI0326 11:57:39.488009   12149 checks.go:376] validating the presence of executable nsenter\r\nI0326 11:57:39.488030   12149 checks.go:376] validating the presence of executable ebtables\r\nI0326 11:57:39.488054   12149 checks.go:376] validating the presence of executable ethtool\r\nI0326 11:57:39.488076   12149 checks.go:376] validating the presence of executable socat\r\nI0326 11:57:39.488096   12149 checks.go:376] validating the presence of executable tc\r\nI0326 11:57:39.488118   12149 checks.go:376] validating the presence of executable touch\r\nI0326 11:57:39.488146   12149 checks.go:520] running all checks\r\nI0326 11:57:39.560686   12149 checks.go:406] checking whether the given node name is reachable using net.LookupHost\r\nI0326 11:57:39.560800   12149 checks.go:618] validating kubelet version\r\nI0326 11:57:39.598654   12149 checks.go:128] validating if the service is enabled and active\r\nI0326 11:57:39.609631   12149 checks.go:201] validating availability of port 10250\r\nI0326 11:57:39.609728   12149 checks.go:286] validating the existence of file /etc/kubernetes/pki/ca.crt\r\nI0326 11:57:39.609743   12149 checks.go:432] validating if the connectivity type is via proxy or direct\r\nI0326 11:57:39.609764   12149 join.go:441] [preflight] Discovering cluster-info\r\nI0326 11:57:39.609782   12149 token.go:78] [discovery] Created cluster-info discovery client, requesting info from \"10.28.36.157:8443\"\r\nI0326 11:57:39.616900   12149 token.go:116] [discovery] Requesting info from \"10.28.36.157:8443\" again to validate TLS against the pinned public key\r\nI0326 11:57:39.622631   12149 token.go:133] [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"10.28.36.157:8443\"\r\nI0326 11:57:39.622650   12149 discovery.go:51] [discovery] Using provided TLSBootstrapToken as authentication credentials for the join process\r\nI0326 11:57:39.622659   12149 join.go:455] [preflight] Fetching init configuration\r\nI0326 11:57:39.622667   12149 join.go:493] [preflight] Retrieving KubeConfig objects\r\n[preflight] Reading configuration from the cluster...\r\n[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\r\nI0326 11:57:39.635104   12149 interface.go:400] Looking for default routes with IPv4 addresses\r\nI0326 11:57:39.635123   12149 interface.go:405] Default route transits interface \"vmbr0\"\r\nI0326 11:57:39.635884   12149 interface.go:208] Interface vmbr0 is up\r\nI0326 11:57:39.636606   12149 interface.go:256] Interface \"vmbr0\" has 3 addresses :[10.28.36.164/16 2a02:2b88:1:2:9657:a5ff:fed3:b6f2/64 fe80::9657:a5ff:fed3:b6f2/64].\r\nI0326 11:57:39.636663   12149 interface.go:223] Checking addr  10.28.36.164/16.\r\nI0326 11:57:39.636681   12149 interface.go:230] IP found 10.28.36.164\r\nI0326 11:57:39.636696   12149 interface.go:262] Found valid IPv4 address 10.28.36.164 for interface \"vmbr0\".\r\nI0326 11:57:39.636710   12149 interface.go:411] Found active IP 10.28.36.164 \r\nI0326 11:57:39.636959   12149 preflight.go:101] [preflight] Running configuration dependant checks\r\nI0326 11:57:39.636986   12149 controlplaneprepare.go:211] [download-certs] Skipping certs download\r\nI0326 11:57:39.637019   12149 kubelet.go:111] [kubelet-start] writing bootstrap kubelet config file at /etc/kubernetes/bootstrap-kubelet.conf\r\nI0326 11:57:39.638010   12149 kubelet.go:119] [kubelet-start] writing CA certificate at /etc/kubernetes/pki/ca.crt\r\nI0326 11:57:39.638919   12149 kubelet.go:145] [kubelet-start] Checking for an existing Node in the cluster with name \"m1c4\" and status \"Ready\"\r\na Node with name \"m1c4\" and status \"Ready\" already exists in the cluster. You must delete the existing Node or change the name of this new joining Node\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/join.runKubeletStartJoinPhase\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/join/kubelet.go:152\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:234\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:422\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd.NewCmdJoin.func1\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/join.go:170\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:826\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:914\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:864\r\nk8s.io/kubernetes/cmd/kubeadm/app.Run\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50\r\nmain.main\r\n\t_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25\r\nruntime.main\r\n\t/usr/local/go/src/runtime/proc.go:203\r\nruntime.goexit\r\n\t/usr/local/go/src/runtime/asm_amd64.s:1357\r\nerror execution phase kubelet-start\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:235\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:422\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207\r\nk8s.io/kubernetes/cmd/kubeadm/app/cmd.NewCmdJoin.func1\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/join.go:170\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:826\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:914\r\nk8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:864\r\nk8s.io/kubernetes/cmd/kubeadm/app.Run\r\n\t/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50\r\nmain.main\r\n\t_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25\r\nruntime.main\r\n\t/usr/local/go/src/runtime/proc.go:203\r\nruntime.goexit\r\n\t/usr/local/go/src/runtime/asm_amd64.s:1357\r\n</pre>\r\n</details>\r\n\r\nThe problem is that the check introduced in https://github.com/kubernetes/kubernetes/commit/b117a928a6c3f650931bdac02a41fca6680548c4 violates the existing diskless node approach.\r\n\r\nEg. we are having cluster of hundreds servers which are booding over PXE and have no any local storage.\r\nAfter the boot they just running `kubeadm join` command to connect to the cluster, they can be rebooted ay any time, and after reboot they will run join procedure again.\r\n\r\nThus node object in cluster may already have some settings, labels and annotation for this node. After the joining the node just reuses them, this is working as a charm.\r\n\r\n**What you expected to happen**:\r\n\r\nExisting node successful joined\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n```\r\nkubeadm join\r\nkubeadm reset\r\nkubeadm join\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nCurrently if you try to join the node you will face with https://github.com/kubernetes/kubernetes/issues/89501, but I created cluster role to check this chage.\r\n\r\n**Environment**:\r\n- Kubernetes version: `v1.18.0`\r\n- Cloud provider or hardware configuration: Bare Metal\r\n- OS: `Ubuntu 18.04.3 LTS`\r\n- Kernel: `4.15.18-24-pve`\r\n- Install tools: `kubeadm`\r\n- Network plugin and version `kube-router v0.4.0`\r\n- Others:\r\n",
  "closed_at": "2020-03-28T07:11:53Z",
  "closed_by": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/20407524?v=4",
    "events_url": "https://api.github.com/users/k8s-ci-robot/events{/privacy}",
    "followers_url": "https://api.github.com/users/k8s-ci-robot/followers",
    "following_url": "https://api.github.com/users/k8s-ci-robot/following{/other_user}",
    "gists_url": "https://api.github.com/users/k8s-ci-robot/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/k8s-ci-robot",
    "id": 20407524,
    "login": "k8s-ci-robot",
    "node_id": "MDQ6VXNlcjIwNDA3NTI0",
    "organizations_url": "https://api.github.com/users/k8s-ci-robot/orgs",
    "received_events_url": "https://api.github.com/users/k8s-ci-robot/received_events",
    "repos_url": "https://api.github.com/users/k8s-ci-robot/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/k8s-ci-robot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/k8s-ci-robot/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/k8s-ci-robot"
  },
  "comments": 19,
  "comments_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/89512/comments",
  "created_at": "2020-03-26T11:20:48Z",
  "events_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/89512/events",
  "html_url": "https://github.com/kubernetes/kubernetes/issues/89512",
  "id": 588345015,
  "labels": [
    {
      "color": "0052cc",
      "default": false,
      "description": null,
      "id": 451459590,
      "name": "area/kubeadm",
      "node_id": "MDU6TGFiZWw0NTE0NTk1OTA=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/area/kubeadm"
    },
    {
      "color": "e11d21",
      "default": false,
      "description": "Categorizes issue or PR as related to a bug.",
      "id": 105146071,
      "name": "kind/bug",
      "node_id": "MDU6TGFiZWwxMDUxNDYwNzE=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/kind/bug"
    },
    {
      "color": "fef2c0",
      "default": false,
      "description": "Lowest priority. Possibly useful, but not yet enough support to actually get it done.",
      "id": 149621825,
      "name": "priority/awaiting-more-evidence",
      "node_id": "MDU6TGFiZWwxNDk2MjE4MjU=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/priority/awaiting-more-evidence"
    },
    {
      "color": "d2b48c",
      "default": false,
      "description": "Categorizes an issue or PR as relevant to SIG Cluster Lifecycle.",
      "id": 173494222,
      "name": "sig/cluster-lifecycle",
      "node_id": "MDU6TGFiZWwxNzM0OTQyMjI=",
      "url": "https://api.github.com/repos/kubernetes/kubernetes/labels/sig/cluster-lifecycle"
    }
  ],
  "labels_url": "https://api.github.com/repos/kubernetes/kubernetes/issues/89512/labels{/name}",
  "locked": false,
  "milestone": null,
  "node_id": "MDU6SXNzdWU1ODgzNDUwMTU=",
  "number": 89512,
  "performed_via_github_app": null,
  "repository_url": "https://api.github.com/repos/kubernetes/kubernetes",
  "state": "closed",
  "title": "a Node with name and status \"Ready\" already exists in the cluster.",
  "updated_at": "2020-03-28T07:11:53Z",
  "url": "https://api.github.com/repos/kubernetes/kubernetes/issues/89512",
  "user": {
    "avatar_url": "https://avatars0.githubusercontent.com/u/7556217?v=4",
    "events_url": "https://api.github.com/users/kvaps/events{/privacy}",
    "followers_url": "https://api.github.com/users/kvaps/followers",
    "following_url": "https://api.github.com/users/kvaps/following{/other_user}",
    "gists_url": "https://api.github.com/users/kvaps/gists{/gist_id}",
    "gravatar_id": "",
    "html_url": "https://github.com/kvaps",
    "id": 7556217,
    "login": "kvaps",
    "node_id": "MDQ6VXNlcjc1NTYyMTc=",
    "organizations_url": "https://api.github.com/users/kvaps/orgs",
    "received_events_url": "https://api.github.com/users/kvaps/received_events",
    "repos_url": "https://api.github.com/users/kvaps/repos",
    "site_admin": false,
    "starred_url": "https://api.github.com/users/kvaps/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/kvaps/subscriptions",
    "type": "User",
    "url": "https://api.github.com/users/kvaps"
  }
}